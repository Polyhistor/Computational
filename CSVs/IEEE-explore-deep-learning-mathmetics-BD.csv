"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Deep Learning and Big Data Integration with Cuckoo Search Optimization for Robust Phishing Attack Detection","B. B. Gupta; A. Gaurav; J. Wu; V. Arya; K. T. Chui","Department of Computer Science and Information Engineering, Asia University, Taichung, Taiwan; Ronin Institute, Montclair, NJ, USA; School of Artificial Intelligence, Guilin University of Electronic Technology, China; Asia University, Taichung, Taiwan; Hong Kong Metropolitan University (HKMU), Hong Kong",ICC 2024 - IEEE International Conference on Communications,"20 Aug 2024","2024","","","1322","1327","Currently, phishing attacks are posing great damage to the online community. As traditions, attack detection strategies are not effective against this new type of threat. Hence, there is a need for advanced attack detection techniques. In this context, this research proposed a hybrid deep learning and big data-based technique for phishing attack detection approach. Our proposed approach used Conv2d layers in sequence for analysis of the incoming traffic and predict its behavior. We used different parameters to measure our proposed approach. Through the use of the cuckoo optimization algorithm, the propsed approach achieves a high accuracy of 92%.","1938-1883","978-1-7281-9054-9","10.1109/ICC51166.2024.10622646","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10622646","Phishing Attack;Deep Learning;Big Data","Deep learning;Adaptation models;Accuracy;Phishing;Organizations;Prediction algorithms;Data models","","","","29","IEEE","20 Aug 2024","","","IEEE","IEEE Conferences"
"Intracranial Hemorrhage Detection using Deep Learning and Optimization Techniques","M. Balipa; P. P Kundapur; Adithya","Department of MCA, Nitte (Deemed to be University) NMAM Institute of Technology, Nitte, Karnataka, India; Department of DS and CA, Manipal Institute of Technology MAHE, Manipal, India; Department of MCA, Nitte (Deemed to be University) NMAM Institute of Technology, Nitte, Karnataka, India",2023 Third International Conference on Ubiquitous Computing and Intelligent Information Systems (ICUIS),"29 Apr 2024","2023","","","483","489","Intracranial Hemorrhage (ICH) is a critical medical condition characterized by bleeding within the skull, specifically in the brain. Timely detection and accurate classification of ICH from computed tomography (CT) scan pictures are essential for successful treatment plan and improved patient outcomes. This study presents a novel approach for ICH detection using Deep Learning and Machine Learning techniques. A hybrid model is proposed, leveraging the power of ResN et50, a state-of-the-art deep learning architecture, together with different machine learning methods. The hybrid models include ResN et50 with Support Vector Machines(SVM), Artificial Neural Networks(ANN), ANN optimized using the Grasshopper Optimization Algorithm, and ANN optimized using the Genetic Optimization Algorithm. By harnessing the learned features of ResN et50 and the optimization capabilities of these algorithms, the models aim to accurately identify and classify hemorrhage patterns in CT scans. Experimental evaluations on a dataset of CT scan images demonstrate the superiority and efficacy of the proposed hybrid models in achieving high classification accuracy. The results highlight the potential of this approach for aiding clinicians in early diagnosis and treatment planning of ICH, thus contributing to improved patient care in cases of intracranial hemorrhage.","","979-8-3503-0698-9","10.1109/ICUIS60567.2023.00086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10505982","Intracranial Hemorrhage;Deep Learning;Machine Learning;ResNet50;Support Vector Machine (SVM);Artificial Neural Networks (ANN);Grasshopper Optimization Algorithm;Genetic Optimization Algorithm;Computed Tomography(CT) scans","Deep learning;Machine learning algorithms;Computed tomography;Genetics;Feature extraction;Vectors;Classification algorithms","","","","18","IEEE","29 Apr 2024","","","IEEE","IEEE Conferences"
"An Interpretation Method for Fault Diagnosis in Rolling Bearings: Integrating Whale Optimization Algorithm and SqueezeNet Model","W. Gu; Z. Zhu; W. Xu","College of Big Data and Information Engineering, Guizhou University, Guizhou, China; School of Mathematical Sciences, Beijing Normal University, Beijing, China; Faculty of Science and Technology, University of Macau, Macao, China","2023 IEEE International Conference on Electrical, Automation and Computer Engineering (ICEACE)","28 Feb 2024","2023","","","1403","1411","The bearing is a commonly encountered element in mechanical systems, and the adoption of a precise and efficient methodology for diagnosing bearing faults can greatly enhance the operational safety and stability of the system. This study introduces an innovative methodology to tackle the problem of diagnosing faults in bearing vibration signal features that are weak and pose challenges. The method proposed in this study entails the utilization of an enhanced SqueezeNet network, which exhibits the ability to interpret features for the purpose of detecting bearing faults in images. The initial aim is to perform preprocessing on the original vibration signal data acquired from the bearing. The process entails converting the initial signal in the time domain into a two-dimensional representation known as a time-frequency map. In order to replicate the noise conditions found in the real working environment, a deliberate introduction of a noise signal is performed during this transformation, with a signal-to-noise ratio varying between 10 and 20. Following a comprehensive comparative analysis of different deep learning models, the SqueezeNet model has been selected due to its exceptional antinoise capabilities and superior accuracy. The progressive grid method is utilized for the purpose of optimization, aiming to analyze the fluctuations in network accuracy. The objective of this study is to examine the relationship between network accuracy and changes in hyperparameters. The SqueezeNet model, which has undergone fine-tuning, is subsequently employed in combination with diverse swarm optimization algorithms. Based on a comparative analysis of the loss function iteration curves, the whale algorithm has been selected as the preferred optimization algorithm. The aforementioned algorithm is subsequently utilized within the framework of resolving the fault detection issue. The experiment also demonstrates the model's feature extraction process by employing the t-SNE visualization method. Furthermore, the utilization of class activation mapping is employed to analyze the time-frequency diagrams pertaining to the four identified faults observed in the bearings. The objective of this approach is to improve the comprehensibility of the model in its classification task and offer a graphical depiction of the model's attention towards different features. The performance of the method is evaluated by utilizing the bearing dataset obtained from Case Western Reserve University. The findings indicate that the approach exhibits a considerable degree of precision, approaching nearly 100%, in the identification of bearing abnormalities. This validation affirms the method's robust potential and feasibility for practical applications, emphasizing its superiority in fault diagnosis and adaptability to diverse application scenarios.","","979-8-3503-0961-4","10.1109/ICEACE60673.2023.10442005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10442005","bearing faults;SqueezeNet;t-SNE visualization method;WOA;class activation mapping","Fault diagnosis;Analytical models;Heuristic algorithms;Rolling bearings;Whales;Whale optimization algorithms;Optimization","","","","18","IEEE","28 Feb 2024","","","IEEE","IEEE Conferences"
"An Intrusion Detection Approach using Hierarchical Deep Learning-based Butterfly Optimization Algorithm in Big Data Platform","Manoranjithem; S. Dhanasekaran; A. Asokan; A. Kumar; C. Yamini; M. Tiwari","Department of CSE, Kalasalingam Academy of Research and Education, Tamilnadu, India; Department of IT, Kalasalingam Academy of Research and Education, Tamilnadu, India; Department of Computer and Communication Engineering, Sri Eshwar College of Engineering, Coimbatore, Tamilnadu, India; Faculty of Computer Applications, Manav Rachna International Institute of Research and Studies, Faridabad, Haryana, India; Department of Mathematics, PSNA College of Engineering and Technology, Dindigul, Tamilnadu, India; Department of Computer Science and Engineering, Bharati Vidyapeeth's College of Engineering, Delhi, India",2023 International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT),"1 Mar 2023","2023","","","212","216","In recent times, a massive count of data and their increase gradually changed the significance of data security and data analysis methods for Big Data. An intrusion detection system (IDS) is a scheme which analyzes and monitors data for detecting some intrusion from the system or network. Massive volume, variety, and maximum speed of data created in the network develop the data analysis procedure for detecting attacks with typical approaches highly complex. Big Data systems can be utilized in IDS for managing Big Data for accurate and effectual data analysis procedures. This study develops an Intrusion Detection Approach using Hierarchical Deep Learning-based Butterfly Optimization algorithm (ID-HDLBOA) in Big Data Platform. The presented ID-HDLBOA technique combines the concept of DL with hyperparameter tuning process. In the presented ID-HDLBOA technique, hierarchical LSTM model is provided for intrusion detection purposes. Finally, BOA is used as a hyperparameter tuning strategy for the LSTM model and it results in improvised detection efficiency. The experimental validation of the ID-HDLBOA technique is assessed on benchmark intrusion dataset and the model gives the accuracy value of 98%. Wide-ranging experiments were performed and the outcomes emphasized the supremacy of the ID-HDLBOA algorithm.","","978-1-6654-7451-1","10.1109/IDCIoT56793.2023.10053504","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10053504","Deep learning;Big data;Security;Intrusion detection;Hyperparameter tuning","Data analysis;Intrusion detection;Big Data;Benchmark testing;Feature extraction;Classification algorithms;Internet of Things","","2","","17","IEEE","1 Mar 2023","","","IEEE","IEEE Conferences"
"Performance optimization algorithm for CRF layer of Chinese sequence labeling model based on deep learning","L. Zhu; Y. Zhang; Y. Dong; C. Cui","Engineering University of PAP, Xi’an, China; Shijiazhuang Division of PLAA Infantry College, Shijiazhuang, China; Joint Operations College, National Defense University, Shijiazhuang, China; China Mobile Chengdu Institute of Research and Development, Chengdu, China","2022 4th International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)","22 May 2023","2022","","","279","285","The “deep learning model + CRF” model architecture such as Bi-LSTM-CRF has outstanding performance| in the tasks of part-of-speech tagging, named entity recognition, and semantic role tagging in the field of natural language processing, and has become the most popular algorithm for sequence tagging tasks. However, in practical applications, due to the strong fitting ability of the feature extractor in the deep learning model, the use efficiency of the CRF layer is relatively low; at the same time, the calculation cost of the CRF layer is relatively large. This paper proposes a performance optimization algorithm for the CRF layer in the sequence annotation task based on the deep learning model. The algorithm introduces five rules. First of all, the tag result of the feature extractor is checked by the five rules, if the adjacent tag is found to be logically incorrect, then use the CRF layer for logic correction. The experimental results show that the algorithm shows great advantages in the process of word segmentation, part-of-speech tagging and named entity recognition, a lot of computing time has been saved.","","979-8-3503-3394-7","10.1109/MLBDBI58171.2022.00061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10125439","Sequence labeling;CRF;deep learning","Deep learning;Analytical models;Computational modeling;Tagging;Predictive models;Feature extraction;Prediction algorithms","","","","11","IEEE","22 May 2023","","","IEEE","IEEE Conferences"
"Revolution of the Indian Agricultural Landscape using Machine Learning and Big Data Techniques: A Systematic Review","J. K. Sondhi; T. Junankar; A. M. Nair","Department of Data Science, CHRIST( Deemed to be University), Pune, India; Department of Data Science, CHRIST( Deemed to be University), Pune, India; Department of Data Science, CHRIST( Deemed to be University), Pune, India","2022 4th International Conference on Advances in Computing, Communication Control and Networking (ICAC3N)","28 Mar 2023","2022","","","724","727","The world of Big Data has been rapidly expanding into the domains of Engineering and Machine Learning. The biggest challenge in the Big Data landscape is the incompetence of processing vast amounts of data in a time-efficient manner. The agriculture domain has so long only relied on traditional method for yield prediction. This can be bettered by using novel Machine Learning techniques and innovative thinking. The study provides the review of most of the techniques already implemented in the ML, Big Data and Agriculture domain- traditional and modern- while focusing on highlighting the difference in accuracy between the traditional methods and the more advanced methods.","","978-1-6654-7436-8","10.1109/ICAC3N56670.2022.10074152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10074152","Machine Learning;Big Data;Agriculture;Regression;Crop yield;Data Mining","Deep learning;Systematics;Neural networks;Crops;Process control;Production;Big Data","","","","13","IEEE","28 Mar 2023","","","IEEE","IEEE Conferences"
"Study on Deep Unsupervised Learning Optimization Algorithm Based on Cloud Computing","H. Yan; P. Yu; D. Long","Changchun Institute of Technology, Changchun, China; Changchun Institute of Technology, Changchun, China; Jilin Province Soft Science Research Institute, Changchun, China","2019 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)","21 Mar 2019","2019","","","679","681","Big data has already occupied a lot in the information society. The application of big data to intelligent agriculture is the core development direction for maximizing the utilization of agricultural data information, and the deep learning method can more effectively extract abstract information from big data and convert it into useful knowledge, thus supporting the development of intelligent agriculture from different dimensions. In this paper, a CNN-RNN model is constructed based on cloud computing technology, and the parallel neural network model divided by training set is adopted to design the batch gradient descent algorithm based on deep unsupervised learning and BP algorithm based on Map-Reduce. An experiment verifies the feasibility of deep unsupervised learning neural network based on cloud computing and verifies that the optimize algorithm proposed in this paper can better increase the training efficiency of neural network.","","978-1-7281-1307-4","10.1109/ICITBS.2019.00168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8669630","Neural network;deep learning;unsupervised learning;cloud computing;big data","Training;Big Data;Cloud computing;Neural networks;Unsupervised learning;Computational modeling;Data models","","14","","7","IEEE","21 Mar 2019","","","IEEE","IEEE Conferences"
"An Imbalanced Big Data Mining Framework for Improving Optimization Algorithms Performance","E. M. Hassib; A. I. El-Desouky; E. -S. M. El-Kenawy; S. M. El-Ghamrawy","Computer Engineering and Systems Department, Faculty of Engineering, Mansoura University, Mansoura, Egypt; Computer Engineering and Systems Department, Faculty of Engineering, Mansoura University, Mansoura, Egypt; Department of Computer and Systems Engineering, Delta Higher Institute for Engineering and Technology (DHIET), Mansoura, Egypt; Head of Communications and Computer Engineering Department, MISR Higher Institute for Engineering and Technology, Mansoura, Egypt",IEEE Access,"5 Dec 2019","2019","7","","170774","170795","Big data is an important factor almost in all nowadays technologies, such as, social media, smart cities, and internet of things. Most of standard classifiers tends to be trapped in local optima problem when dealing with such massive datasets. Hence, investigating new techniques for dealing with such massive data sets is required. This paper presents a novel imbalanced big data mining framework for improving optimization algorithms performance by eliminating the local optima problem consists of three main stages. Firstly, the preprocessing stage, which uses the LSH-SMOTE algorithm for solving the class imbalance problem, then it uses the LSH algorithm for hashing the data set instances into buckets. Secondly, the bucket search stage, which uses the GWO for training bidirectional recurrent neural network BRNN and searching for the global optimum in each bucket. Lastly, the final tournament winner stage, which uses the GWO+BRNN for finding the global optimum of the whole data set among all global optimums from all buckets. Our proposed framework LSHGWOBRNN has been tested over 9 data sets one of them is big data set in terms of AUC, MSE, against seven well-known machine-learning algorithms (Naive Bayes, Random Tree, Decision Table, and AdaBoostM1, WOA+MLP, GWO+MLP, and WOA+BRNN), then, we tested our algorithm over four well-known data sets against GWO+MLP, ACO+MLP, GA+MLP, PSO+MLP, PBIL+MLP, and ES+MLP in terms of classification accuracy and MSE. Our experimental results have proved that our proposed framework LSHGWOBRNN has provided high local optima avoidance, and higher accuracy, less complexity and overhead.","2169-3536","","10.1109/ACCESS.2019.2955983","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8913526","Grey wolf optimizer;neural network;big data mining;deep learning;imbalanced data sets;optimization","Big Data;Classification algorithms;Neurons;Data mining;Optimization;Deep learning;Social networking (online)","","39","","72","CCBY","26 Nov 2019","","","IEEE","IEEE Journals"
"A new Algorithm based on the Gbest of Particle Swarm Optimization algorithm to improve Estimation of Distribution Algorithm","Q. Zhao; Y. Gao","Computer science and education software institute Guangzhou University, China; Computer science and education software institute Guangzhou University, China",2018 International Conference on Smart Computing and Electronic Enterprise (ICSCEE),"18 Nov 2018","2018","","","1","5","In recent years, with the rise of artificial intelligence and deep learning, as an evolutionary algorithm based on probability model, estimation of distribution algorithm has been widely research and development. The estimation of distribution algorithm without the traditional genetic operation such as crossover and mutation, is a new kind of evolution model. As an algorithm based on probabilistic mode, the estimation of distribution algorithm establishes a probabilistic model describing the solution space of optimization problems. With the emergence for big data, the convergence of the algorithm and the requirements for solving precision are also increasing. This paper attempts to improve the distribution estimation algorithm. The optimal population of each iteration is found through the location update of each iteration of the Particle Swarm Optimization (PSO) algorithm. The simulation test was carried out with ten benchmark test function. The proposed algorithm was compared with the GA_EDA9improved genetic algorithm) and the basic distribution estimation (EDA) algorithm. Experimental results show that the new algorithm is superior to GA_EDA and basic EDA in terms of convergence and accuracy.","","978-1-5386-4838-4","10.1109/ICSCEE.2018.8538372","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8538372","Estimation of distribution algorithm;particle Swarm Optimization algorithm;simulation","Estimation;Sociology;Statistics;Particle swarm optimization;Optimization;Software algorithms;Simulation","","4","","18","IEEE","18 Nov 2018","","","IEEE","IEEE Conferences"
"Review on Analog Circuit Optimization Using Deep Learning Algorithm","A. A. Elwehili; D. B. Aissa","Microwave Electronics research laboratory, Faculty of Sciences of Tunis, Tunis El-Manar University, Tunis, Tunisia; Microwave Electronics research laboratory, Faculty of Sciences of Tunis, Tunis El-Manar University, Tunis, Tunisia",2023 22nd Mediterranean Microwave Symposium (MMS),"16 Feb 2024","2023","","","1","5","Within many circumstances, analog circuits are made up of intricate pairings of op-amps, resistors, capacitors, and several other fundamental electrical devices. Making anything as completely flawless, usable, or beneficial as feasible is known as optimization. Recent developments in very large-scale integration (VLSI) techniques have made it possible to create sophisticated integrated circuits and systems. When it comes to aspects and areas in wide variations, analog devices are a significant part of integrated systems. On the other hand, analog devices play an essential role in modern system architecture. In this article, we reviewed a wide range of different optimization approaches in deep learning (DL), analog circuit optimization, and its relevant applications. In addition, significant criteria such as efficiency, latency, performance, and accuracy are being considered when confirming the outcome. As a direct consequence of this, the efficiency of deep learning optimization for analog circuits may be evaluated.","2157-9830","979-8-3503-0847-1","10.1109/MMS59938.2023.10421610","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10421610","Analog circuit;deep learning;very large-scale integration (VLSI);efficiency;latency;performance;accuracy","Deep learning;Resistors;Systems architecture;Analog circuits;Very large scale integration;Microwave filters;Optimization","","","","30","IEEE","16 Feb 2024","","","IEEE","IEEE Conferences"
"Adaptive Deep Learning Model for Air Pollution Analysis Using Meteorological Big Data","S. Suganya; T. Meyyappan","Department of Computer Science, Alagappa University, Karaikudi, India; Department of Computer Science, Alagappa University, Karaikudi, India","2021 2nd International Conference on Communication, Computing and Industry 4.0 (C2I4)","28 Jan 2022","2021","","","1","6","To successfully control and prevent air pollution, it is important to focus on mandatory variables of air quality. Various past choices have explored the links between air pollution and related factors. Nevertheless, the techniques now used do not deal well with the problem of multipolarity or refuse to clarify the importance of mandatory variables. Moreover, much of the current writing restricts their focus to a city or a small area and focuses on factors from a single angle. Hence, in this paper, Adaptive deep learning model for air pollution analysis using meteorological big data. The proposed deep learning model is consisting of Deep Recurrent Neural Network (DRNN) and Arithmetic Optimization Algorithm (AOA). Initially, the databases are collected from the open source system. After that, the data's send to the proposed deep learning model for prediction of air pollution. In the DRNN, the weights are selected by utilizing the AOA. The combined model of the adaptive deep learning model is utilized to identify the air pollution prediction. The proposed methodology is implemented in R and performance are evaluated by performance matrices such as accuracy, precision, recall, specificity and F_Measure. The proposed methodology is compared with the conventional methods such as Particle Swarm Optimization (PSO) and Genetic Algorithm (GA) respectively.","","978-1-6654-2013-6","10.1109/C2I454156.2021.9689298","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9689298","Deep learning model;Meteorological big data;Air pollution analysis;Particle swarm optimization;Genetic algorithm and Arithmetic optimization algorithm","Deep learning;Adaptation models;Analytical models;Atmospheric modeling;Predictive models;Big Data;Air pollution","","2","","20","IEEE","28 Jan 2022","","","IEEE","IEEE Conferences"
"A Fully Streaming Big Data Framework for Cyber Security Based on Optimized Deep Learning Algorithm","N. Hussen; S. M. Elghamrawy; M. Salem; A. I. El-Desouky","Computers Engineering and Systems Department, Mansoura University, Mansoura, Egypt; Computer Engineering Department, Misr Higher Institute for Engineering and Technology, Mansoura, Egypt; Computers Engineering and Systems Department, Mansoura University, Mansoura, Egypt; Computers Engineering and Systems Department, Mansoura University, Mansoura, Egypt",IEEE Access,"7 Jul 2023","2023","11","","65675","65688","Real-time deep learning faces the challenge of balancing accuracy and time, especially in cybersecurity where intrusion detection is crucial. Traditional deep learning techniques have been insufficient in identifying network anomalies and intrusions. To address this, a Fully Streaming Big Data Framework based on optimized Deep Learning for cybersecurity (FSBDL) was proposed. The framework leverages two parallel optimization algorithms, Adam and RMSprop, labeled Hyper-parallel optimization (HPO) techniques to enhance efficiency and stability. The optimized CNN in the proposed framework achieves high accuracy in real-time intrusion detection without compromising reliability. The CNN is customized to address overfitting issues in recurrent connections by reducing the number of training parameters, using customized activation functions and regularization techniques. The CNN is trained in parallel using Adam and RMSprop optimization algorithms, resulting in significant accuracy improvements that surpass traditional methods and current state-of-the-art approaches. The HPO is a crucial component of the proposed framework, as it enables the system to detect intrusions in real-time, ensuring prompt response to potential cyber threats. The six-layer FSBDL framework includes data pre-processing, feature selection, hyper-parallelism, a customized CNN, a GUI layer for interpretation, and a detection-evaluation layer. The optimized CNN was designed to detect intrusions in real-time without compromising accuracy or reliability. The proposed algorithms were evaluated using various performance metrics, showing that the accuracy of the framework surpasses 99.9%, indicating its superiority over other intrusion detection models. This novel intrusion detection model offers promising prospects for cybersecurity, and its effectiveness and accuracy have been demonstrated through experimentation.","2169-3536","","10.1109/ACCESS.2023.3281893","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10141602","Cyber security;streaming data;intrusion detection;deep learning;conventional neural network (CNN);optimization","Optimization;Deep learning;Feature extraction;Real-time systems;Intrusion detection;Computer security;Data models;Neural networks","","4","","54","CCBYNCND","1 Jun 2023","","","IEEE","IEEE Journals"
"A Intelligent CNN-BiLSTM Approach for Chinese Sentiment Analysis on Spark","R. Cai; Q. Tao","School of Software Engineering, South China University of Technology, Guangzhou, Guangdong, P. R. China; School of Software Engineering, South China University of Technology, Guangzhou, Guangdong, P. R. China",2020 IEEE 6th International Conference on Computer and Communications (ICCC),"12 Feb 2021","2020","","","1689","1693","Short text Chinese sentiment classification has become an important task in sentiment analysis fields. In recent years, deep learning-based methods have been widely used in sentiment classification. However, with the complexity of deep learning models, the number of model parameters has increased. The effectiveness of the model depends largely on the choice of hyperparameter combinations. The optimization of model parameters becomes a tricky problem. In this paper, we propose an intelligent neural network sentiment classification approach that uses particle swarm optimization algorithm to optimize the parameters of the neural network model (CNN-BiLSTM). Meanwhile, in order to improve the calculation efficiency of the algorithm, we propose a parallel variant of particle swarm optimization algorithm, which can be iteratively calculated on the Spark distributed platform (big data technology). Experiments on Chinese sentiment analysis dataset validate the effectiveness of our approach.","","978-1-7281-8635-1","10.1109/ICCC51575.2020.9344910","National Key R&D Program of China(grant numbers:2019YFC1510400); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9344910","sentiment analysis;natural language processing;deep learning;particle swarm optimization algorithm;big data","Deep learning;Sentiment analysis;Analytical models;Computational modeling;Classification algorithms;Sparks;Particle swarm optimization","","1","","18","IEEE","12 Feb 2021","","","IEEE","IEEE Conferences"
"Identification of Papilionidae Species in Yunnan Province Based on Deep Learning","M. Fan; Y. Lu; Q. Xu; H. Zhang; J. Chang; W. Deng","College of Big Data and Intelligent Engineering, Southwest Forestry University, Kunming, China; College of Big Data and Intelligent Engineering, Southwest Forestry University, Kunming, China; College of Big Data and Intelligent Engineering, Southwest Forestry University, Kunming, China; College of Big Data and Intelligent Engineering, Southwest Forestry University, Kunming, China; College of Big Data and Intelligent Engineering, Southwest Forestry University, Kunming, China; College of Big Data and Intelligent Engineering, Southwest Forestry University, Kunming, China","2022 7th International Conference on Image, Vision and Computing (ICIVC)","19 Sep 2022","2022","","","611","614","Yunnan is known as the ""Hometown of Butterflies"" in China. The colorful and morphological diversity of the Papilioidae in Yunnan province is the subject of insect ecology and evolution research. At the same time, the Yunnan Papilio has great ornamental value. It is of great significance to accurately identify the species of Papilionidae in Yunnan Province. At present, Yunnan Papilio has not been classified in the related research on butterfly identification using deep learning methods, and there is a situation that the sample data set between species is small and the number is unbalanced, which may cause the model to fail to learn the morphological characteristics of butterflies. In response to the above problems, this study established a data set consisting of 12,956 original images of papilionidae from Yunnan Province, including two subfamilies, 12 genera and 80 species. Five deep learning network models (VGG-19, ResNet-34, ResNet-50, ResNet-101 and DenseNet-121) were explored from the perspective of prediction accuracy and loss value by transfer learning method. And modeling effects of SGD, Adam, Adamax and RMsprop optimization algorithms. The final data set adopts balanced sampling and 11 data enhancement methods for data fusion to expand the data set to 16,000 images. The ResNet-50 network structure optimized by Adamax algorithm is selected to achieve the optimal effect. The experimental results show that the recognition accuracy of ResNet-50 in the constructed model reaches 87.47%. The study provides a basis for constructing a visual recognition model of Papilioidae in Yunnan and applying it to the mobile terminal, and provides a fast and efficient new method for species identification of Papillidae in Yunnan. (Abstract)","","978-1-6654-6734-6","10.1109/ICIVC55077.2022.9886389","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9886389","Papilionidae from Yunnan province;deep learning;the butterfly recognition;transfer learning;ResNet-50;Adamax","Deep learning;Visualization;Image recognition;Biological system modeling;Insects;Transfer learning;Predictive models","","1","","20","IEEE","19 Sep 2022","","","IEEE","IEEE Conferences"
"Research on Bearing Fault Diagnosis Base on Deep Learning","W. Xu","Shanghai Co., Ltd., China Coal Technology and Engineering Group, Shanghai, China",2021 4th International Conference on Artificial Intelligence and Big Data (ICAIBD),"28 Jun 2021","2021","","","261","264","In order to improve the diagnosis rate of bearing signals, the bearing fault diagnosis model ODCNN based on one-dimensional convolutional neural network is proposed to adapt to one-dimensional time-domain signals on the basis of AlexNet. The model is composed of a multilevel alternating convolutional layer and a pooling layer, which can complete the adaptive extraction of the original input signal features, and combine the fully connected layer to realize fault classification and recognition. Support vector machine, convolutional neural network classic AlexNet architecture, LeNet-5, particle swarm optimization algorithm support vector machine and BP neural network are introduced for comparison and verification by using the Shanghai Tiandi bearing test data set for fault diagnosis test. The results indicate that the algorithm proposed in this paper has high recognition accuracy. Finally, Principal component analysis is used to verify the model's feature learning and classification capabilities of vibration signals.","","978-1-6654-1515-6","10.1109/ICAIBD51990.2021.9459073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9459073","deep learning;rotating machinery;fault diagnosis","Fault diagnosis;Support vector machines;Deep learning;Adaptation models;Convolution;Neural networks;Feature extraction","","5","","16","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"A Deep Learning H2O Framework for Emergency Prediction in Biomedical Big Data","A. S. Elsayad; A. I. E. Desouky; M. M. Salem; M. Badawy","Department of Computer Engineering and Systems, Faculty of Engineering, Mansoura University, Mansoura, Egypt; Department of Computer Engineering and Systems, Faculty of Engineering, Mansoura University, Mansoura, Egypt; Department of Computer Engineering and Systems, Faculty of Engineering, Mansoura University, Mansoura, Egypt; Department of Computer Engineering and Systems, Faculty of Engineering, Mansoura University, Mansoura, Egypt",IEEE Access,"2 Jun 2020","2020","8","","97231","97242","Recently, the design and implementation of new healthcare systems have gained an interest in both industry and academia. The amalgamation between the Internet of Things, cloud, edge computing, and big data helps the proliferation of new scenarios for smart medical services and applications. Deep learning is currently paying a lot of attention for its utilization with big healthcare data. To this end, the main objective of this study is to propose a Deep Learning H2O (DLH2O) framework for improving the performance and selection of the optimal features to predict emergency cases. The proposed DLH2O framework consists of data preprocessing layer, feature selection layer and deep learning layer. The DLH2O framework aims to find the optimal subset of features and minimize the error of the classification through a proposed new variant of the Whale Optimization Algorithm (WOA) called ACP-WOA. The proposed changes have been done on the following parameters a, a2, A, and C which should affect both exploration and exploitation of WOA. The experiments conducted in order to test the validity of DLH2O Framework. In regard to the datasets, five experiments are performed for this purpose. The results demonstrate the superiority of ACP-WOA compared to the other state-of-the-art meta-heuristic algorithms in terms of time, error, and scalability. The proposed ACP-WOA is also tested on CEC2017 benchmark functions and proves its superiority over WOA in terms of accuracy.","2169-3536","","10.1109/ACCESS.2020.2995790","U.S. Department of Commerce(grant numbers:BS123456); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9096313","Deep learning;H2O framework;healthcare;neural networks;optimization;whale optimizer","Whales;Water;Big Data;Machine learning;Optimization;Medical services;Feature extraction","","10","","30","CCBY","19 May 2020","","","IEEE","IEEE Journals"
"Classification of Massive Data Sets Using a Revolutionary Grey Wolf Optimization Algorithm and a Deep Learning Model in a Cloud-Based Setting","P. Anandan; A. Manju; M. R. Reddy","Department of Electronics and Communication Engineering, Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Chennai, Tamil Nadu, India; Department of Computing Technologies, School of Computing, College of Engineering and Technology, SRM Institute of Science & Technology, Chennai, Tamil Nadu, India; Technical Architect, HCL Technologies Ltd, Chennai, Tamil Nadu, India","2023 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)","4 Mar 2024","2023","","","1","6","The field of big data analytics has attracted a considerable amount of attention in the realm of academic research due to the fact that it is incredibly useful in a wide variety of real-time applications. This is the reason why the subject has received so much attention. The relatively recent development of machine learning and deep learning models has resulted in an improvement in performance. This improvement has been brought about as a result of the development of these models. The application of these models to the study of massive datasets has become much simpler as a result of these models, which has led to an improvement in performance outcomes. When considering the complexity of large data and the processing requirements that it imposes, it is beneficial to employ feature selection strategies that make use of metaheuristic optimization algorithms. This is due to the fact that big data is distinguished by the vast quantity of information that it contains. There is a huge advantage in the fact that these algorithms are able to successfully uncover the best potential set of features, which ultimately results in improved classification performance. In this particular piece of literature, a new approach that is known as the GWOA-DBN model is presented as a solution to the problem. In order to construct this model and the benefits that come along with it, the Grey Wolf Optimization algorithm and the optimal deep belief network have been coupled. The objective of the Apache Spark environment is to find a solution to the problem of classifying enormous volumes of data. This is the goal of the environment, which is the ultimate objective of the environment. The GWOA-DBN technique, which involves the construction of a feature selection method, is built on the base of the Grey Wolf Optimization Algorithm (GWOA), which acts as the cornerstone for the methodology. In the process of utilizing this strategy, the goal is to determine which subset of traits is the most ideal. The DBN-based classification model is also applied in order to appropriately categorize the large amounts of data into the precise categories that are necessary. This is done in order to fulfill the requirements. We aim to get the best possible results. The efficient processing of enormous data sets is another goal of using the Apache Spark platform. In order to provide the most optimal outcomes, this is carried out. This takes place in addition to the purpose that was specified earlier. A number of tests were carried out in an effort to improve the efficiency of the GWOA-DBN method on the whole. As a result of the results of these studies, it was proved that this method is more effective than other approaches.","","979-8-3503-4891-0","10.1109/ICDSAAI59313.2023.10452437","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10452437","Big data;Grey Wolf optimization;DBN Classifier;Cloud computing;Classification","Deep learning;Computational modeling;Cluster computing;Big Data;Feature extraction;Data models;Classification algorithms","","","","19","IEEE","4 Mar 2024","","","IEEE","IEEE Conferences"
"Communication Optimization Algorithms for Distributed Deep Learning Systems: A Survey","E. Yu; D. Dong; X. Liao","College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China",IEEE Transactions on Parallel and Distributed Systems,"7 Nov 2023","2023","34","12","3294","3308","Deep learning's widespread adoption in various fields has made distributed training across multiple computing nodes essential. However, frequent communication between nodes can significantly slow down training speed, creating a bottleneck in distributed training. To address this issue, researchers are focusing on communication optimization algorithms for distributed deep learning systems. In this paper, we propose a standard that systematically classifies all communication optimization algorithms based on mathematical modeling, which is not achieved by existing surveys in the field. We categorize existing works into four categories based on the optimization strategies of communication: communication masking, communication compression, communication frequency reduction, and hybrid optimization. Finally, we discuss potential future challenges and research directions in the field of communication optimization algorithms for distributed deep learning systems.","1558-2183","","10.1109/TPDS.2023.3323282","National Key Research and Development Program of China(grant numbers:2022YFB4501702); National Natural Science Foundation of China(grant numbers:62302512); Excellent Youth Foundation of Hunan Province(grant numbers:2021JJ10050); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10275049","Communication optimization algorithms;distributed computing;distributed deep learning;parallel algorithms","Training;Parallel processing;Optimization;Computational modeling;Deep learning;Computer architecture;Costs","","1","","126","IEEE","10 Oct 2023","","","IEEE","IEEE Journals"
"Parallel Optimization of Depth Learning Algorithm Based on Behavior Recognition","Y. Shi; B. Zhou; C. Li","Wuhan University of Science and Technology, Wuhan, Hubei, CN; Wuhan University of Science and Technology, Wuhan, Hubei, CN; Wuhan University of Science and Technology, Wuhan, Hubei, CN","2019 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)","21 Mar 2019","2019","","","660","664","In order to solve the problem of low parallel processing efficiency of current deep learning algorithm data, a parallel optimization method of deep learning algorithm based on behavior recognition is proposed. The behavior recognition method is used to extract learning morphological features, and the clustering specification parameters of deep learning features are calculated. The learning state feature values and clustering specification parameters are compared according to the calculation results, so as to effectively check out the heterogeneous data hidden in the parallel processing process. The parallel processing execution steps are optimized according to the parameters of the depth learning feature clustering specification, and the control flow of the parallel processing of the behavior depth learning is improved by planning the parallel rules of the depth learning algorithm, so as to finally achieve the research goal of the parallel optimization of the depth learning algorithm based on the behavior recognition. Finally, experiments show that the parallel optimization efficiency of the depth learning algorithm based on behavior recognition is significantly higher than that of the traditional depth learning algorithm.","","978-1-7281-1307-4","10.1109/ICITBS.2019.00164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8669539","Behavior recognition;In-depth study;Feature extraction","Optimization;Feature extraction;Clustering algorithms;Parallel processing;Deep learning;Character recognition;Neural networks","","1","","9","IEEE","21 Mar 2019","","","IEEE","IEEE Conferences"
"A Parameter-Optimized CNN Using WOA and Its Application in Fault Diagnosis of Bearing","Y. Cui; S. Zhang; Z. Zhang","College of Intelligent Manufacturing Modern Industry (College of Mechanical Engineering), Xinjiang University, Urumqi, China; School of Mechanical Engineering and Automation, Huaqiao University, Xiamen, China; School of Mechatronic Engineering and Automation, Foshan University, Foshan, China","2023 IEEE 2nd International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)","10 Apr 2023","2023","","","402","406","Traditional bearing fault diagnosis is usually foundation on expert experience or manual selection. This diagnosis method has weak feature information and low Signal To Noise Ratio (SNR), which may cause the loss or redundancy of fault features; To solve these problems, this paper presents a training method of bearing fault diagnosis model based on Convolutional Neural Network (CNN). Firstly, the original signal is transformed into a two-dimensional wavelet time-frequency transform to build a data set. Then, with the test error as the target, five population optimization algorithms such as WOA are used to search for the best parameter model of CNN. Finally, the AlexNet model is set with the best parameters. The network is trained and tested to creat a bearing fault diagnosis model. The experimental results show that this method can effectively enhance the adaptive feature extraction capabilities of the model and the troubleshooting accuracy, and has good generalization performance. Using WOA algorithm can not only complete the troubleshoot task of rolling bearing, but also its detection speed and accuracy are higher than other traditional diagnosis methods and other group optimization algorithms.","","978-1-6654-6253-2","10.1109/EEBDA56825.2023.10090663","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10090663","bearing fault diagnosis;Convolution neural network;Group optimization algorithm","Fault diagnosis;Training;Wavelet transforms;Adaptation models;Time-frequency analysis;Classification algorithms;Convolutional neural networks","","1","","15","IEEE","10 Apr 2023","","","IEEE","IEEE Conferences"
"Optimization Strategy of Classification Model Based on Weighted Implicit Optimal Extreme Learning Machine","N. Zhang; X. Wang","School of Computer and Artificial Intelligence, Henan Finance University, Zhengzhou, China; Graduate School of Business (GSB), SEGi University, Kota Damansara, Petaling Jaya, Malaysia",IEEE Access,"30 May 2024","2024","12","","74169","74184","Classification algorithms are one of the important research topics in the artificial intelligence, widely applied in various scientific and engineering fields. Extreme learning machine is a single hidden layer feed-forward neural network algorithm. Compared with traditional neural network models, the training speed and the generalization ability are also better. In terms of methodology, this study first innovatively improves the traditional Grey Wolf Optimization (GWO) algorithm to enhance its convergence and search ability. Specific improvement measures include implementing the reverse learning strategy to reduce the initial dependence of the algorithm on population distribution, and adding exploration perception strategy to enhance its global search ability by calculating heuristic factors, so as to identify the global optimal solution more effectively. The results showed that the improved W-DH-ELM model had excellent performance on multiple standard data sets. In particular, the average accuracy was more than 90%, which was significantly higher than other benchmark classification models. In terms of operation efficiency, the running time of the new model on different data sets was significantly reduced, accounting for less than 25%, and the lowest running time was only 4.89%. These experimental results verify the effectiveness of the introduced intelligent optimization algorithm in improving the performance of traditional ELM model without changing the original model structure. The improved W-DH-ELM model not only maintains the fast training performance of ELM, but also has higher accuracy and stability, which shows its superiority in dealing with complex classification tasks. In summary, the weighted two-hidden layer extreme learning machine optimized by the improved GWO proposed in this study has significant advantages in classification problems, providing a new perspective for future machine learning applications and research.","2169-3536","","10.1109/ACCESS.2024.3404606","Science and Technology Research Project of Henan Provincial Department of Science and Technology: Research on Big Data Analysis and Mining Based on Deep Learning(grant numbers:212102210501); Educational Teaching Reform and Practice Project of Henan Province: Construction and Practice of Financial Corpus Based on Industry-Education Integration in Finance and Economics Colleges(grant numbers:2021SJGLX599); Key Research Project of Higher Education Institutions in Henan Province: Key Technology Research on Multi-Source Information Fusion for Educational Big Data(grant numbers:22A520017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10537182","Two-hidden layer extreme learning machine;Grey wolf optimization algorithm;classification model;reverse learning;perception strategy","Optimization;Classification algorithms;Convergence;Predictive models;Heuristic algorithms;Prediction algorithms;Feature extraction;Learning systems;Reverse logistics","","","","31","CCBYNCND","23 May 2024","","","IEEE","IEEE Journals"
"Toward Optimally Efficient Search With Deep Learning for Large-Scale MIMO Systems","L. He; K. He; L. Fan; X. Lei; A. Nallanathan; G. K. Karagiannidis","School of Computer Science and Cyber Engineering, Guangzhou University, Guangzhou, China; School of Computer Science and Cyber Engineering, Guangzhou University, Guangzhou, China; School of Computer Science and Cyber Engineering, Guangzhou University, Guangzhou, China; School of Information Science and Technology, Institute of Mobile Communications, Southwest Jiaotong University, Chengdu, China; School of Electronic Engineering and Computer Science, Queen Mary University of London, London, U.K; Wireless Communications and Information Processing Group (WCIP), Aristotle University of Thessaloniki, Thessaloniki, Greece",IEEE Transactions on Communications,"17 May 2022","2022","70","5","3157","3168","This paper investigates the optimal signal detection problem with a particular interest in large-scale multiple-input multiple-output (MIMO) systems. The problem is NP-hard and can be solved optimally by searching the shortest path on the decision tree. Unfortunately, the existing optimal search algorithms often involve prohibitively high complexities, which indicates that they are infeasible in large-scale MIMO systems. To address this issue, we propose a general heuristic search algorithm, namely, hyper-accelerated tree search (HATS) algorithm. The proposed algorithm employs a deep neural network (DNN) to estimate the optimal heuristic, and then use the estimated heuristic to speed up the underlying memory-bounded search algorithm. This idea is inspired by the fact that the underlying heuristic search algorithm reaches the optimal efficiency with the optimal heuristic function. Simulation results show that the proposed algorithm reaches almost the optimal bit error rate (BER) performance in large-scale systems, while the memory size can be bounded. In the meanwhile, it visits nearly the fewest tree nodes. This indicates that the proposed algorithm reaches almost the optimal efficiency in practical scenarios, and thereby it is applicable for large-scale systems. Besides, the code for this paper is available at https://github.com/skypitcher/hats.","1558-0857","","10.1109/TCOMM.2022.3158367","NSFC(grant numbers:61871139,62101145); Natural Science Foundation of Guangdong Province(grant numbers:2021A1515011392); research program of Guangzhou University(grant numbers:YJ2021003); National Key Research and Development Program of China(grant numbers:2019YFB1803400); National Natural Science Foundation of China(grant numbers:61971360); Fundamental Research Funds for the Central Universities(grant numbers:XJ2021KJZK007); open research fund of National Mobile Communications Research Laboratory, Southeast University(grant numbers:2021D05); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9732440","Signal detection;integer least-squares;deep learning;maximum likelihood detection;MIMO;sphere decoding;best-first search","MIMO communication;Search problems;Complexity theory;Deep learning;Signal detection;Lattices;Heuristic algorithms","","18","","43","IEEE","10 Mar 2022","","","IEEE","IEEE Journals"
"A Patch Based 3D CNN Approach for Diagnosing Early Stages of Alzheimer’s Disease by Applying OBL-WOA Algorithm","R. Kumari; S. Goel; S. Das","School of Computer Science and Engineering and Technology, Bennett University, Greater Noida, Uttar Pradesh, India; School of Computer Science and Engineering and Technology, Bennett University, Greater Noida, Uttar Pradesh, India; Deapartment of EEE, BIT MESRA, Ranchi, Jharkhand, India",2023 8th International Conference on Cloud Computing and Big Data Analytics (ICCCBDA),"26 Jun 2023","2023","","","455","459","Alzheimer’s disease (AD) is one of the most degraded neurodegenerative brain disorders, with no treatments involved. Understanding the early stages of Alzheimer’s disease would be necessary for treating the disease and preventing further Tumour Necrosis Factor α (TNF-α) degeneration cells. Previous studies demonstrated the application of deep learning techniques for distinguishing AD from Normal Control (NC) by applying T1-weighted MRI images. This paper proposes a novel Patch Based Convolutional Neural Network (PB-CNN) network for classifying three binary classifications. A new optimization technique, the Opposition Based Learning- Whale Optimization Algorithm (OBL-WOA), has been proposed to update the weights of the proposed PB-CNN Network. 326 ADNI subjects are investigated for the feasibility of the proposed optimization technique, where the highest classification accuracy is achieved when compared with other state-of-the-art techniques. Moreover, the proposed technique could assist doctors in diagnosing the early stages of AD.","2832-3734","978-1-6654-5533-6","10.1109/ICCCBDA56900.2023.10154706","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10154706","Alzheimer Disease;Convolutional Neural Network;Magnetic Resonance Imaging;Whale Optimization","Deep learning;Three-dimensional displays;Magnetic resonance imaging;Medical services;Whale optimization algorithms;Classification algorithms;Convolutional neural networks","","1","","27","IEEE","26 Jun 2023","","","IEEE","IEEE Conferences"
"Efficiency Study of SSAG on RNN Framework","X. Xie; A. Chen","College of Statistics and Mathematics, Guangdong University of Finance & Economics, Guangzhou, China; College of Statistics and Mathematics, Guangdong University of Finance & Economics, Guangzhou, China","2022 IEEE 6th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC )","9 Nov 2022","2022","","","528","533","SGD (Stochastic gradient descent) is widely used in deep learning, however SGD cannot get linear convergence and is not effective in large amounts of data. This paper use SSAG to improve the efficiency. SSAG contains two optimization strategies, one is stratified sampling strategy and the other is historical gradient averaging strategy. It has the advantages of fast convergence of variance, flexible application to big data, and easy work in deep network. This paper studies the efficiency of SSAG gradient optimization algorithm based on RNN framework. The proposed RNN framework comprises a feature extraction layer, a stacked RNN layer, and a transcription layer. The experimental results confirm that the accuracy of SSAG is better than the SGD and the Momentum. Both stratified sampling and historical averaging strategies have the effect of improving task accuracy. Experimental results verified that SSAG has better effect in image classification task.","2689-6621","978-1-6654-5864-1","10.1109/IAEAC54830.2022.9929855","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9929855","SSAG;RNN framework;stratified sampling;historical gradient","Deep learning;Image recognition;Optimization methods;Big Data;Feature extraction;Classification algorithms;Task analysis","","","","15","IEEE","9 Nov 2022","","","IEEE","IEEE Conferences"
"Deep Convolutional Network for Citrus Leaf Diseases Recognition","Q. Chen; X. Liu; C. Dong; T. Tong; C. Yang; R. Chen; T. Zou; X. Yang","College of Computer and Information Sciences, Fujian Agriculture and Forestry University, Fuzhou, China; College of Computer and Information Sciences, Fujian Agriculture and Forestry University, Fuzhou, China; College of Computer and Information Sciences, Fujian Agriculture and Forestry University, Fuzhou, China; College of Physics and Information Engineering, Fuzhou University, Fuzhou, China; College of Computer and Information Sciences, Fujian Agriculture and Forestry University, Fuzhou, China; College of Computer and Information Sciences, Fujian Agriculture and Forestry University, Fuzhou, China; College of Mechanical and Electronic Engineering, Fujian Agriculture and Forestry University, Fuzhou, China; Fuzhou Yinfeng Huinong Technology Service Co., Ltd, Fuzhou, China","2019 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)","26 Mar 2020","2019","","","1490","1494","This paper explores the identification methods of three common citrus leaf diseases that are citrus canker, citrus scab and citrus anthracnose respectively. The traditional machine learning algorithms typically suffer from the problem of low recognition accuracy. In this paper, we propose a method based on deep convolutional network to solve this problem. The core idea is to build a 7-layer network structure, through which the main purpose is to extract rich features of citrus. These features are better than traditional features in identifying different categories of diseases, thus can improve the recognition accuracy. We propose a novel network which includes input layer, three convolutional layers, two fully connection layers and one output layer. The convolutional layer includes a convolutional operation and a pooling operation. The proposed method yields good results in the identification of citrus diseases. In order to show that our results are more accurate than those of two other common machine learning algorithms, we conduct three sets of experiments. Finally, experiments demonstrate that the proposed method is effective in identifying citrus diseases, which provides effective technical support for precise recognition and the prevention of citrus diseases.","","978-1-7281-4328-6","10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9047360","deep learning;citrus disease recognition;TensorFlow framework;CNN;machine learning","Diseases;Agriculture;Training;Feature extraction;Deep learning;Optimization;Forestry","","8","","19","IEEE","26 Mar 2020","","","IEEE","IEEE Conferences"
"Deep Neural Networks for traffic flow prediction","Hongsuk Yi; HeeJin Jung; Sanghoon Bae","Supercomputing Center, Korea Institute of Science and Technology Information, Daejeon, Republic of Korea; Supercomputing Center, Korea Institute of Science and Technology Information, Daejeon, Republic of Korea; Department of Spatial Information Engineering, Pukyung National University, Busan, Republic of Korea",2017 IEEE International Conference on Big Data and Smart Computing (BigComp),"20 Mar 2017","2017","","","328","331","Traffic flow prediction is an essential function of traffic information systems. Conventional approaches, using artificial neural networks with narrow network architecture and poor training samples for supervised learning, have been only partially successful. In this paper, a deep-learning neural-network based on TensorFlow™ is suggested for the prediction traffic flow conditions, using real-time traffic data. Until now, no research has applied the TensorFlow™ deep learning neural network model to the estimation of traffic conditions. The suggested supervised model is trained by a deep learning algorithm, which uses real traffic data aggregated every five minutes. Results demonstrate that the model's accuracy rate is around 99%.","2375-9356","978-1-5090-3015-6","10.1109/BIGCOMP.2017.7881687","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881687","traffic prediction;deep learning neural networks;transportation big data","Data models;Transportation;Machine learning;Analytical models;Real-time systems;Big Data;Optimization","","34","","8","IEEE","20 Mar 2017","","","IEEE","IEEE Conferences"
"T-Distributed Stochastic Neighbor Embedding with Gauss Initialization of Quantum Whale Optimization Algorithm","Z. Yang; Y. Sun; D. Li; Z. Zhang; Y. Xie","Faculty of Science, Tongji Zhejiang College, Jiaxing, P. R. China; Department of Electronics and Information Engineering, Tongji Zhejiang College, Jiaxing, P. R. China; Faculty of Science, Tongji Zhejiang College, Jiaxing, P. R. China; Department of Electronics and Information Engineering, Tongji Zhejiang College, Jiaxing, P. R. China; Department of Mechanical and Automobile Engineering, Tongji Zhejiang College, Jiaxing, P. R. China",2020 39th Chinese Control Conference (CCC),"9 Sep 2020","2020","","","3200","3205","T-distributed stochastic neighbor embedding is an important nonlinear dimensionality reduction algorithm in manifold learning, which has great application value in big data, data mining, machine learning, deep learning and other fields. The essence of its algorithm is to solve the minimum value problem of KL divergence. Because KL divergence is a convex function, gradient descent method or stochastic gradient descent method are often used to solve it. However, the global convergence of gradient dependent method is poor, so the main purpose of this paper is to introduce the quantum whale optimization algorithm with Gaussian initialization into the optimization part of t-SNE.","1934-1768","978-9-8815-6390-3","10.23919/CCC50068.2020.9189639","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9189639","t-distributed stochastic neighbor embedding;quantum whale optimization algorithm","Probability distribution;Training;Optimization;Dimensionality reduction;Gaussian distribution;Whales;Convergence","","2","","10","","9 Sep 2020","","","IEEE","IEEE Conferences"
"Machine Learning for Large-Scale Optimization in 6G Wireless Networks","Y. Shi; L. Lian; Y. Shi; Z. Wang; Y. Zhou; L. Fu; L. Bai; J. Zhang; W. Zhang","Department of Mobile Communications and Terminal Research, China Telecom Research Institute, Guangzhou, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Informatics and the Key Laboratory of Underwater Acoustic Communication and Marine Information Technology (Ministry of Education), Xiamen University, Xiamen, China; School of Cyber Science and Technology, Beihang University, Beijing, China; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China; School of Electrical Engineering and Telecommunications, The University of New South Wales, Sydney, NSW, Australia",IEEE Communications Surveys & Tutorials,"21 Nov 2023","2023","25","4","2088","2132","The sixth generation (6G) wireless systems are envisioned to enable the paradigm shift from “connected things” to “connected intelligence”, featured by ultra high density, large-scale, dynamic heterogeneity, diversified functional requirements, and machine learning capabilities, which leads to a growing need for highly efficient intelligent algorithms. The classic optimization-based algorithms usually require highly precise mathematical model of data links and suffer from poor performance with high computational cost in realistic 6G applications. Based on domain knowledge (e.g., optimization models and theoretical tools), machine learning (ML) stands out as a promising and viable methodology for many complex large-scale optimization problems in 6G, due to its superior performance, computational efficiency, scalability, and generalizability. In this paper, we systematically review the most representative “learning to optimize” techniques in diverse domains of 6G wireless networks by identifying the inherent feature of the underlying optimization problem and investigating the specifically designed ML frameworks from the perspective of optimization. In particular, we will cover algorithm unrolling, learning to branch-and-bound, graph neural network for structured optimization, deep reinforcement learning for stochastic optimization, end-to-end learning for semantic optimization, as well as wireless federated learning for distributed optimization, which are capable of addressing challenging large-scale problems arising from a variety of crucial wireless applications. Through the in-depth discussion, we shed light on the excellent performance of ML-based optimization algorithms with respect to the classical methods, and provide insightful guidance to develop advanced ML techniques in 6G networks. Neural network design, theoretical tools of different ML methods, implementation issues, as well as challenges and future research directions are also discussed to support the practical use of the ML model in 6G wireless networks.","1553-877X","","10.1109/COMST.2023.3300664","National Nature Science Foundation of China(grant numbers:62101331); Natural Science Foundation of Shanghai(grant numbers:21ZR1442700); National Nature Science Foundation of China(grant numbers:62271318); Shanghai Rising-Star Program(grant numbers:22QA1406100); National Natural Science Foundation of China(grant numbers:U20A20159,62001294); Natural Science Foundation of Shanghai(grant numbers:23ZR1442800); Research Impact Fund (RIF) from Hong Kong RGC(grant numbers:R5009-21); Special Project for Research and Development in Key areas of Guangdong Province(grant numbers:2020B0101110003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10198239","Large-scale optimization;machine learning;deep neural network;6G;large-scale networks;wireless communications;learning to optimize;non-convex optimization","Optimization;6G mobile communication;Wireless networks;Machine learning algorithms;Matching pursuit algorithms;Machine learning;Mathematical models","","20","","287","IEEE","1 Aug 2023","","","IEEE","IEEE Journals"
"Research on Cross-media Semantic Retrieval Methods of Information Resources Based on Deep Learning","Y. Zhu","Library Changchun University Of Technology, Changchun, China",2019 14th International Conference on Computer Science & Education (ICCSE),"23 Sep 2019","2019","","","340","344","This paper is to study cross-media semantic retrieval method of information resources based on deep learning. By analyzing the concept of deep learning, the depth structure and the prerequisites for deep learning, this paper studies the relationship between deep learning and information resources cross-media semantic retrieval, and points out the cross-media correlation learning technology of information resources on the basis of depth structure, buildings a distinct framework for information retrieval. As a new information retrieval model, the combination of deep learning and cross-media semantic retrieval can solve the problems of retrieving semantic information across the media and processing complex dimensional data, greatly improving the efficiency of data retrieval and integration. This model will replace the existing information retrieval tools, to become a sword to enhance the level of knowledge service in the era of big data.","2473-9464","978-1-7281-1846-8","10.1109/ICCSE.2019.8845474","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8845474","deep learning;information resources;crossmedia;semantic retrieval","Semantics;Deep learning;Correlation;Feature extraction;Data models;Data mining;Computational modeling","","","","15","IEEE","23 Sep 2019","","","IEEE","IEEE Conferences"
"Early Screening Prediction System of Breast Cancer MRI Images Based on Deep Learning","L. Xu; L. Liu","Department of Computer Science and Technology, Baotou Medical College, Inner Mongolia University of Science and Technology, Baotou, China; Department of Computer Science and Technology, Baotou Medical College, Inner Mongolia University of Science and Technology, Baotou, China","2024 IEEE 3rd International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)","8 Apr 2024","2024","","","1459","1463","With the increasing incidence rate of breast cancer in China, how to effectively improve the accuracy and sensitivity of breast cancer diagnosis has become an important issue facing the Chinese medical industry. In recent years, the application of computer deep learning technology in medical images has become increasingly widespread. It can automatically extract features from images and classify and predict them. At present, deep learning has made significant breakthroughs in the field of medical image analysis. Deep learning algorithms can automatically discover disease features in massive image data, and their performance has also been significantly improved. The purpose of this study was to study and analyze breast cancer MRI (Magnetic Resonance Imaging) images by using deep learning algorithm, hoping to provide help for early diagnosis of breast cancer. The results showed that the accuracy of the early screening prediction system for breast cancer MRI images based on deep learning reached 95.6%, which laid the foundation for the early diagnosis and treatment of breast cancer.","","979-8-3503-8098-9","10.1109/EEBDA60612.2024.10485978","Inner Mongolia Natural Science Foundation of China(grant numbers:2023LHMS06001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10485978","Breast Cancer Prediction;Deep Learning;Magnetic Resonance Imaging;Early Screening Prediction System","Deep learning;Industries;Sensitivity;Magnetic resonance imaging;Medical services;Manuals;Prediction algorithms","","","","16","IEEE","8 Apr 2024","","","IEEE","IEEE Conferences"
"Online Aid For Detecting Brain Tumor And Tuberculosis Using Deep Learning","V. Mercy Rajaselvi Beaulah; U. Brinda; R. Adithya; S. Akshey","Computer Science and Engineering, Easwari Engineering College, Chennai, India; Computer Science and Engineering, Easwari Engineering College, Chennai, India; Computer Science and Engineering, Easwari Engineering College, Chennai, India; Computer Science and Engineering, Easwari Engineering College, Chennai, India","2021 International Conference on Forensics, Analytics, Big Data, Security (FABS)","9 Feb 2022","2021","1","","1","8","Artificial intelligence(AI) is transforming the healthcare industry in a variety of ways, including disease diagnosis through medical imaging, increasing overall hospital efficiency, and so on. AI in health care will play a significant role in lowering death rates and thereby enhancing the nation’s overall medical infrastructure and standards. From its present valuation of $4.9 billion USD, the AI healthcare market is predicted to reach $45.2 billion USD by 2026. The absence of well-experienced medical experts and diagnosticians, as well as the expense and time required to diagnose and detect the illness, are major risk factors in particular underdeveloped countries and underrated areas. In developing countries across the world, the shortage of trained healthcare personnel can severely limit access to life-saving care. In the event of a positive diagnostic, the patient’s risk ultimately rises as time passes. As a result, we require an intelligent system capable of overcoming all of the aforementioned risks. Artificial intelligence (AI) may help alleviate the effect of the extreme shortage of trained healthcare personnel by taking over some of the diagnostic duties performed by them. Deep learning is shown to be superior at detecting diseases using X-rays, MRI scans, and CT scans, which could help doctors, diagnose patients faster and more accurately. The convolutional neural networks like Residual network (ResNet), MobileNetV2 are used for disease detection. These trained models are deployed in an application available to users all over the world even in low-resource areas, minimizing the requirement for an on-site certified diagnostic radiologist. In a better perspective, these applications can assist medical practitioners and diagnosticians in making more accurate clinical judgments, hence lowering the rate of medical error and increasing efficiency.","","978-1-6654-2005-1","10.1109/FABS52071.2021.9702602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9702602","Brain Tumor;Tuberculosis;Deep learning;ResNet;MobileNetV2","Deep learning;Medical services;X-rays;Personnel;Security;Medical diagnostic imaging;Standards","","","","10","IEEE","9 Feb 2022","","","IEEE","IEEE Conferences"
"splitDyn: Federated Split Neural Network for Distributed Edge AI Applications","T. A. Khoa; D. -V. Nguyen; M. -S. Dao; K. Zettsu","Big Data Integration Research Center, National Institute of Information and Communications Technology, Tokyo, Japan; Big Data Integration Research Center, National Institute of Information and Communications Technology, Tokyo, Japan; Big Data Integration Research Center, National Institute of Information and Communications Technology, Tokyo, Japan; Big Data Integration Research Center, National Institute of Information and Communications Technology, Tokyo, Japan",2022 IEEE International Conference on Big Data (Big Data),"26 Jan 2023","2022","","","6066","6073","Split learning (SL) is a popular distributed machine learning (ML) method used to enable ML. It divides a neural network based model into subnetworks. Then, it separately trains the subnetworks on distributed parties (e.g., client and server). In distributed ML, data are generated and collected on the client-side. In contrast, the collected data are processed using an application deployed on the server side. However, when applied in practice using Internet of things systems and clients, numerous obstacles occur because of limited configuration and resources. Dividing neural networks in the SL is the biggest problem and an open question in numerous studies. This study introduces splitDyn, which is a new dynamic SL solution to solve the aforementioned problems. This method provides a solution for eliminating their inherent drawbacks. The main idea is to apply a Round-Robin schedule to select the client for the training process. Then, the next idea is to use the Hungarian optimization algorithm to assign a layer to a client and enhance the accuracy. The proposed method reasonably achieved better accuracy and reduced processing time than the other learning models. Furthermore, it applies the incident datasets to predict the incident event and in edge computing for edge artificial intelligence (AI) applications.","","978-1-6654-8045-1","10.1109/BigData55660.2022.10020803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10020803","Split learning;splitDyn;Machine learning;Edge Computing;Edge AI","Training;Temperature distribution;Schedules;Neural networks;Big Data;Prediction algorithms;Servers","","5","","17","IEEE","26 Jan 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Energy Efficiency Optimization in Wireless Networks","H. Fan; L. Zhu; C. Yao; J. Guo; X. Lu","College of Communication Engineering, Army Engineering University of PLA, Nanjing, China; College of Communication Engineering, Army Engineering University of PLA, Nanjing, China; College of Communication Engineering, Army Engineering University of PLA, Nanjing, China; College of Communication Engineering, Army Engineering University of PLA, Nanjing, China; The 799 Research Institution, Nanjing, hina",2019 IEEE 4th International Conference on Cloud Computing and Big Data Analysis (ICCCBDA),"30 May 2019","2019","","","465","471","We consider a joint decision-making problem of the transmission power and channel in wireless networks with unknown state switching patterns for improving energy efficiency (EE). To overcome the unknown dynamics of network, we model the problem as a sequential decision making process, and apply deep reinforcement learning (DRL), which aggregates reinforcement learning (RL) being appropriate for complex sequential decision problem and deep learning (DL) characterized by its advantages of learning feature and approximating the nonlinear function, for figuring out the difficulties of dealing the problem with massive state space and modeling the temporal characteristics between sequential decisions. Simulation results show the proposed algorithm performs efficiently in term of energy efficiency.","","978-1-7281-1410-1","10.1109/ICCCBDA.2019.8725683","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8725683","deep reinforcement learning;energy efficiency optimization;joint decision-making","Optimization;Energy efficiency;Interference;Wireless networks;Heuristic algorithms;Decision making;Receivers","","8","","24","IEEE","30 May 2019","","","IEEE","IEEE Conferences"
"A Fault Diagnosis Model Based on Kernel Auto-encoder and Improved Chaos Firefly Optimization Algorithm","W. Fengtao; L. Xiaofei; M. Linjie; D. Gang; H. Qingkai; L. Hongkun","Institute of Vibration Engineering, Dalian University of Technology, Dalian, China; Institute of Vibration Engineering, Dalian University of Technology, Dalian, China; Institute of Vibration Engineering, Dalian University of Technology, Dalian, China; Institute of Vibration Engineering, Dalian University of Technology, Dalian, China; Institute of Vibration Engineering, Dalian University of Technology, Dalian, China; Institute of Vibration Engineering, Dalian University of Technology, Dalian, China",2019 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),"9 Sep 2019","2019","","","1","6","Automatically extracting features from large scale raw data for fault diagnosis is important in the current era of big data. In this paper, a deep neural network based on the kernel function and denoising auto-encoder is proposed. The kernel denoising auto-encoder (KDAE) neural network consists of one KDAE layer and multiple auto-encoder (AE) layers to automatically extract the fault features from raw data. Then, the softmax classifier is added as classifier layer. The improved chaos firefly algorithm is used to optimize the undetermined parameters of the kernel function and the network to obtain the diagnosis model. The proposed method is then verified by the typical failure test data of the aero-engine intermediate bearing, which has achieved higher classification accuracy than the traditional denoising auto-encoder network.","2642-2077","978-1-5386-3460-8","10.1109/I2MTC.2019.8827055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8827055","features;kernel function;denoising auto-encoder;chaos firefly algorithm;diagnosis model","Kernel;Noise reduction;Feature extraction;Chaos;Optimization;Vibrations;Encoding","","","","15","IEEE","9 Sep 2019","","","IEEE","IEEE Conferences"
"Convolutional Neural Network Compression Based on Improved Fractal Decomposition Algorithm for Large Scale Optimization","A. Llanza; F. E. Keddous; N. Shvai; A. Nakib","Laboratoire LISSI, University Paris Est Creteil; Laboratoire LISSI, University Paris Est Creteil; Laboratoire LISSI, University Paris Est Creteil; Laboratoire LISSI, University Paris Est Creteil","2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","29 Jan 2024","2023","","","5084","5089","Deep learning methods have shown state-of-the-art results in various application areas such as computer vision, NLP, etc. However, their practical use presents many challenges, including those caused by the large size of the models, especially in the context of model weight storage and transmission. One of the possible solutions to this problem is Neural Network (NN) compression, which is a process of obtaining a derived model serving the same task with a smaller number of parameters or with parameters of lower precision. The most common NN compression techniques include pruning, sparse representation, quantization, and knowledge transfer. In this article, the compression of Convolutional Neural Networks (CNNs) using fractional differentiation is investigated. A for-mulation of this task as a large-scale continuous optimization problem is then proposed, and its resolution is performed through a new optimization algorithm, called the Improved Fractal Decomposition Algorithm (IFDA), based on space geometric fractal decomposition. The results obtained show that MobileNetV3, for instance, is compressed by 18.5% with only a 2.5% decrease in accuracy. Additionally, the proposed IFDA algorithm outperforms all other competing metaheuristics in solving this problem.","2577-1655","979-8-3503-3702-0","10.1109/SMC53992.2023.10394477","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10394477","","Machine learning algorithms;Metaheuristics;Fractals;Convolutional neural networks;Task analysis;Kernel;Context modeling","","1","","48","IEEE","29 Jan 2024","","","IEEE","IEEE Conferences"
"FOA Application in the GRNN Optimization for CBM Productivity Prediction","L. Yin; W. Yang; G. Yongwei; Z. Shu","Earthquake Administration of Henan Province, Zhengzhou, China; School Of Computer Science, Southwest Petroleum University, Chengdu, China; Earthquake Administration of Henan Province, Zhengzhou, China; School of Information, Southwest Petroleum University, Nanchong, China",2023 6th International Conference on Artificial Intelligence and Big Data (ICAIBD),"10 Aug 2023","2023","","","644","649","With the development of computer technology, the relationship between machine learning and the application of Coalbed Methane (CBM) gas has become closer, i.e., using machine learning to predict the CBM production capacity can not only improve the prediction accuracy but also significantly lower the prediction cost. This paper proposes an improved fruit fly optimization algorithm (FOA)-GRNN model for CBM production prediction based on the CBM production capacity classification. The proposed model can predict different production capacity types by integrating logging, fracturing, and drainage data of CBM wells. It employs the FOA to optimize the GRNN network smoothing factor to improve algorithm accuracy by improving the optimization target. The proposed model is verified by comparison experiments with linear regression, local weighted regression, RBF regression, traditional GRNN regression, and other algorithms on data obtained from high-, middle, and low-yield wells. The algorithms are compared regarding the regression evaluation indexes, and the results indicate that the improved FOA-GRNN model has better prediction accuracy and prediction effect than the other models. This study represents an interdisciplinary application and research of deep learning and CBM development. The main objective of this study is related to the CBM production prediction and the CBM productivity analysis and evaluation system design. The results presented in this study could help to improve the single-well gas production of CBM and also have high application value.","2769-3554","978-1-6654-9125-9","10.1109/ICAIBD57115.2023.10206369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10206369","improved FOA-GRNN;fruit fly optimization algorithm;CBM production prediction;smoothing factor;regression algorithm","Productivity;Methane;Smoothing methods;Machine learning algorithms;Predictive models;Prediction algorithms;Data models","","","","25","IEEE","10 Aug 2023","","","IEEE","IEEE Conferences"
"Emergency Drug Procurement Planning Based on Big-Data Driven Morbidity Prediction","Q. Song; Y. -J. Zheng; Y. -J. Huang; Z. -G. Xu; W. -G. Sheng; J. Yang","Scientific Research Institute, Hangzhou Normal University, Hangzhou, China; Hangzhou Institute of Service Engineering, Hangzhou Normal University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; Hangzhou Institute of Service Engineering, Hangzhou Normal University, Hangzhou, China; Medical College, Hangzhou Normal University, Hangzhou, China",IEEE Transactions on Industrial Informatics,"5 Dec 2019","2019","15","12","6379","6388","Due to the uncertainty of diseases, traditional approaches of drug procurement planning in hospitals often cause drug overstocking or understocking, which can have strong negative effects on healthcare services. This paper proposes a big-data driven approach, which uses a deep neural network to predict morbidities of acute gastrointestinal infections based on a huge amount of environmental data, and then constructs an optimization problem of drug procurement planning for maximizing the expected therapeutic effect on the predicted cases. The problem is solved by an efficient heuristic optimization algorithm. Computational experiments demonstrate the performance advantages of both the deep learning model and the heuristic algorithm over existing ones, and two real case studies in Central China show that the average prediction error of our approach is only 8% and the estimated recovery rate reaches 99%, much better than the currently used method. Our approach can also be extended for many other medical resource planning problems.","1941-0050","","10.1109/TII.2018.2870879","National Natural Science Foundation of China(grant numbers:61473263,61573316,61872123); Natural Science Foundation of Zhejiang Province(grant numbers:LY18F030023); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8466910","Big-data;drug procurement planning;deep learning;heuristic optimization;morbidity prediction;water wave optimization (WWO)","Drugs;Diseases;Procurement;Planning;Gastrointestinal tract;Predictive models;Optimization","","13","","48","OAPA","16 Sep 2018","","","IEEE","IEEE Journals"
"Sentiment Analysis of YouTube Video Comments with the Topic of Starlink Mission Using Long Short Term Memory","A. M. Putri; D. Ananda Putra Basya; M. T. Ardiyanto; I. Sarathan","Department of Chemistry, Research Center for Artificial Intelligence and Big Data, Padjadjaran University, Sumedang, Indonesia; Department of Computer Engineer, Telkom University, Bandung, Indonesia; Department Robotics and Artificial Intelligence Engineering, Airlangga University, Surabaya, Indonesia; Faculty of Cultural Studies, Padjadjaran University, Sumedang, Indonesia",2021 International Conference on Artificial Intelligence and Big Data Analytics,"1 Feb 2022","2021","","","28","32","The launch of the Starlink Satellite by a private company from the United States (SpaceX) has become a hot topic for conversation. SpaceX is a project that aims to provide satellite-based internet services worldwide with high performance and affordable prices even in remote places. The satellite launch video was shared on the YouTube platform. Many comments were on the satellite launch video, and various reactions were written on the video's comments column. Therefore, in this study, sentiment analysis will be carried out to analyze the responses of internet users through comments on YouTube to the launch of the Starlink satellite using deep learning. The YouTube comment data used in this study amounted to 22,000 comments. The model used is Long Short Term Memory (LSTM). This study will produce an accuracy value of the LSTM model by applying different activation and optimization functions. This study shows that the highest accuracy is 86% using the LSTM model, Softmax activation function, and Adam's optimization.","","978-1-6654-0890-5","10.1109/ICAIBDA53487.2021.9689718","Universitas Padjadjaran; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9689718","Deep Learning;LSTM;Sentiment Analysis;Starlink Mission;YouTube","Training;Deep learning;Sentiment analysis;Satellites;Social networking (online);Web and internet services;Companies","","","","18","IEEE","1 Feb 2022","","","IEEE","IEEE Conferences"
"An Intrusion Detection System Empowered by Deep Learning Algorithms","W. Tang; D. Li; W. Fan; T. Liu; M. Chen; O. Dib","department of computer science, wenzhou-kean university, Wenzhou, China; department of computer science, wenzhou-kean university, Wenzhou, China; department of computer science, wenzhou-kean university, Wenzhou, China; department of computer science, wenzhou-kean university, Wenzhou, China; department of computer science, wenzhou-kean university, Wenzhou, China; department of computer science, wenzhou-kean university, Wenzhou, China","2023 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)","25 Dec 2023","2023","","","1137","1142","The Industrial Internet of Things (IIoT) faces significant challenges in managing the increasing volume of network traffic, which calls for robust cybersecurity measures, especially in intrusion detection. An effective intrusion detection system (IDS) is essential to protect industrial systems from cyber attacks, enabling quick identification and response to security threats. However, traditional machine learning techniques have limitations in capturing complex patterns and correlations in the extensive data generated by IoT devices. Deep learning (DL) offers a promising alternative by employing multi-layer neural networks to extract hierarchical abstract features, resulting in improved accuracy and generalization. DL models handle large and complex datasets and address intricate problems, making them ideal for overcoming security challenges in cybersecurity domains. In this context, this paper explores the application of recent DL and optimization techniques in IIoT intrusion detection to develop more accurate, robust, and efficient models for threat detection. Experimental results on the recent EdgeIIoT dataset demonstrate a remarkable accuracy rate, and rapid responsiveness within our framework, effectively identifying various known attacks and affirming the proposed IDS system's feasibility.","2837-0740","979-8-3503-0460-2","10.1109/DASC/PiCom/CBDCom/Cy59711.2023.10361315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10361315","Intrusion detection;deep learning;Autokeras;vision transformer;EfficientNet;hyper-parameter optimization","Deep learning;Volume measurement;Intrusion detection;Transformers;Hyperparameter optimization;Feature extraction;Threat assessment","","","","23","IEEE","25 Dec 2023","","","IEEE","IEEE Conferences"
"Research on Image Recognition and Classification Algorithms Based on Artificial Intelligence","C. Zhang; R. Chang","The Twentieth Research Institute of China Electronics Technology Group Coporation, Xi'an, China; The Twentieth Research Institute of China Electronics Technology Group Coporation, Xi'an, China","2024 IEEE 2nd International Conference on Control, Electronics and Computer Technology (ICCECT)","7 Jun 2024","2024","","","1381","1385","In the field of artificial intelligence image recognition and classification, an innovative model named Visual Transformer enhanced by Gull Swarm Optimization (ViGO) has been developed recently. The model cleverly integrates the Vision Transformer (ViT) with the Gull Optimization algorithm (GSO). As an innovative achievement in the field of deep learning, ViT can segment images into a series of patches of fixed size and input them into Transformer architecture in the form of sequence data to effectively capture and utilize the global context information and long-term dependencies of images, thus significantly improving the accuracy and efficiency of image recognition. However, although ViT has demonstrated powerful image recognition capabilities, its need for large-scale annotated data sets and computing resources has become a key factor limiting its widespread application. To solve this problem, ViGO introduces the Seagull optimization algorithm. Seagull optimization is an intelligent optimization algorithm inspired by the foraging behavior of seagulls in nature, which has excellent global searching ability and local fine optimization characteristics. In ViGO model, gull optimization algorithm is used to optimize the key links of ViT such as hyperparameter setting and weight initialization, aiming to improve the learning process and convergence speed of the model, and improve its generalization performance under limited resources. Through the organic integration of ViT and Seagull optimization algorithm, ViGO model not only inherits the powerful function of ViT in image recognition, but also solves the problem of training complexity and resource requirements with the help of Seagull optimization algorithm, so that the model can achieve higher recognition accuracy and faster training speed under small data sets and limited computing power environment. The successful construction and application of this new ViGO model not only promotes the progress of artificial intelligence image recognition technology, but also provides a new idea and methodology for future deep learning model design and optimization, which has a wide range of practical application value and far-reaching academic influence.","","979-8-3503-8095-8","10.1109/ICCECT60629.2024.10546014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10546014","image recognition;artificial intelligence;seagull optimization algorithm","Training;Deep learning;Image segmentation;Image recognition;Computational modeling;Transformers;Data models","","","","10","IEEE","7 Jun 2024","","","IEEE","IEEE Conferences"
"Convergence of Photovoltaic Power Forecasting and Deep Learning: State-of-Art Review","M. Massaoudi; I. Chihi; H. Abu-Rub; S. S. Refaat; F. S. Oueslati","Department of Electrical and Computer Engineering, Texas A&M University at Qatar, Doha, Qatar; Laboratory of Energy Applications and Renewable Energy Efficiency (LAPER), El Manar University, Tunis, Tunisia; Department of Electrical and Computer Engineering, Texas A&M University at Qatar, Doha, Qatar; Department of Electrical and Computer Engineering, Texas A&M University at Qatar, Doha, Qatar; National Engineering School of Carthage, Carthage University, Tunis, Tunisia",IEEE Access,"11 Oct 2021","2021","9","","136593","136615","Deep learning (DL)-based PV Power Forecasting (PVPF) emerged nowadays as a promising research direction to intelligentize energy systems. With the massive smart meter integration, DL takes advantage of the large-scale and multi-source data representations to achieve a spectacular performance and high PV forecastability potential compared to classical models. This review article taxonomically dives into the nitty-gritty of the mainstream DL-based PVPF methods while showcasing their strengths and weaknesses. Firstly, we draw connections between PVPF and DL approaches and show how this relation might cross-fertilize or extend both directions. Then, fruitful discussions are conducted based on three classes: discriminative learning, generative learning, and deep reinforcement learning. In addition, this review analyzes recent automatic architecture optimization algorithms for DL-based PVPF. Next, the notable DL technologies are thoroughly described. These technologies include federated learning, deep transfer learning, incremental learning, and big data DL. After that, DL methods are taxonomized into deterministic and probabilistic PVPF. Finally, this review concludes with some research gaps and hints about future challenges and research directions in driving the further success of DL techniques to PVPF applications. By compiling this study, we expect to help aspiring stakeholders widen their knowledge of the staggering potential of DL for PVPF.","2169-3536","","10.1109/ACCESS.2021.3117004","National Priorities Research Program (NPRP) Grant from Qatar National Research Fund (a member of Qatar Foundation)(grant numbers:NPRP10-0101-170082); Qatar National Library; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555586","Photovoltaic power forecasting;deep learning;big data;discriminative learning;generative learning;deep reinforcement learning","Forecasting;Predictive models;Deep learning;Generative adversarial networks;Feature extraction;Data models;Biological system modeling","","35","","155","CCBY","1 Oct 2021","","","IEEE","IEEE Journals"
"Research on 5G Network Slicing Type Prediction Based on Random Forest and Deep Neural Network","X. Zhu; J. Wang; Q. Lai; X. Luo; R. Ren; H. Lu","School of Information Science and Engineering, Southeast University, Nanjing, China; School of Information Science and Engineering, Southeast University, Nanjing, China; School of Information Science and Engineering, Southeast University, Nanjing, China; School of Information Science and Engineering, Southeast University, Nanjing, China; School of Information Science and Engineering, Southeast University, Nanjing, China; Guangdong Communication and Networks Institute, Guangzhou, China",2023 IEEE 8th International Conference on Big Data Analytics (ICBDA),"21 Apr 2023","2023","","","154","158","Network slicing is a new network architecture that provides multiple logical networks on the same shared network infrastructure, each serving a specific business type or industry user. The network slicing feature will provide end-to-end isolation between slices and the ability to customize each slice based on service requirements (bandwidth, coverage, security, latency, reliability, etc.). This paper uses random forest and deep neural network (DNN) algorithms to build intelligent models capable of proactively detecting and eliminating incoming connection-based threats to select the most appropriate network slices, even in the case of network failure. Simulation results show that the random forest algorithm always guarantees 100% prediction accuracy regardless of whether the dataset has multidimensional features, but the time complexity is approximately one order of magnitude more than that of the DNN algorithm; in addition, the use of the DNN algorithm can effectively improve the prediction accuracy after data preprocessing of the dataset.","","979-8-3503-1076-4","10.1109/ICBDA57405.2023.10104899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10104899","5G;network slicing;random forest;DNN","Deep learning;Network slicing;Simulation;Neural networks;Data preprocessing;Prediction algorithms;Approximation algorithms","","","","11","IEEE","21 Apr 2023","","","IEEE","IEEE Conferences"
"Improved Deep Neural Network for OFDM Signal Recognition Using Hybrid Grey Wolf Optimization","Y. Zhang; D. Liu; J. Liu; Y. Xian; X. Wang","College of Information Science and Engineering, China University of Petroleum, Beijing, China; College of Information Science and Engineering, China University of Petroleum, Beijing, China; College of Information Science and Engineering, China University of Petroleum, Beijing, China; College of Information Science and Engineering, China University of Petroleum, Beijing, China; College of Information Science and Engineering, China University of Petroleum, Beijing, China",IEEE Access,"28 Jul 2020","2020","8","","133622","133632","Orthogonal Frequency Division Multiplexing (OFDM), as the core technology in mobile communications, is a multi-carrier modulation technology with high frequency spectrum utilization, which has strong anti-multipath interference and anti-fading ability. The significant advantage of OFDM signals lies in the anti-multipath effect, so its application environment is mostly multipath fading channels. Therefore, it is of great significance to study the identification of OFDM signals in multipath channels. Deep learning, with superior big data processing and classification capabilities, is a potential solution to these problems. Based on the problem of OFDM signal recognition in complex signals in multi-path channel, an OFDM signal recognition method based on hybrid grey wolf optimization algorithm to optimize deep neural network model is proposed. Because the basic grey wolf optimization algorithm (GWO) is easy to fall into a stasis state when attacking prey, differential evolution algorithm (differential evolution algorithm) is integrated into GWO to force GWO to jump out of the stasis state with its strong search ability. The convergence speed and recognition performance of the proposed algorithm are greatly improved. The experimental results show that under the condition of low SNR, the recognition accuracy of proposed algorithm is 9.95% higher than the traditional DNN method, and nearly 4.5% higher than the other two intelligent optimization methods, and the values of Precision and Recall increase obviously, which indicates that the hybrid algorithm not only improves the accuracy of recognition, but also makes the search more complete and accurate. Compared with classical particle swarm optimization (PSO) and whale algorithm optimization algorithm (WOA), the hybrid algorithm has strong competitiveness both in recognition performance and optimization stability, which provides a new, simpler and more effective method for modulation recognition of OFDM signals in wireless communications.","2169-3536","","10.1109/ACCESS.2020.3010589","National Research and Development Projects for Key Scientific Instruments through the Research and Development of Key Instruments and Technologies for Deep Resources Prospecting(grant numbers:ZDYZ2012-1-07-02-01); National Natural Science Foundation of China(grant numbers:41374151,41074099); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9144585","OFDM;multipath channel;deep neural network;grey wolf optimization;differential evolution;modulation recognition","OFDM;Modulation;Optimization;Feature extraction;Neural networks;Signal to noise ratio;Multipath channels","","9","","39","CCBY","20 Jul 2020","","","IEEE","IEEE Journals"
"An Intelligent Big Data Security Framework Based on AEFS-KENN Algorithms for the Detection of Cyber-Attacks from Smart Grid Systems","S. Muthubalaji; N. K. Muniyaraj; S. P. V. S. Rao; K. Thandapani; P. R. Mohan; T. Somasundaram; Y. Farhaoui","Department of Electrical and Electronics Engineering, CMR College of Engineering & Technology, Hyderabad, India; Department of Electronics and Communication Engineering, Vardhaman College of Engineering Kacharam, Shamshabad, India; Department of Electronics and Communication Engineering, Sreenidhi Institute of Science and Technology, Hyderabad, India; Department of Electronics and Communication Engineering, Vel Tech Rangarajan Dr. Sagunthala R&D Institute of Science and Technology, Chennai, India; Department of Electrical and Electronics Engineering, Bharat Institute of Engineering and Technology, Hyderabad, India; Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidhyapeetham, Bengaluru, India; T-IDMS, Department of Computer Science, Faculty of Sciences and Techniques, Moulay Ismail University, Errachidia, Morocco",Big Data Mining and Analytics,"22 Apr 2024","2024","7","2","399","418","Big data has the ability to open up innovative and ground-breaking prospects for the electrical grid, which also supports to obtain a variety of technological, social, and financial benefits. There is an unprecedented amount of heterogeneous big data as a consequence of the growth of power grid technologies, along with data processing and advanced tools. The main obstacles in turning the heterogeneous large dataset into useful results are computational burden and information security. The original contribution of this paper is to develop a new big data framework for detecting various intrusions from the smart grid systems with the use of AI mechanisms. Here, an AdaBelief Exponential Feature Selection (AEFS) technique is used to efficiently handle the input huge datasets from the smart grid for boosting security. Then, a Kernel based Extreme Neural Network (KENN) technique is used to anticipate security vulnerabilities more effectively. The Polar Bear Optimization (PBO) algorithm is used to efficiently determine the parameters for the estimate of radial basis function. Moreover, several types of smart grid network datasets are employed during analysis in order to examine the outcomes and efficiency of the proposed AdaBelief Exponential Feature Selection- Kernel based Extreme Neural Network (AEFS-KENN) big data security framework. The results reveal that the accuracy of proposed AEFS-KENN is increased up to 99.5% with precision and AUC of 99% for all smart grid big datasets used in this study.","2097-406X","","10.26599/BDMA.2023.9020022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10506810","smart grid;big data analytics;Machine Learning (ML);AdaBelief Exponential Feature Selection (AEFS);Polar Bear Optimization (PBO);Kernel Extreme Neural Network (KENN)","Computational modeling;Neural networks;Training data;Big Data;Feature extraction;Turning;Smart grids","","","","58","","22 Apr 2024","","","TUP","TUP Journals"
"Hybrid Nature-Inspired Based Oversampling and Feature Selection Approach for Imbalance Data Streams Classification","M. Arya; B. K. Dewangan; M. Verma; M. Rohini; A. Motwani; S. K. Sar","Dept. Of Computer Science & Engineering, Bhilai Institute of Technology, Durg CG, India; Dept. of computer Science Engineering, OP Jindal University, Raigarh, India; Dept. of computer Science & Engineering, Bhilai Institute of Technology, Durg, India; Dept. of computer Science & Engineering, Sri Krishna College of Engineering and Technology, Coimbatore, India; School of Computing Science & Engineering, VIT Bhopal University, Sehore MP, India; Dept. of computer Science & Engineering, Bhilai Institute of Technology, Durg, India",2022 OPJU International Technology Conference on Emerging Technologies for Sustainable Development (OTCON),"8 May 2023","2023","","","1","6","A big data stream is described using 5 $\mathrm{V}prime$s (Volume, Variety, Velocity, Variability, and Veracity). These characteristics impose various challenges. In many cases, data streams are unbalanced, making traditional data mining approaches impossible to employ. The standard data mining approaches are not suitable for imbalanced data streams for achieving analytical efficiency because they require periodic analyses, but big data requires real-time analytics. Additionally, the induction model must be re-run and rebuilt each time to add up the most recent data. Mining these unique streams, on the other hand, is one of the most intriguing research areas. Deep learning (DL) algorithms were developed to increase classification performance for issues requiring large data sets with varying types and characteristics. Feature selection (F.S.) is a critical stage in any classification application. F.S. entails eliminating superfluous and redundant characteristics, resulting in a prediction model that is more efficient, interpretable, and fast. While complete solutions are available for F.S., managing massive data streams that need instantaneous processing is challenging by its own nature. This work provides a hybrid metaheuristic strategy with FFSMOTE for oversampling imbalanced data stream and Honey Bee algorithm for feature selection. Hybridization aims to improve feature selection processes by combing the advantage of both the algorithms. Furthermore, the Ensemble Deep classifiers are used for data stream classification.","","978-1-6654-9294-2","10.1109/OTCON56053.2023.10113972","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10113972","Big Data;Data Stream;Data Mining;Deep Learning;Feature Selection;SMOTE Algorithm;Honey-Bee Algorithm;Ensemble;Classification.","Metaheuristics;Big Data;Predictive models;Feature extraction;Prediction algorithms;Real-time systems;Classification algorithms","","2","","19","IEEE","8 May 2023","","","IEEE","IEEE Conferences"
"Truck Traffic Speed Prediction Under Non-Recurrent Congestion: Based on Optimized Deep Learning Algorithms and GPS Data","J. Zhao; Y. Gao; Z. Yang; J. Li; Y. Feng; Z. Qin; Z. Bai","Key Laboratory of Transport Industry of Big Data Application Technologies for Comprehensive Transport, Ministry of Transport, Beijing Jiaotong University, Beijing, China; School of Mechanical, Electronic and Control Engineering, Beijing Jiaotong University, Beijing, China; School of Traffic and Transportation, Beijing Jiaotong University, Beijing, China; School of Traffic and Transportation, Beijing Jiaotong University, Beijing, China; School of Traffic and Transportation, Beijing Jiaotong University, Beijing, China; School of Traffic and Transportation, Beijing Jiaotong University, Beijing, China; School of Mechanical, Electronic and Control Engineering, Beijing Jiaotong University, Beijing, China",IEEE Access,"29 Jan 2019","2019","7","","9116","9127","Due to the restriction of traffic management measure in large cities, large heavy-haul trucks can only travel on the circuits and expressways around the city, which often causes congestion in these areas. It is necessary to study the travel speed prediction of trucks on the urban ring road and provide special information services for trucks. Based on the data generated by the trucks driving on the Sixth Ring Road in Beijing, an optimized GRU algorithm is proposed to predict the travel speed of trucks driving on urban express roads under non-recurrent congested conditions. First, a GPS map-matching algorithm that can simultaneously meet the accuracy and efficiency requirements of matching is proposed. Then, the trucks' data traveling on the Sixth Ring Road in Beijing are extracted from the original data. Aiming at getting rid of the abnormal data in GPS data, the screening and processing rules of the abnormal data are made, and then, the traffic speed sequence is extracted. Aiming at the problem that the commonly used weight optimization algorithm SGD cannot adaptively adjust the learning rate, Adam, Adadelta, and Rmsprop are used to optimize the weights in the GRU model in this paper. Considering the four scenarios, including workday, weekend, rainy, and accident, the accuracies of the proposed methods are verified.","2169-3536","","10.1109/ACCESS.2018.2890414","Fundamental Research Funds for the Central Universities(grant numbers:2018YJS138); National Natural Science Foundation of China(grant numbers:71871011,71871010,71621001,71671014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598930","GPS data;GRU model;optimization algorithm;traffic congestion;traffic speed prediction","Global Positioning System;Prediction algorithms;Roads;Deep learning;Automobiles;Predictive models;Urban areas","","47","","34","OAPA","1 Jan 2019","","","IEEE","IEEE Journals"
"Sorting the Digital Stream: Big Data-driven Insights into Email Classification for Spam and Ham Detection","S. A. Shah; E. A. Arputham; A. Ahmed; M. B. Farah; A. Shah; A. Aziz","School of Computing and Digital Technology, Birmingham City University, STEAMhouse, Birmingham, United Kingdom; School of Computing and Digital Technology, Birmingham City University, STEAMhouse, Birmingham, United Kingdom; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Sichuan, China; School of Computing and Digital Technology, Birmingham City University, STEAMhouse, Birmingham, United Kingdom; Department of Computer Engineering, Faculty of ICT BUITEMS, Quetta, Pakistan; Aragon Institute of Engineering Research, Universidad de Zaragoza, Zaragoza, Spain",2023 IEEE International Conference on Big Data (BigData),"22 Jan 2024","2023","","","5598","5607","In contemporary email communication, the ever- expanding volume of digital correspondence has ushered in an era where big data plays a pivotal role in addressing the challenge of distinguishing between legitimate (ham) and unsolicited (spam) emails. The primary objective of this paper is the meticulous identification and establishment of criteria for the discrimination between ham and spam emails. To achieve this, the study harnesses data from three distinct datasets, aiming to identify common attributes shared across all emails, irrespective of their classification, while concurrently devising methodologies for precise spam detection. Central to this endeavor is the evaluation of the effectiveness of feature selection techniques, specifically Chi-Square and Pearson Correlation, in elevating the accuracy of email classification. The investigation extends to assessing how the combination of these feature selection techniques with the broader machine learning framework can be optimized. This optimization entails the application of diverse preprocessing techniques to the datasets, all designed to amplify the precision of email classification. Furthermore, this research scrutinizes the performance evaluation metrics employed in the assessment of email classifiers. By conducting comprehensive experiments, the study identifies optimal classifiers based on rigorous evaluation metrics. This contributes valuable insights to the toolkit of techniques for proficient email classification within the realm of big data analysis.","","979-8-3503-2445-7","10.1109/BigData59044.2023.10386224","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10386224","Machine Learning;Classification;Email Spam Detection;Decision Tree;Naive Bayes;Artificial Neural Network;Data Pre-processing;Feature Selection","Performance evaluation;Correlation;Unsolicited e-mail;Machine learning;Big Data;Feature extraction;Object recognition","","","","32","IEEE","22 Jan 2024","","","IEEE","IEEE Conferences"
"Deep Learning-Based Resource Allocation in UAV-RIS-Aided Cell-Free Hybrid NOMA/OMA Networks","C. Huang; G. Chen; Y. Wen; Z. Lin; Y. Xiao; P. Xiao","5GIC & 6GIC, Institute for Communication Systems (ICS), University of Surrey, United Kingdom; 5GIC & 6GIC, Institute for Communication Systems (ICS), University of Surrey, United Kingdom; 5GIC & 6GIC, Institute for Communication Systems (ICS), University of Surrey, United Kingdom; School of Electrical and Information Engineering, University of Sydney, Australia; National Key Laboratory of Science and Technology on Communications, UESTC, China; 5GIC & 6GIC, Institute for Communication Systems (ICS), University of Surrey, United Kingdom",GLOBECOM 2023 - 2023 IEEE Global Communications Conference,"26 Feb 2024","2023","","","5641","5646","This paper investigates a deep learning-based algorithm to optimize the unmanned aerial vehicle (UAV) trajectory and reconfigurable intelligent surface (RIS) reflection coefficients in UAV-RIS-aided cell-free (CF) hybrid non-orthogonal multiple-access (NOMA)/orthogonal multiple-access (OMA) networks. The practical RIS reflection model and user grouping optimization are considered in the proposed network. A double cascade correlation network (DCCN) is proposed to optimize the RIS reflection coefficients, and based on the results from DCCN, an inverse-variance deep reinforcement learning (IV-DRL) algorithm is introduced to address the UAV trajectory optimization problem. Simulation results show that the proposed algorithms significantly improve the performance in UAV-RIS-assisted CF networks.","2576-6813","979-8-3503-1090-0","10.1109/GLOBECOM54140.2023.10437110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10437110","Reconfigurable intelligent surface;unmanned aerial vehicle;reinforcement learning;deep learning","Correlation;Simulation;Reconfigurable intelligent surfaces;Autonomous aerial vehicles;Reflection coefficient;Resource management;Trajectory optimization","","1","","27","IEEE","26 Feb 2024","","","IEEE","IEEE Conferences"
"Individual Recognition of Big Data Radar Digital Waveform Based on Long Short-Term Memory Network","Y. Jiang; W. Sheng; D. Cheng; L. Xiang; R. Song; W. Jiang","Early Warning Academy, Wuhan, China; Early Warning Academy, Wuhan, China; Early Warning Academy, Wuhan, China; Early Warning Academy, Wuhan, China; Early Warning Academy, Wuhan, China; Early Warning Academy, Wuhan, China","2023 IEEE 3rd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)","6 Jul 2023","2023","3","","857","862","Individual recognition of Big Data radar digital waveform is based on the classification of target individuals using radar target waveform. Traditional recognition methods suffer from problems such as low accuracy and complex recognition processes. To address these issues, this paper proposes an individual recognition method for radar digital waveform based on Long Short-Term Memory (LSTM) network. The actual collected radar target digital waveform is simulated to generate big data of radar digital waveform, and then the basic frequency domain feature data is constructed. The LSTM network is used to extract the individual data features of the radar, and the network parameters are iteratively trained using the SGD optimization algorithm to achieve effective recognition of individual radar signals. Experimental results show that the radar signal individual recognition method based on LSTM has significantly improved accuracy compared to SVM and XGBOOST methods.","","978-1-6654-9079-5","10.1109/ICIBA56860.2023.10165161","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10165161","individual recognition of radar digital waveform;big data;long short-term memory;fourier transform;confusion matrix","Support vector machines;Target recognition;Frequency-domain analysis;Radar;Big Data;Feature extraction;Radar signal processing","","1","","17","IEEE","6 Jul 2023","","","IEEE","IEEE Conferences"
"Deep Signal Recovery with One-bit Quantization","S. Khobahi; N. Naimipour; M. Soltanalian; Y. C. Eldar","Department of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, USA; Department of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, USA; Department of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, USA; Department of Electrical Engineering, Technion, Israel Institute of Technology, Haifa, Israel","ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","17 Apr 2019","2019","","","2987","2991","Machine learning, and more specifically deep learning, have shown remarkable performance in sensing, communications, and inference. In this paper, we consider the application of the deep unfolding technique in the problem of signal reconstruction from its one-bit noisy measurements. Namely, we propose a model-based machine learning method and unfold the iterations of an inference optimization algorithm into the layers of a deep neural network for one-bit signal recovery. The resulting network, which we refer to as DeepRec, can efficiently handle the recovery of high-dimensional signals from acquired one-bit noisy measurements. The proposed method results in an improvement in accuracy and computational efficiency with respect to the original framework as shown through numerical analysis.","2379-190X","978-1-4799-8131-1","10.1109/ICASSP.2019.8683876","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8683876","Deep learning;deep unfolding;MIMO communications;big data;machine learning;neural network;maximum likelihood;one-bit quantization","","","37","","22","IEEE","17 Apr 2019","","","IEEE","IEEE Conferences"
"Face Recognition System Based on CNN","D. Wang; H. Yu; D. Wang; G. Li","Northwest Minzu University, Lanzhou, China; Northwest Minzu University, Lanzhou, China; Northwest Minzu University, Lanzhou, China; Northwest Minzu University, Lanzhou, China",2020 International Conference on Computer Information and Big Data Applications (CIBDA),"27 Jul 2020","2020","","","470","473","With the development of computer vision and artificial intelligence, face recognition is widely used in daily life. As one of the most concerned methods of biometric recognition, face recognition has become one of the research hotspots in the field of computer vision and artificial intelligence. However, face recognition is easily affected by internal and external differences, and it is often difficult for traditional face recognition methods to achieve ideal results. In order to further improve the recognition accuracy of current face recognition algorithms, this paper proposes a face recognition algorithm based on improved convolutional neural network. Experiments show that the improved algorithm can be effectively applied to the data set.","","978-1-7281-9837-8","10.1109/CIBDA50819.2020.00111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9148141","convolutional neural network CNN;face recognition;optimization algorithm","Optimization;Training;Convolution;Feature extraction;Face recognition;Convolutional neural networks;Kernel","","29","","7","IEEE","27 Jul 2020","","","IEEE","IEEE Conferences"
"Data-Driven Fast Electrostatics and TDDB Aging Analysis","S. Peng; W. Jin; L. Chen; S. X. . -D. Tan","Department of Electrical and Computer Engineering, University of California at Riverside, Riverside, CA; Department of Electrical and Computer Engineering, University of California at Riverside, Riverside, CA; Department of Electrical and Computer Engineering, University of California at Riverside, Riverside, CA; Department of Electrical and Computer Engineering, University of California at Riverside, Riverside, CA",2020 ACM/IEEE 2nd Workshop on Machine Learning for CAD (MLCAD),"9 Apr 2021","2020","","","71","76","Computing the electric potential and electric field is a critical step for modeling and analysis of VLSI chips such as TDDB (Time dependent dielectric breakdown) aging analysis. Data-driven deep learning approach provides new perspectives for learning the physics-law and representations of the physics dynamics from the data. In this work, we propose a new data -driven learning based approach for fast 2D analysis of electric potential and electric fields based on DNNs (deep neural networks). Our work is based on the observation that the synthesized VLSI layout with multi interconnect layers can be viewed as layered images. Image transformation techniques via CNN (convolutional neural network) are adopted for the analysis. Once trained, the model is applicable to any synthesized layout of the same technology. Training and testing are done on a dataset built from a synthesized CPU chip. Results show that the proposed method is around 138x faster than the conventional numerical methods based software COMSOL, while keeping 99% of the accuracy on potential analysis, and 97% for TDDB aging analysis.","","978-1-4503-7519-1","10.1145/3380446.3430623","NSF(grant numbers:CCF-1816361); NSF(grant numbers:CCF-2007135,OISE-1854276); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9394631","Reliability;TDDB;Lifetime;Machine Learning","Training;Electric potential;Neural networks;Layout;Aging;Very large scale integration;Electrostatics","","4","","15","","9 Apr 2021","","","IEEE","IEEE Conferences"
"Data-Driven Fast Electrostatics and TDDB Aging Analysis","S. Peng; W. Jin; L. Chen; S. X. . -D. Tan","Department of Electrical and Computer Engineering, University of California at Riverside, Riverside, CA; Department of Electrical and Computer Engineering, University of California at Riverside, Riverside, CA; Department of Electrical and Computer Engineering, University of California at Riverside, Riverside, CA; Department of Electrical and Computer Engineering, University of California at Riverside, Riverside, CA",2020 ACM/IEEE 2nd Workshop on Machine Learning for CAD (MLCAD),"9 Apr 2021","2020","","","71","76","Computing the electric potential and electric field is a critical step for modeling and analysis of VLSI chips such as TDDB (Time dependent dielectric breakdown) aging analysis. Data-driven deep learning approach provides new perspectives for learning the physics-law and representations of the physics dynamics from the data. In this work, we propose a new data-driven learning based approach for fast 2D analysis of electric potential and electric fields based on DNNs (deep neural networks). Our work is based on the observation that the synthesized VLSI layout with multi interconnect layers can be viewed as layered images. Image transformation techniques via CNN (convolutional neural network) are adopted for the analysis. Once trained, the model is applicable to any synthesized layout of the same technology. Training and testing are done on a dataset built from a synthesized CPU chip. Results show that the proposed method is around 138x faster than the conventional numerical methods based software COMSOL, while keeping 99% of the accuracy on potential analysis, and 97% for TDDB aging analysis.","","978-1-4503-7519-1","10.1145/3380446.3430620","NSF(grant numbers:CCF-1816361); NSF(grant numbers:CCF-2007135,OISE-1854276); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9394646","Reliability;TDDB;Lifetime;Machine Learning","Training;Electric potential;Neural networks;Layout;Aging;Very large scale integration;Electrostatics","","2","","15","","9 Apr 2021","","","IEEE","IEEE Conferences"
"An Efficient Cancer Classification Model Using Deep Neural Network with Arithmetic Optimization Algorithm-Based Optimal Gene Selection","S. G. B; S. Anu H Nair; K. P. Sanal Kumar; S. Kamalakkannan","Department of Computer Science and Engineering, Annamalai University; Department of CSE, Annamalai University; Department of CS, R V Government Arts College Chengalpattu; Department of Information Technology, School of Computing Sciences VISTAS","2023 7th International Conference on Electronics, Communication and Aerospace Technology (ICECA)","9 Feb 2024","2023","","","946","953","Cancers are the most disastrous and inevitable ailment that occurs in individuals. Due to the hazardous effects of cancer, people get at death in very early age. In today's date, cancer is categorized into many types, which are affected by the external and internal parts of the body. In general, cancers are caused by the growth of abnormal tissues where cancer originates and it is gradually spread to other parts. Therefore, the medical industry struggles to detect the different types of cancer disorders without any loss of people. Hence, the automated detection system is implemented to predict cancer in its early stages to prevent the people gets worsening. Normally, the collection of individual data is another challenging concern. Several methods have been implemented yet they exist with constraints to provide better results. Machine learning models are also used, but it does not tackle the big data collection process and also fail to obtain the relevant features. Henceforth, the deep learning model has emerged for various processes like prediction, classification, and recognition. So, a new and improved classification framework for classifying cancer is executed in this paper. At first, the data is gathered from the benchmark database. From the data, the genes are optimally selected using an Improved Arithmetic Optimization Algorithm (IAOA). Then, the optimally chosen genes are given as input to the “Optimized Deep Neural Network (ODNN)” for classification. The constraints in the DNN framework are optimized by the improved AOA. From the DNN, the classified output is obtained. Various experimentations are carried out by contrasting the developed optimization algorithm enhanced DNN model to verify the efficient working of the suggested cancer classification model. Throughout the result analysis, the accuracy and precision rate of the designed method is 93.42% and 9363% for all datasets.","","979-8-3503-4060-0","10.1109/ICECA58529.2023.10395446","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10395446","Cancer Classification;Raw Data;Optimal Gene Selection;Improved Arithmetic Optimization Algorithm;Optimized Deep Neural Network","Deep learning;Genomics;Artificial neural networks;Predictive models;Classification algorithms;Ensemble learning;Bioinformatics","","","","24","IEEE","9 Feb 2024","","","IEEE","IEEE Conferences"
"Short-term Scheduling Decision Method of Cascade Hydropower Based on Scenarios Reduction-Deep Learning (SR-DL)","B. Changyu; X. Jun; C. Yifan; Z. Xinyi; F. Lina; X. Yuze","College of Energy and Electrical Engineering, Hohai University, Nanjing, China; College of Energy and Electrical Engineering, Hohai University, Nanjing, China; College of Energy and Electrical Engineering, Hohai University, Nanjing, China; College of Energy and Electrical Engineering, Hohai University, Nanjing, China; College of Energy and Electrical Engineering, Hohai University, Nanjing, China; College of Energy and Electrical Engineering, Hohai University, Nanjing, China",2021 IEEE Sustainable Power and Energy Conference (iSPEC),"24 Mar 2022","2021","","","3702","3707","With the rapid development of large-scale cascade hydropower in southwest and northwest China, it is of great theoretical significance and application value to study the short-term scheduling decision-making method of cascade hydropower with high applicability and high efficiency. The traditional mechanism-driven solution focuses on the optimization model and optimization algorithm. Although the conceptual logic is clear, it ignores historical data and historical decision-making schemes, and it is difficult to modify the model algorithm. However, the data-driven solution method does not study the internal mechanism of short-term scheduling, and directly constructs the mapping relationship between input and output through a large number of historical data training, which is efficient and easy to modify. In view of this, this paper proposes a scheduling decision-making method based on scenarios reduction-deep learning (SR-DL), which is applied to the short-term dispatching decision of cascade hydropower. The analysis of an example shows that this method can improve the decision accuracy and efficiency.","","978-1-6654-1439-5","10.1109/iSPEC53008.2021.9736133","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9736133","data-driven;cascade hydropower;short-term scheduling;scenarios reduction;deep learning","Training;Conferences;Decision making;Hydroelectric power generation;Dispatching;Data models;Optimization","","","","17","IEEE","24 Mar 2022","","","IEEE","IEEE Conferences"
"Exploring the Integration of National Cultural Resources through Content Retrieval and Visual Semantic Segmentation Display","H. Zhang","Chongqing Vocational Institute of Tourism, Chongqing, China","2023 IEEE 3rd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)","6 Jul 2023","2023","3","","1575","1580","In the era of ubiquitous information and abundant data, the ability to extract relevantinformation from scattered, obscure, and heterogeneous data sources is of paramount importance. Despitethe considerable richness of ethnic cultural resources, they are often characterized by significant heterogeneity and fragmentation. Semantic segmentation technology can effectively leverage the power of computers to extract image features and compile feature databases, enabling the fusion of heterogeneous knowledge. To this end, this paper proposes an image classification method that combines object semantics and deep learning algorithms to identify object areas within an image and retrieve matching images from a database based on implicit semantic information. Through the integration of national cultural resources, this approach enables content retrieval and visual semantic segmentation. The use of multi-layer perceptrons for supervised learning facilitates the generation of object semantic templates to address the challenge of object semantic similarity. The resulting templates can then be applied to classify images, leading to a high classification accuracy rate for binary semantic images. Experimental results demonstrate that the proposed method enables automatic feature comparison with target images in the feature library during the retrieval of ethnic cultural resources, allowing for the mining of previously undiscovered and valuable knowledge. Finally, the best matching results and relevant information are output to users, enabling the effective use of these resources","","978-1-6654-9079-5","10.1109/ICIBA56860.2023.10165102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10165102","content retrieval;National cultural resources;Multilayer perceptron;Feature extraction;Supervised learning","Deep learning;Visualization;Semantic segmentation;Semantics;Image retrieval;Supervised learning;Feature extraction","","","","12","IEEE","6 Jul 2023","","","IEEE","IEEE Conferences"
"Optimizing Predictive Models and Streamlining Feature Selection Process In Oncology","K. Subrmanian; I. Faye; G. Thangarasu; K. N. Kannan","Deptartment of Applied Science, Faculty of Engineering and IT, Saveetha School of Engineering, MAHSA University, Universiti Teknologi PETRONAS, Saveetha Institute of Medical and Technical Sciences, Malaysia, Chennai; Department of Fundamental & Applied Science, Universiti Teknologi PETRONAS, Perak, Malaysia; Centre of Digital Health and Health Informatics, Saveetha School of Engineering Inernational Medical Univeristy, Saveetha Institute of Medical and Technical Sciences, Malaysia, Chennai; Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Chennai, India",2024 IEEE 14th Symposium on Computer Applications & Industrial Electronics (ISCAIE),"8 Jul 2024","2024","","","233","238","In the ever-evolving landscape of healthcare, the intersection of big data and oncology presents a realm of possibilities for advancements in diagnosis, treatment, and patient care. This abstract explores the application of Deep Bee Swarm Optimization (DBSO) in the realm of big data analytics within oncology, a novel approach poised to revolutionize data-driven decision-making. Deep Bee Swarm Optimization, inspired by the collective intelligence of bee swarms, leverages the principles of deep learning to sift through vast datasets inherent in oncological research. The algorithm's innate ability to navigate complex, high-dimensional spaces makes it particularly well-suited for the intricacies of big data in healthcare. The primary objective of this research is to harness the power of DBSO in optimizing the analysis of diverse oncological datasets, ranging from genomic information to patient records. By employing this hybrid approach, the study aims to enhance the accuracy and efficiency of predictive models, leading to more precise diagnostics and personalized treatment strategies. Furthermore, the integration of DBSO addresses the challenge of feature selection in big data analytics within oncology. The swarm intelligence enables the identification of crucial biomarkers and relevant features, streamlining the data preprocessing phase and contributing to the development of robust predictive models.","2836-4317","979-8-3503-4879-8","10.1109/ISCAIE61308.2024.10576328","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10576328","Deep Bee Swarm Optimization;Big data analytics;Oncology;Predictive modeling;Feature selection","Deep learning;Accuracy;Navigation;Medical services;Big Data;Predictive models;Oncology","","","","12","IEEE","8 Jul 2024","","","IEEE","IEEE Conferences"
"An Optimized Nasnet-Mobile Network Based on the BOA Algorithm for Bearing Health Monitoring","H. Yao; F. He; Y. Yang","School of Economics and Management, Beijing Jiaotong University, Beijing, China; School of Economics and Management, Communication University of China, Beijing, China; School of Artificial Intelligence, Wuchang University of Technology, Wuhan, China",2023 IEEE International Conference on Image Processing and Computer Applications (ICIPCA),"27 Sep 2023","2023","","","365","370","In the situation that the poor adaptive ability of traditional feature extraction methods and the weak generalization ability of a shallow neural network, a Nasnet-Mobile network framework method rely on the butterfly optimization algorithm (BOA) is proposed to optimize the internal parameters. Firstly, to process the bearing data, the bearing data is shifted by two-dimensional time-frequency transformation to obtain some normalized time-frequency spectrum. And the uppermost accuracy of the test set in the neural network is regraded as a fitness function, where BOA is used to search for the best parameter combination of Nasnet-Mobile. Then, the learning rate and the amount of batch learning that play the vital roles in the BOA will be selected appropriately, and at the same time, the Nasnet-Mobile will be given by comparison Finally, combined with the powerful adaptive feature extraction deep learning, the obtained samples were input into Nasnet Mobile to train, and a bearing fault diagnosis model using Nasnet-Mobile is established. After several tests on the test samples, the diagnostic rate of the model can be more than 99%, which is much better than the traditional fault diagnosis methods. Experimental results show that the proposed method can extract effective features better and has higher classification and detection accuracy.","","979-8-3503-1467-0","10.1109/ICIPCA59209.2023.10257902","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10257902","Industrial big data;Nasnet-Mobile network;fault diagnosis;BOA algorithm","Fault diagnosis;Time-frequency analysis;Adaptation models;Search methods;Neural networks;Signal processing algorithms;Signal processing","","1","","11","IEEE","27 Sep 2023","","","IEEE","IEEE Conferences"
"An Improved CNN Employing SSA and Its Application in Bearing Fault Diagnosis","Y. Zhu; X. Cao; Z. Han","School of Computer and Artificial Intelligence, Wuhan University of Technology, Wuhan, China; School of Mechanical Engineering and Automation, Dalian Polytechnic University, Dalian, China; School of Economics and Management, Chinese University of Hong Kong, Shenzhen, China","2022 IEEE 5th International Conference on Automation, Electronics and Electrical Engineering (AUTEEE)","2 Jan 2023","2022","","","714","718","Aspiring to the problems of inadequate self-adaptive ability in classical feature extraction methods and weak generalization ability in single classifier under big data, an internal improved Convolutional neural network (CNN) method founded on Sparrow Search Algorithm (SSA) is proposed. First, to process bearing data, two-dimensional wavelet time-spectral transform is performed with bearing data, and some normalized time-frequency diagrams are obtained. The highest correct rate of the test set in the training network is used as the fitness function, in which SSA is used to hunt for the optimum parameter combination of CNN. Then correctly select the learning rate and batch learning times of SSA in the SSA that have a large impact on the training error. Simultaneously, the ideal structure distribution of CNN is given through comparison. Finally, combined with deep learning with powerful adaptive feature extraction and nonlinear mapping capabilities, the obtained samples are fed into CNN for training, and a load-bearing fault diagnosis model based on CNN is established. By testing the remaining samples multiple times, the diagnostic rate of the model can reach more than 99.5%, which is far superior to the traditional fault diagnosis method based upon pattern recognition and feature extraction. Experimental results show that the proposed method can effectively enhance the adaptive feature extraction ability and the accuracy of fault diagnosis of the model, so as to have better generalization performance.","2831-4549","978-1-6654-7197-8","10.1109/AUTEEE56487.2022.9994390","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9994390","Convolutional neural network;Sparrow Search Algorithm;deep learning;Bearing Fault Diagnosis","Fault diagnosis;Training;Wavelet transforms;Adaptation models;Time-frequency analysis;Signal processing algorithms;Feature extraction","","1","","5","IEEE","2 Jan 2023","","","IEEE","IEEE Conferences"
"A Convolutional Autoencoder Method for Simultaneous Seismic Data Reconstruction and Denoising","J. Jiang; H. Ren; M. Zhang","Key Laboratory of Geoscience Big Data and Deep Resource of Zhejiang Province, School of Earth Sciences, Zhejiang University, Hangzhou, China; Key Laboratory of Geoscience Big Data and Deep Resource of Zhejiang Province, School of Earth Sciences, Zhejiang University, Hangzhou, China; Shengli Oilfield Geophysical Research Institute of SINOPEC, Dongying, China",IEEE Geoscience and Remote Sensing Letters,"30 Dec 2021","2022","19","","1","5","Petroleum geophysical exploration is based on seismic data and has been widely affected by deep learning technology in recent years. As a consequence of the high efficiency and nonlinear fitting ability of deep learning models, we propose an improved convolutional autoencoder (CAE) method to achieve simultaneous reconstruction and denoising of seismic data. The architecture of the improved CAE is based on group convolution and inception structures, which have powerful feature extraction capabilities for seismic data. The CAE method regards the reconstruction and denoising of seismic data as a feature extraction process of the target seismic signals; this enables the method to simultaneously reconstruct the seismic signal accurately and suppress the random noise mixed in the seismic data. During the training of the CAE, the mean absolute error (MAE) loss function and Adam optimization algorithm were used. Because this is a data-driven method and does not require the threshold to be input manually, it can process a large amount of seismic data quickly and intelligently. Synthetic and field data examples demonstrate the effectiveness of the CAE method.","1558-0571","","10.1109/LGRS.2021.3073560","Natural Science Foundation of China(grant numbers:42074135,41674123); Zhejiang Province Basic Public Welfare Research Program(grant numbers:LY19D040002); Shengli Oilfield Geophysical Research Institute of SINOPEC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9416991","Convolutional autoencoder (CAE);deep learning;denoising;seismic data reconstruction","Convolution;Noise reduction;Interpolation;Image reconstruction;Deep learning;Signal to noise ratio;Feature extraction","","30","","27","IEEE","27 Apr 2021","","","IEEE","IEEE Journals"
"PANDA: Population Automatic Neural Distributed Algorithm for Deep Leaning","J. Wei; X. Zhang; Z. Ji; J. Li; Z. Wei","High Performance Computing Lab, Xi’an Jiaotong University, Xi’an, China; High Performance Computing Lab, Xi’an Jiaotong University, Xi’an, China; High Performance Computing Lab, Xi’an Jiaotong University, Xi’an, China; High Performance Computing Lab, Xi’an Jiaotong University, Xi’an, China; High Performance Computing Lab, Xi’an Jiaotong University, Xi’an, China","2021 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)","22 Dec 2021","2021","","","1370","1377","Deep neural network models perform very brightly in the field of artificial intelligence, but their success is affected by hyperparameters, and the learning rate schedule is one of the most important hyperparameters, and the search for the learning rate schedule is often time-consuming and computationally resource-intensive. In this paper, we propose a Population Automatic Neural Distributed Algorithm (PANDA) based on population joint optimization, which uses distributed data parallel deep neural network training to implement a dynamic learning rate schedule optimization strategy based on the population idea, with almost no loss of test accuracy, PANDA is able to dynamically refine the learning rate schedule during model training instead of following the usual suboptimal strategy. We conducted experiments on typical AlexNet, VGG16, and ResNet18 using the Tianhe-3 supercomputing prototype, and the results show that using PANDA to dynamically update the learning rate greatly reduces the learning rate schedule search time while ensuring close performance with the latest population hyperparameter algorithm, and in our experiments, PANDA lead to at max 123.85x speedup, and the experimental results prove the effectiveness and robustness of PANDA.","","978-1-6654-3574-1","10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00187","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9644805","Deep Learning;Distributed Training;Hyperparameter Search;Data Parallel;Population Algorithm;Learning Rate Search","Training;Deep learning;Schedules;Heuristic algorithms;Sociology;Learning (artificial intelligence);Statistics","","","","49","IEEE","22 Dec 2021","","","IEEE","IEEE Conferences"
"IoT Intrusion Detection System Using Deep Learning and Enhanced Transient Search Optimization","A. Fatani; M. Abd Elaziz; A. Dahou; M. A. A. Al-Qaness; S. Lu","School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Mathematics, Faculty of Science, Zagazig University, Zagazig, Egypt; LDDI Laboratory, Faculty of Science and Technology, University of Ahmed Draia, Adrar, Algeria; State Key Laboratory for Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Hubei Engineering Research Center on Big Data Security, School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, China",IEEE Access,"13 Sep 2021","2021","9","","123448","123464","The great advancements in communication, cloud computing, and the internet of things (IoT) have opened critical challenges in security. With these developments, cyberattacks are also rapidly growing since the current security mechanisms do not provide efficient solutions. Recently, various artificial intelligence (AI) based solutions have been proposed for different security applications, including intrusion detection. In this paper, we propose an efficient AI-based mechanism for intrusion detection systems (IDS) in IoT systems. We leverage the advancements of deep learnings and metaheuristics (MH) algorithms that approved their efficiency in solving complex engineering problems. We propose a feature extraction method using the convolutional neural networks (CNNs) to extract relevant features. Also, we develop a new feature selection method using a new variant of the transient search optimization (TSO) algorithm, called TSODE, using the operators of differential evolution (DE) algorithm. The proposed TSODE uses the DE to improve the process of balancing between exploitation and exploration phases. Furthermore, we use three public datasets, KDDCup-99, NSL-KDD, BoT-IoT, and CICIDS-2017 to assess the performance of the developed method, which achieved higher accuracy compared to several existing approaches.","2169-3536","","10.1109/ACCESS.2021.3109081","Hubei Provincial Science and Technology Major Project of China(grant numbers:2020AEA011); Key Research and Development Plan of Hubei Province of China(grant numbers:2020BAB100); Project of Science, Technology and Innovation Commission of Shenzhen Municipality of China(grant numbers:JCYJ20210324120002006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9525369","Internet of Things (IoT);security;cyberattack;intrusion detection system;feature selection;optimization algorithms","Feature extraction;Intrusion detection;Transient analysis;Deep learning;Convolutional neural networks;Cloud computing;Internet of Things","","72","","65","CCBYNCND","30 Aug 2021","","","IEEE","IEEE Journals"
"Hard Example Mining based Adversarial Autoencoder Recommendation Algorithm","J. Sun; D. Wei; M. M. B. Shagor","Taiyuan University of Technology,College of Software,Taiyuan,China; Taiyuan University of Technology,College of Software,Taiyuan,China; Taiyuan University of Technology,College of Information and Computer,Taiyuan,China",2020 Eighth International Conference on Advanced Cloud and Big Data (CBD),"21 Apr 2021","2020","","","103","106","Commonly used datasets in recommendation research suffer from unbalanced data distribution, sparsity, and different user rating preferences. All these problems affect the quality of recommendation. Thus, this paper proposed a recommendation model by combining hard example mining with adversarial autoencoder. Considering the difference in users' preference, Mean Model based triplet loss algorithm was introduced to classify the dataset into positive and negative samples and thus improve the quality of the training data. Using classified samples, the rating prediction model was trained from both reconstruction and adversarial aspects. Adam optimization algorithm was used to calculate different update gradients for different parameters. Experimental results show that the recommendation model improves the recommendation accuracy significantly, and several performance indicators are better than baseline models.","","978-1-6654-2313-7","10.1109/CBD51900.2020.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9406925","recommendation system;hard example mining;Mean Model;adversarial autoencoder","Training data;Predictive models;Big Data;Prediction algorithms;Data models;Classification algorithms;Data mining","","","","16","IEEE","21 Apr 2021","","","IEEE","IEEE Conferences"
"EMGraph: Fast Learning-Based Electromigration Analysis for Multi-Segment Interconnect Using Graph Convolution Networks","W. Jin; L. Chen; S. Sadiqbatcha; S. Peng; S. X. . -D. Tan","Department of Electrical and Computer Engineering, University of California, Riverside, CA; Department of Electrical and Computer Engineering, University of California, Riverside, CA; Department of Electrical and Computer Engineering, University of California, Riverside, CA; Department of Electrical and Computer Engineering, University of California, Riverside, CA; Department of Electrical and Computer Engineering, University of California, Riverside, CA",2021 58th ACM/IEEE Design Automation Conference (DAC),"8 Nov 2021","2021","","","919","924","Electromigration (EM) becomes a major concern for VLSI circuits as the technology advances in the nanometer regime. With Korhonen equations, EM assessment for VLSI circuits remains challenged due to the increasing integrated density. VLSI multisegment interconnect trees can be naturally viewed as graphs. Based on this observation, we propose a new graph convolution network (GCN) model, which is called EMGraph considering both node and edge embedding features, to estimate the transient EM stress of interconnect trees. Compared with recently proposed generative adversarial network (GAN) based stress image-generation method, EMGraph model can learn more transferable knowledge to predict stress distributions on new graphs without retraining via inductive learning. Trained on the large dataset, the model shows less than 1.5% averaged error compared to the ground truth results and is orders of magnitude faster than both COMSOL and state-of-the-art method. It also achieves smaller model size, $4\times$ accuracy and $14\times$ speedup over the GAN-based method.","0738-100X","978-1-6654-3274-0","10.1109/DAC18074.2021.9586239","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9586239","Electromigration (EM);graph convolution network (GCN);multisegment interconnect;hydrostatic stress assessment","Electromigration;Convolution;Image edge detection;Integrated circuit interconnections;Very large scale integration;Predictive models;Generative adversarial networks","","13","","40","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"PVHArray: An Energy-Efficient Reconfigurable Cryptographic Logic Array With Intelligent Mapping","Y. Du; W. Li; Z. Dai; L. Nan","Institute of Information Science and Technology, Zhengzhou, China; Institute of Information Science and Technology, Zhengzhou, China; Institute of Information Science and Technology, Zhengzhou, China; Institute of Information Science and Technology, Zhengzhou, China",IEEE Transactions on Very Large Scale Integration (VLSI) Systems,"24 Apr 2020","2020","28","5","1302","1315","This article presents a coarse-grained reconfigurable cryptographic logic array named PVHArray and an intelligent mapping algorithm for cryptographic algorithms. We propose three techniques to improve energy efficiency without affecting performance. First, the coarse-grained pipeline variable reconfigurable operation units balance the system critical path delay and number of algorithm operations to ensure the best performance. Second, the hierarchical interconnect network overcomes the shortcomings of a single network, providing PVHArray with good interconnectivity and scalability while managing the network hardware resource overhead. Third, the distributed control network supports accurate period-oriented control with a lightweight hardware structure, preserving hardware resources for other performance enhancements. We combine these advances with deep learning to propose a type of smart ant colony optimization mapping algorithm to improve algorithm mapping performance. We implemented our PVHArray on a 12.25 mm2 silicon square with 55-nm CMOS technology, with each algorithm working at its optimum frequency. Experiments show that PVHArray improved performance by about 12.9% per unit area and 13.9% per unit power compared with the reconfigurable cryptographic logic array REMUS_LPP and other state-of-the-art cryptographic structures. For cryptographic algorithm mapping, our smart ant colony optimization (SACO) algorithm reduced compilation time by nearly 38%. Finally, PVHArray supports a variety of types of cryptographic algorithms.","1557-9999","","10.1109/TVLSI.2020.2972392","National Natural Science Foundation of China(grant numbers:61404175); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9027953","Cryptographic;energy efficiency;intelligent mapping;pipeline variable;reconfigurable array","Logic arrays;Ciphers;Arrays;Program processors;Heuristic algorithms;Hardware","","19","","26","IEEE","9 Mar 2020","","","IEEE","IEEE Journals"
"An Adaptive Gradient Method with Differentiation Element in Deep Neural Networks","R. Wang; W. Wang; T. Ma; B. Zhang","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; ShenYuan Honors College, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Shenzhen Academy of Aerospace Technology, China",2020 15th IEEE Conference on Industrial Electronics and Applications (ICIEA),"9 Nov 2020","2020","","","1582","1587","Current adaptive gradient algorithm (such as Adam) used in deep neural network has the advantages of fast training speed, simple tuning task and high computational efficiency. However, these methods are usually based on the gradient update using the root mean square of the past gradient, which often causes the learning rate shock. Thus the model overshoot may be large and even cannot converge. The PID optimization algorithm for deep neural network provides a new way to solve this problem. It introduces the idea of automatic control to solve the problem of overshooting in the stochastic gradient algorithm. The Adam algorithm is similar to an adaptive PI controller. Inspired by this, the differentiation element is introduced into Adam algorithm to accelerate model convergence. The algorithm was tested on MNIST, Cifar-10, Cifar-100 and Tiny-ImageNet data sets in the section of experiment. It is shown that the training speed by 10% on the premise of guaranteeing the accuracy of the model.","2158-2297","978-1-7281-5169-4","10.1109/ICIEA48937.2020.9248376","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9248376","adaptive optimizer;gradient descent;differentiation element","Training;Adaptation models;Adaptive systems;Neural networks;Task analysis;Tuning;Convergence","","2","","19","IEEE","9 Nov 2020","","","IEEE","IEEE Conferences"
"Elimination of Data Modification in Sensor Nodes of WSN Using Deep Learning Model","J. Surendiran; B. M. Nathisiya; A. RadhaKrishnan; J. M. Stella; M. Marimuthu; N. Balaji","Department of ECE, HKBKCE, Bangalore, India; Stella Mary's College of Engineering, Nagercoil, India; University College of Engineering, Nagercoil, India; Department of CSE, HKBKCE, Bangalore, india; Department of CSE, Sona College of Technology, Tamilnadu, India; Department of ECE, Sri Venkataswara college of technology, vadangal, india","2022 International Conference on Computer, Power and Communications (ICCPC)","23 Mar 2023","2022","","","35","38","Wireless sensor networks (WSNs) are usually made up of small computers that measure things like temperature, pressure, light, and vibration, as well as medical devices that have signal transceivers that work on a certain radio display. It is often used in large-scale sensor networks because it can be built in different ways and doesn't cost much to set up. Other wireless networks use smart sensors to send data interfaces. The vast majority of devices that are connected to each other are given wireless networks. On the other hand, the u200b touch network can connect up to 65,000 devices. As the price of wireless solutions keeps going down and their features keep getting better, you may start to use wired solutions again for telemetry data collection systems and long-distance communication. This study showed how to use deep learning to stop sensor nodes from changing data. The sensor nodes store estimates and parameters. If these expected data values change, the network won't work right, which will also shorten the life of the node. When the sensor nodes were given out, data protection became more important. The proposed algorithm uses 98.82% as much energy as it needs to.","","979-8-3503-9784-0","10.1109/ICCPC55978.2022.10072261","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10072261","Wireless sensor networks;energy efficiency;touch network;connected devices","Temperature measurement;Performance evaluation;Deep learning;Computers;Vibrations;Wireless sensor networks;Temperature","","","","19","IEEE","23 Mar 2023","","","IEEE","IEEE Conferences"
"A Novel Deep Learning Character-Level Solution to Detect Language and Printing Style from a Bilingual Scanned Document","A. K. M. Shahariar Azad Rabby; M. M. Islam; N. Hasan; J. Nahar; F. Rahman","Apurba Technologies, Dhaka, Bangladesh; Apurba Technologies, Dhaka, Bangladesh; Apurba Technologies, Dhaka, Bangladesh; Apurba Technologies, Dhaka, Bangladesh; Apurba Technologies, Sunnyvale, CA, USA",2020 IEEE International Conference on Big Data (Big Data),"19 Mar 2021","2020","","","5218","5226","Bangla is one of the world's most widely-spoken languages, but few languages (or ""script"") automation solutions have been reported for it. To build an OCR system, it is very important to detect the language and type of printing style to run specific character recognition and segmentation modules. This paper presents a novel solution to automatically detect the language (Bangla vs English in terms of the script), and printing style (printed vs handwritten) from any given bilingual scanned document using multiple deep learning models.","","978-1-7281-6251-5","10.1109/BigData50022.2020.9378262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9378262","Multilingual OCR;Script Detection;Language Detection;Script Type Detection;Document Processing;OCR Optimization","Printing;Deep learning;Conferences;Big Data;Optical character recognition software;Character recognition;Optimization","","","","29","IEEE","19 Mar 2021","","","IEEE","IEEE Conferences"
"Research on industrial Internet Platform Technology based on machine learning algorithm","Z. Wang","Guangdong Industry Polytechnic, Guangdong, China","2022 4th International Conference on Communications, Information System and Computer Engineering (CISCE)","17 Aug 2022","2022","","","390","396","Industrial Internet, is a new generation of information technology and industrial system full range of deep integration of the product, mainly for the transformation and upgrading of the manufacturing industry, can greatly boost the development of productivity, to pull the real economy has an important role. However, as the industrial Internet is an emerging industry, its development and construction experience and capacity are still in the initial stage. The report to the 19th National Congress of the Communist Party of China made it clear that we will promote the deep integration of the Internet, big data, artificial intelligence and the real economy. After the financial crisis, the manufacturing industry once again became the focus of global attention. With the digital transformation of the manufacturing industry, the industrial Internet platform based on cloud computing, big data, Internet of Things, artificial intelligence and other new generation of information technology emerged. With the development of machine learning and deep learning, machine learning models such as genetic algorithm and gradient optimization have achieved good results in data classification, regression and fitting. Internet project evaluation based on industry characteristics, puts forward the improved analytic hierarchy process (ahp) based on machine learning methods, and machine learning algorithms and hierarchical analysis method, puts forward a new solution to the problem of the expert scoring to determine judgment matrix and in order to achieve the consistency and accuracy of the algorithm and introduce the weight of the particle swarm algorithm to adjust and sorting, Reduce the inaccuracy and complexity of ahp algorithm in the face of complex problems. To achieve the expert scoring more comprehensive consideration method, as well as more intelligent consistency adjustment and final weight calculation, the index of the industrial Internet design scheme weight calculation, finally according to the scoring scheme and the final hair there are final evaluation scores and further analysis and explanation of the results.","","978-1-6654-9848-7","10.1109/CISCE55963.2022.9851027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9851027","Industrial Internet;machine learning;analytic hierarchy Process;particle swarm optimization","Manufacturing industries;Machine learning algorithms;Analytic hierarchy process;Big Data;Internet;Complexity theory;Indexes","","","","18","IEEE","17 Aug 2022","","","IEEE","IEEE Conferences"
"A Method of Detecting and Modeling Mining-Induced Deformation Areas in Interferograms","X. Tian; X. Wu; M. Jiang","School of Transportation, Southeast University, Nanjing, China; School of Transportation, Southeast University, Nanjing, China; School of Geospatial Engineering and Science, Sun Yat-sen University, Guangzhou, China",IGARSS 2024 - 2024 IEEE International Geoscience and Remote Sensing Symposium,"5 Sep 2024","2024","","","11066","11069","The lack of attention to frequent or illegal mining activities that cause localized rapid surface subsidence can lead to significant accidents. Therefore, it is of great significance to identify and monitor such activities. The dense fringes generated by deformation gradients can affect the effectiveness and accuracy of InSAR processing. We proposed a new processing method that utilizes a two-dimensional mixed Gaussian model and mining area models to generate simulated interferograms to expand the dataset. Then we use YOLOv8 automatically detect suspected mining deformation areas from large-scale interferograms. Finally, by combining optimization algorithms with mining deformation models, we invert and model the deformation phase of the mining area, obtaining the modeled mining deformation phase results. This method can successfully detect the deformation areas and recover the deformation fringes, providing a foundation for improving the accuracy of subsequent InSAR processing.","2153-7003","979-8-3503-6032-5","10.1109/IGARSS53475.2024.10642640","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10642640","InSAR;object detection;image processing;mining subsidence","Deformable models;Accuracy;Deformation;Geoscience and remote sensing;Simulated annealing;Object detection;Object recognition","","","","16","IEEE","5 Sep 2024","","","IEEE","IEEE Conferences"
"A Metaheuristics-Based Hyperparameter Optimization Approach to Beamforming Design","K. -X. Thuc; H. M. Kha; N. Van Cuong; T. Van Luyen","Faculty of Electronics, Hanoi University of Industry, Hanoi, Vietnam; Faculty of Electronics, Hanoi University of Industry, Hanoi, Vietnam; Faculty of Electronics, Hanoi University of Industry, Hanoi, Vietnam; Faculty of Electronics, Hanoi University of Industry, Hanoi, Vietnam",IEEE Access,"2 Jun 2023","2023","11","","52250","52259","The paradigm shift from “connected things” to “connected intelligence” is anticipated to be made possible by the sixth-generation wireless systems, which typically use millimeter wave beamforming to mitigate the significant propagation loss. However, beamforming design in millimeter wave communications poses many different challenges owing to the large antenna arrays with the limitation of radio frequency chains and analog beamforming architectures. To circumvent this problem, deep learning models have recently been utilized as a disruptive method for solving difficult optimization problems in sixth-generation mobile systems, such as maximizing spectral efficiency. However, it is still unclear how to produce high-performance deep learning models which require considering appropriate hyperparameters. This study proposes a metaheuristics-based approach for optimizing hyperparameters that are used to build optimized deep learning models to maximize spectral efficiency. The research results demonstrate that the proposed approach-based models establish higher spectral efficiency than the state-of-the-art approach-based models and the reference model whose hyperparameters are based on empirical trials.","2169-3536","","10.1109/ACCESS.2023.3277625","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10129031","Hyperparameter optimization;beamforming;metaheuristics;millimeter wave;large-scale antenna arrays","Array signal processing;Millimeter wave communication;Metaheuristics;Radio frequency;6G mobile communication;Wireless communication;Antenna arrays","","5","","37","CCBYNCND","18 May 2023","","","IEEE","IEEE Journals"
"A deep learning approach for optimizing content delivering in cache-enabled HetNet","L. Lei; L. You; G. Dai; T. X. Vu; D. Yuan; S. Chatzinotas","Reliability and Trust (SnT), University of Luxembourg, Luxembourg; Department of Information Technology, Uppsala University, Sweden; Department of Information Technology, Uppsala University, Sweden; Reliability and Trust (SnT), University of Luxembourg, Luxembourg; Department of Information Technology, Uppsala University, Sweden; Reliability and Trust (SnT), University of Luxembourg, Luxembourg",2017 International Symposium on Wireless Communication Systems (ISWCS),"16 Nov 2017","2017","","","449","453","In ultra-dense heterogeneous networks, caching popular contents at small base stations is considered as an effective way to reduce latency and redundant data transmission. Optimization of caching placement/replacement and content delivering can be computationally heavy, especially for large-scale networks. The provision of both time-efficient and high-quality solutions is challenging. Conventional iterative optimization methods, either optimal or heuristic, typically require a large number of iterations to achieve satisfactory performance, and result in considerable computational delay. This may limit their applications in practical network operations where online decisions have to be made. In this paper, we provide a viable alternative to the conventional methods for caching optimization, from a deep learning perspective. The idea is to train the optimization algorithms through a deep neural network (DNN) in advance, instead of directly applying them in real-time caching or scheduling. This allows significant complexity reduction in the delay-sensitive operation phase since the computational burden is shifted to the DDN training phase. Numerical results demonstrate that the DNN is of high computational efficiency. By training the designed DNN over a massive number of instances, the solution quality of the energy-efficient content delivering can be progressively approximated to around 90% of the optimum.","2154-0225","978-1-5386-2913-0","10.1109/ISWCS.2017.8108157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8108157","Deep neural network;caching;energy optimization;user clustering;heterogeneous network","Optimization;Real-time systems;Training;Algorithm design and analysis;Delays;Heterogeneous networks;Time division multiple access","","66","","11","IEEE","16 Nov 2017","","","IEEE","IEEE Conferences"
"Effects of image quality and quantity on building a competitive COVID-19 diagnosis model","P. S. Orfanoudakis; P. Tzouveli; S. Kollias","School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece",2021 IEEE International Conference on Big Data (Big Data),"13 Jan 2022","2021","","","4405","4410","The outbreak of a health crisis, such as Covid-19, leads to decisions that must combine efficiency and speed. Often there is a trade-off between these two values, as the faster a decision is made, the less information is considered. This paper presents a deep learning model pipeline that balances these two values with the primary goal of classifying human lung X-rays into three categories: pneumonia, covid-19 and normal. Through this process, we tried to explore whether the quality of an image can enhance the learning process to a greater extent as opposed to having larger number of images. For this purpose, we follow two approaches by viewing quality and quantity as competing objectives to increasing the level of information obtained. The first is through increasing the number of X-ray images in the dataset, and the second is through improving the quality of the X-ray images. In the first approach, our goal is achieved using a Generative Adversarial Network (GAN) to generate plasmatic covid-19 class X-rays, while in the second approach, we improve the resolution of the X-ray images. To find the hyperparameters in both approaches that lead to better system performance, we exploit the Particle Swarm Optimization (PSO) algorithm. Rapid training and hyperparameter tuning better perform through this algorithm. Our experiments depict the performance that our models, based on the two approaches, achieved. Accuracy reaches 93% while sensitivity reaches 90% over Covid-19 cases. Finally, we conclude which characteristic, quality or quantity, is most useful in our case.","","978-1-6654-3902-2","10.1109/BigData52589.2021.9671791","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671791","deep learning;transfer learning;generative adversial network;particle swarm optimization;quality;quantity;Covid-19","COVID-19;Training;Deep learning;Sensitivity;Image resolution;Transfer learning;X-rays","","2","","13","IEEE","13 Jan 2022","","","IEEE","IEEE Conferences"
"Enhanced UNet Approach for Detecting Stroke Lesions in Computed Tomography Brain Scans","A. Tursynova; N. Ualiyev; A. Sakhipov; I. Omirzak; Y. Tulebayev; A. Seitenov","Department of Artificial Intelligence and Big Data, Al-Farabi Kazakh National University, Almaty, Kazakhstan; Higher School of Technical Sciences, I. Zhansugurov Zhetysu University, Taldykorgan, Kazakhstan; Department of Computer Engineering, Astana IT University, Astana, Kazakhstan; Department of Computer Engineering, Astana IT University, Astana, Kazakhstan; Department of Computer Engineering, Astana IT University, Astana, Kazakhstan; Department of Computer Engineering, Astana IT University, Astana, Kazakhstan",2024 IEEE 4th International Conference on Smart Information Systems and Technologies (SIST),"15 Aug 2024","2024","","","334","339","This research introduces an advanced UNet architecture for the precise segmentation of stroke lesions in Computed Tomography (CT) brain scans, addressing the critical need for accurate and efficient diagnosis in clinical settings. The literature reveals a spectrum of technologies utilized in detecting brain lesions, ranging from manual segmentation to threshold-based techniques and conventional machine learning methods, each with its limitations in terms of accuracy, time efficiency, and generalization capability. Deep learning techniques, notably the UNet model, have emerged as promising alternatives, offering significant advancements in medical image segmentation due to their ability to learn complex patterns and features from data. Our enhanced UNet model incorporates innovative techniques such as data augmentation, dropout, the Adam optimization algorithm, L2 regularization, and instance normalization to address the inherent challenges in stroke lesion segmentation, including the subtle appearance of lesions and the variability across different cases. The comparison of our approach with traditional methods using the ISLES 2018 dataset demonstrates substantial improvements in key performance metrics such as dice/f1 score, precision, recall/sensitivity, and Jaccard index, highlighting the effectiveness of our model in achieving precise segmentation outcomes. The utilization of these advanced techniques enables our model to outperform existing methods, offering a significant advantage in the rapid and accurate segmentation of stroke lesions. This research underscores the potential of the UNet architecture, augmented with modern deep learning techniques, to set a new benchmark in the field of stroke lesion detection, providing a robust tool for enhancing diagnostic processes and patient care in neurology.","","979-8-3503-7486-5","10.1109/SIST61555.2024.10629549","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10629549","UNet;deep learning;stroke lesion segmentation;computed tomography;image processing","Deep learning;Image segmentation;Adaptation models;Accuracy;Computed tomography;Manuals;Computer architecture","","","","26","IEEE","15 Aug 2024","","","IEEE","IEEE Conferences"
"Shallow water equations-fused dam-break wave propagation prediction model ensembled with a training process resampling method","C. Li; Z. Han","School of Civil Engineering, Central South University, Changsha, China; School of Civil Engineering, Central South University, Changsha, China",2023 International Conference on Intelligent Computing and Next Generation Networks（ICNGN),"25 Jan 2024","2023","","","1","6","The prediction of dam-break wave propagation behavior is a long-standing problem in hydrodynamics and hydrology. In addition to the traditional numerical simulation methods, deep learning technology has gradually made some beneficial attempts in the exploration of dam-break wave propagation. However, pure data-driven deep learning methods have certain limitations in terms of dataset requirements and long-term prediction. Therefore, with the help of the idea of the physical information neural networks (PINNs), this paper proposes a dam-break wave prediction neural network model embedded with the shallow water equations (SWEs) ——SWENet model. At the same time, in order to overcome the problem of low computational efficiency caused by large-scale computational domain and better capture the characteristics of the dam-break wave, a training progress random resampling (TPRR) method is proposed. The effectiveness of the SWENet model is verified by using two-dimensional dam-break cases, and compared with the pure data-driven multilayer perceptron (MLP) model. The results show that the SWENet model can achieve long-term dam-break wave prediction with less training data. We also analyze the training data size Ns, Ne and the resampling parameter Ee, and give the appropriate values.","","979-8-3503-2907-0","10.1109/ICNGN59831.2023.10396666","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10396666","Physical information neural networks;training progress random resampling;shallow water wave equations;two-dimensional dam-break","Training;Computational modeling;Propagation;Neural networks;Training data;Predictive models;Mathematical models","","","","20","IEEE","25 Jan 2024","","","IEEE","IEEE Conferences"
"Vehicle Type Classification System for Expressway Based on Improved Convolutional Neural Network","Z. Yibo; L. Qi; H. Peifeng","Software College, Northeastern University, Shenyang, China; Software College, Northeastern University, Shenyang, China; Software College, Northeastern University, Shenyang, China",2020 3rd International Conference on Artificial Intelligence and Big Data (ICAIBD),"9 Jul 2020","2020","","","78","82","Intelligent transportation system is becoming more and more popular, and vehicle type classification is the most important and basic part of it, which can be used in traffic flow detection, expressway billing system and other fields. On this basis, a vehicle type classification system based on the improved convolutional neural network is designed to classify the vehicle types on the expressway. And the trained model is applied to vehicle classification of video surveillance images. The validity of the improved convolutional neural network model is verified in this paper, and the model shows good effects on the accuracy of vehicle type classification. It has achieved more than 95% accuracy.","","978-1-7281-9741-8","10.1109/ICAIBD49809.2020.9137449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9137449","vehicle type classification;improved convolutional neural network;deep learning","Learning (artificial intelligence);Big Data;Video surveillance;Data models;Convolutional neural networks;Intelligent transportation systems","","1","","20","IEEE","9 Jul 2020","","","IEEE","IEEE Conferences"
"Joint Request Offloading and Resource Allocation for Energy Efficient D2D Enabled Multi-type Inference Services","Z. Zeng; J. Huang; J. Wu; J. Wu","School of Computer Science and Technology, Guangdong University of Technology, Guangzhou, China; School of Computer Science and Technology, Guangdong University of Technology, Guangzhou, China; School of Computer Science and Technology, Guangdong University of Technology, Guangzhou, China; School of Computer Science and Technology, Guangdong University of Technology, Guangzhou, China","2023 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)","11 Apr 2024","2023","","","213","220","Tremendous research efforts have devoted in providing multi-type inference services for users by deploying heterogeneous models in edge servers (ESs). But, resource contention in ES can degrade the quality-of-service (QoS) of users. To obtain the services with QoS guarantee, users can leverage device-to-device (D2D) collaboration to share the models and resources in mobile devices (MDs) with each other. However, it brings non-trivial energy consumption to MDs, and thus dramatically decreases their battery life. To fill this gap, this paper formulates an energy efficiency optimization problem for D2D enabled multi-type inference services. The goal is to minimize the total energy consumption of MDs, by jointly optimizing the decisions for request offloading and resource allocation under the constraints of resources and QoS. The formulated problem is NP-hard, as it is a mixed integer nonlinear programming problem. To solve the problem, this paper transforms it into an equivalent master request offloading (RO) problem with a low-complexity resource allocation (RA) subproblem. A two-level optimization algorithm problem is proposed for the transformed problem. In the outer level, a low-complexity heuristic algorithm is proposed to iteratively seek the optimal request offloading decision for RO problem. In the inner level, a subgradient based algorithm is proposed to derive the optimal resource allocation decision for RA subproblem. Experimental results show that, the proposed algorithms achieve the best performance, in terms of energy efficiency for all cases, in comparison to the baseline algorithms.","","979-8-3503-2922-3","10.1109/ISPA-BDCloud-SocialCom-SustainCom59178.2023.00061","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10491846","Multi-type inference;energy efficiency;device-to-device;collaboration;request offloading;resource allocation","Energy consumption;Quality of service;Transforms;Programming;Inference algorithms;Energy efficiency;Device-to-device communication","","","","21","IEEE","11 Apr 2024","","","IEEE","IEEE Conferences"
"Research on Fault Prediction Technology Based on Improved LSTM Neural Network","H. Qin; X. Jiang; J. Song; Z. Wang; K. Ding; H. Gong","Unit 92145 of PLA Houkou, Shanghai, China; Unit 92145 of PLA Houkou, Shanghai, China; Unit 92145 of PLA Houkou, Shanghai, China; Unit 92145 of PLA Houkou, Shanghai, China; Unit 92145 of PLA Houkou, Shanghai, China; Unit 92145 of PLA Houkou, Shanghai, China","2023 5th International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)","2 Apr 2024","2023","","","74","78","This paper presents a fault prediction method based on improved long short-term memory neural networks, including network structure design, network training, and prediction process implementation algorithms, etc. Further aiming at minimizing prediction error, a parameter optimization algorithm for SSA-BiLSTM prediction model is particularly designed, which is validated to have strong applicability and higher accuracy in fault time series analysis through experimental comparison with various typical time series prediction models.","2994-2977","979-8-3503-5993-0","10.1109/MLBDBI60823.2023.10482240","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10482240","LSTM;SSA;Fault prediction;Multi layer grid search;Deep learning","Training;Analytical models;Time series analysis;Neural networks;Predictive models;Prediction algorithms;Data models","","1","","11","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"New Insights Into Drug Repurposing for COVID-19 Using Deep Learning","C. Y. Lee; Y. -P. P. Chen","Department of Computer Science and Information Technology, La Trobe University, Melbourne, VIC, Australia; Department of Computer Science and Information Technology, La Trobe University, Melbourne, VIC, Australia",IEEE Transactions on Neural Networks and Learning Systems,"27 Oct 2021","2021","32","11","4770","4780","The coronavirus disease 2019 (COVID-19) has continued to spread worldwide since late 2019. To expedite the process of providing treatment to those who have contracted the disease and to ensure the accessibility of effective drugs, numerous strategies have been implemented to find potential anti-COVID-19 drugs in a short span of time. Motivated by this critical global challenge, in this review, we detail approaches that have been used for drug repurposing for COVID-19 and suggest improvements to the existing deep learning (DL) approach to identify and repurpose drugs to treat this complex disease. By optimizing hyperparameter settings, deploying suitable activation functions, and designing optimization algorithms, the improved DL approach will be able to perform feature extraction from quality big data, turning the traditional DL approach, referred to as a “black box,” which generalizes and learns the transmitted data, into a “glass box” that will have the interpretability of its rationale while maintaining a high level of prediction accuracy. When adopted for drug repurposing for COVID-19, this improved approach will create a new generation of DL approaches that can establish a cause and effect relationship as to why the repurposed drugs are suitable for treating COVID-19. Its ability can also be extended to repurpose drugs for other complex diseases, develop appropriate treatment strategies for new diseases, and provide precision medical treatment to patients, thus paving the way to discover new drugs that can potentially be effective for treating COVID-19.","2162-2388","","10.1109/TNNLS.2021.3111745","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9542933","Coronavirus;coronavirus disease 2019 (COVID-19);deep learning (DL);drug repurposing (DR);interpretable deep learning for anti-COVID-19 drugs prediction;severe acute respiratory syndrome coronavirus 2 (SARS-CoVID-2)","COVID-19;Drugs;Coronaviruses;Proteins;RNA;Deep learning","Antiviral Agents;COVID-19;Deep Learning;Drug Discovery;Drug Repositioning;Humans;Neural Networks, Computer","13","","73","IEEE","21 Sep 2021","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Task Assignment Based on Domain Knowledge","J. Liu; G. Wang; X. Guo; S. Wang; Q. Fu","Air Defense and Antimissile School, Air Force Engineering University, Xi’an, China; Air Defense and Antimissile School, Air Force Engineering University, Xi’an, China; Air Defense and Antimissile School, Air Force Engineering University, Xi’an, China; Air Defense and Antimissile School, Air Force Engineering University, Xi’an, China; Air Defense and Antimissile School, Air Force Engineering University, Xi’an, China",IEEE Access,"7 Nov 2022","2022","10","","114402","114413","Deep Reinforcement Learning (DRL) methods are inefficient in the initial strategy exploration process due to the huge state space and action space in large-scale complex scenarios. This is becoming one of the bottlenecks in their application to large-scale game adversarial scenarios. This paper proposes a Safe reinforcement learning combined with Imitation learning for Task Assignment (SITA) method for a representative red-blue game confrontation scenario. Aiming at the problem of difficult sampling of Imitation Learning (IL), this paper combines human knowledge with adversarial rules to build a knowledge rule base; We propose the Imitation Learning with the Decoupled Network (ILDN) pre-training method to solve the problem of excessive initial invalid exploration; In order to reduce invalid exploration and improve the stability in the later stages of training, we incorporate Safe Reinforcement Learning (Safe RL) method after pre-training. Finally, we verified in the digital battlefield that the SITA method has higher training efficiency and strong generalization ability in large-scale complex scenarios.","2169-3536","","10.1109/ACCESS.2022.3217654","Project of National Natural Science Foundation of China(grant numbers:62106283,72001214); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9931113","Deep reinforcement learning;imitation learning;knowledge rule base;safe reinforcement learning;task assignment","Deep learning;Games;Reinforcement learning;Sensors;Task analysis;Behavioral sciences;Reinforcement learning;Knowledge engineering","","4","","33","CCBYNCND","26 Oct 2022","","","IEEE","IEEE Journals"
"Delivery Route Optimization Based on Deep Reinforcement Learning","E. Xing; B. Cai","McDonald's (China) Co., Ltd.; School of Geography, Nanjing Normal University","2020 2nd International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)","26 Feb 2021","2020","","","334","338","With the rapid development of fast food industry, the research on the algorithm of delivery problem becomes increasingly important. The delivery problem of takeout is essentially the optimal path problem, while the traditional algorithm optimization of delivery path has been unable to meet the needs of modern takeout development. Based on this, this paper carries out the research on the delivery path optimization based on Deep Reinforcement learning algorithm. In this paper, we use the improvement method Heuristics in Deep Reinforcement learning to optimize the delivery path. In addition, we compare this method with the traditional tabu search algorithm, three distribution locations are selected and compared from two aspects of delivery time and customer satisfaction. The results show that using Deep Reinforcement learning algorithm to optimize delivery path can effectively reduce delivery time and improve delivery efficiency.","","978-1-7281-9638-1","10.1109/MLBDBI51377.2020.00071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9361015","Deep Reinforcement learning;Delivery;Route Optimization","Customer satisfaction;Food industry;Reinforcement learning;Big Data;Food products;Business intelligence;Optimization","","6","","10","IEEE","26 Feb 2021","","","IEEE","IEEE Conferences"
"Graph Neural Networks for Scalable Radio Resource Management: Architecture Design and Theoretical Analysis","Y. Shen; Y. Shi; J. Zhang; K. B. Letaief","Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; Department of Electronic and Information Engineering, The Hong Kong Polytechnic University, Hong Kong; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong",IEEE Journal on Selected Areas in Communications,"15 Dec 2020","2021","39","1","101","115","Deep learning has recently emerged as a disruptive technology to solve challenging radio resource management problems in wireless networks. However, the neural network architectures adopted by existing works suffer from poor scalability and generalization, and lack of interpretability. A long-standing approach to improve scalability and generalization is to incorporate the structures of the target task into the neural network architecture. In this paper, we propose to apply graph neural networks (GNNs) to solve large-scale radio resource management problems, supported by effective neural network architecture design and theoretical analysis. Specifically, we first demonstrate that radio resource management problems can be formulated as graph optimization problems that enjoy a universal permutation equivariance property. We then identify a family of neural networks, named message passing graph neural networks (MPGNNs). It is demonstrated that they not only satisfy the permutation equivariance property, but also can generalize to large-scale problems, while enjoying a high computational efficiency. For interpretablity and theoretical guarantees, we prove the equivalence between MPGNNs and a family of distributed optimization algorithms, which is then used to analyze the performance and generalization of MPGNN-based methods. Extensive simulations, with power control and beamforming as two examples, demonstrate that the proposed method, trained in an unsupervised manner with unlabeled samples, matches or even outperforms classic optimization-based algorithms without domain-specific knowledge. Remarkably, the proposed method is highly scalable and can solve the beamforming problem in an interference channel with 1000 transceiver pairs within 6 milliseconds on a single GPU.","1558-0008","","10.1109/JSAC.2020.3036965","General Research Fund from the Research Grants Council of Hong Kong(grant numbers:16210719); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9252917","Radio resource management;wireless networks;graph neural networks;distributed algorithms;permutation equivariance","Wireless networks;Neural networks;Resource management;Optimization;Computer architecture;Array signal processing;Scalability","","164","","43","IEEE","9 Nov 2020","","","IEEE","IEEE Journals"
"A Neural Network Anomaly Detection Model Based on Archimedes Optimization Algorithm for Intelligent Operation and Maintenance","Y. Wei; Z. Li; J. Pan; Z. Yang; Y. Nong; S. Zhou; Z. Zheng; N. Chen","School of Computer and Electronic Information, Guangxi University, Nanning, China; School of Computer and Electronic Information, Guangxi University, Nanning, China; School of Computer and Electronic Information, Guangxi University, Nanning, China; School of Computer and Electronic Information, Guangxi University, Nanning, China; School of Computer and Electronic Information, Guangxi University, Nanning, China; School of Computer and Electronic Information, Guangxi University, Nanning, China; School of Computer and Electronic Information, Guangxi University, Nanning, China; School of Computer and Electronic Information, Guangxi University, Nanning, China",2023 6th International Conference on Artificial Intelligence and Big Data (ICAIBD),"10 Aug 2023","2023","","","476","481","The operation and maintenance data abnormal detection technology is currently hampered by difficult detection procedures and low accuracy. The primary purpose of this paper is to investigate and address the problem of low accuracy in the abnormal detection of operation and maintenance data by combining artificial neural network models with deep learning technology. We choose long and short term memory (LSTM) and Encoder-Decoder LSTM model as the object for further optimization because they perform better in data anomaly identification than other neural network models in many measures. Also, by altering model parameters like loss rate, learning rate, training epoch, and batch size, artificial neural network model efficiency can be increased. Therefore, we propose an artificial neural network model that combines the recently developed Archimedes optimization algorithm (AOA). By using the AOA algorithm to optimize the model parameters, we can make the LSTM and Encoder-Decoder LSTM models reach a better state. The optimized model performs better than the non-optimized model in several measures of anomaly detection, with accuracy and precision showing the most pronounced improvement. In addition, the recall rate, classification effect, and error between the predicted value and real value all show significant improvements in the optimized model. By applying the optimized model, the anomaly detection accuracy of the ordinary computer room operation and maintenance data can achieve good effects, and provide the possibility for the model to be widely used in the actual intelligent operation and maintenance data anomaly detection.","2769-3554","978-1-6654-9125-9","10.1109/ICAIBD57115.2023.10206253","China Tobacco Guangxi Industrial; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10206253","intelligent operation and maintenance;anomaly detection;archimedes optimization algorithm;LSTM;encoder-decoder LSTM","Training;Computational modeling;Artificial neural networks;Learning (artificial intelligence);Maintenance engineering;Predictive models;Data models","","","","17","IEEE","10 Aug 2023","","","IEEE","IEEE Conferences"
"Detecting Sex From Handwritten Examples","S. Saha; M. A. Bin Khaled; M. S. Islam; N. S. Puja; M. Hasan","Fab Lab, Independent University Bangladesh, Dhaka, Bangladesh; Department of Computer Science, Independent University Bangladesh, Dhaka, Bangladesh; Department of Computer Science, Independent University Bangladesh, Dhaka, Bangladesh; Department of Computer Science, Independent University Bangladesh, Dhaka, Bangladesh; Fab Lab, Independent University Bangladesh, Dhaka, Bangladesh","2018 ieee international conference on system, computation, automation and networking (icscan)","22 Nov 2018","2018","","","1","7","There are several tasks that human excel at and computers do not and vice-versa. Just until a few years ago computers were as good as a storage for images and videos. However, in the past 6 years with the boon in artificial neural network, labeled data and computation power; machines have started becoming smart at tasks like recognizing images, detecting different objects in images, captioning images, understanding and summarizing videos, detecting semantic actions in videos and so on. Deep learning researchers and practitioners have started demonstrating notable performance of AI(Artificial Intelligence) on many different tasks that pushes the boundaries and as a continuation of that process, we took one specific problem to solve using deep learning that even human can not solve. We have taken Bangla handwritten characters, then trained them applying several deep learning techniques such as Convolutional Neural Network and Recurrent Neural Network to predict the sex of the writer. Consequently, we have got 91.85% accuracy rate and also demonstrated further analysis of the results that we got.","","978-1-5386-4866-7","10.1109/ICSCAN.2018.8541214","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8541214","Deep Learning;Convolutional Neural Network;Recurrent Neural Network;LSTM;Image Classification;Multi Label Classification style;Auto Encoders;Computer Vision;Sex Identification","Mathematical model;Training;Recurrent neural networks;Convolutional neural networks;Videos;Neurons;Feature extraction","","1","","40","IEEE","22 Nov 2018","","","IEEE","IEEE Conferences"
"Versatile Communication Optimization for Deep Learning by Modularized Parameter Server","P. -Y. Wu; P. Liu; J. -J. Wu","Department of Computer Science and Information Engineering, National Taiwan University; Department of Computer Science and Information Engineering, National Taiwan University; Institute of Information Science, Academia Sinica",2018 IEEE International Conference on Big Data (Big Data),"24 Jan 2019","2018","","","366","371","Deep learning has become one of the most promising approaches to solve the artificial intelligence problems. Training large-scale deep learning models efficiently is challenging. A widely used approach to accelerate the training process is by distributing the computation across multiple nodes with a centralized parameter server. To overcome the communication overhead caused by exchanging information between workers and the parameter server, three types of optimization methods are adopted - data placement, consistency control, and compression. In this paper, we proposed modularized parameter server, an architecture composed of key components that can be overridden without much effort. This allows developers to easily incorporate optimization techniques in the training process instead of using ad-hoc ways in existing systems. With this platform, the users can analyze different combinations of techniques and develop new optimization algorithms. The experiment results show that, compared with Google's distributed TensorFlow, our distributed training system based on the proposed modularized parameter server can achieve near-linear speedup for computing and reduce half of the training time by combining multiple optimization techniques while maintaining the convergent accuracy.","","978-1-5386-5035-6","10.1109/BigData.2018.8622358","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622358","Deep learning;distributed training;parameter server;communication optimization","Servers;Training;Deep learning;Optimization;Buffer storage;Computational modeling;Computer architecture","","","","27","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"Large-Scale Mobile App Identification Using Deep Learning","S. Rezaei; B. Kroencke; X. Liu","Computer Science Department, University of California at Davis, Davis, USA; Computer Science Department, University of California at Davis, Davis, USA; Computer Science Department, University of California at Davis, Davis, USA",IEEE Access,"2 Jan 2020","2020","8","","348","362","Many network services and tools (e.g. network monitors, malware-detection systems, routing and billing policy enforcement modules in ISPs) depend on identifying the type of traffic that passes through the network. With the widespread use of mobile devices, the vast diversity of mobile apps, and the massive adoption of encryption protocols (such as TLS), large-scale encrypted traffic classification becomes increasingly difficult. In this paper, we propose a deep learning model for mobile app identification that works even with encrypted traffic. The proposed model only needs the payload of the first few packets for classification, and, hence, it is suitable even for applications that rely on early prediction, such as routing and QoS provisioning. The deep model achieves between 84% to 98% accuracy for the identification of 80 popular apps. We also perform occlusion analysis to bring insight into what data is leaked from SSL/TLS protocol that allows accurate app identification. Moreover, our traffic analysis shows that many apps generate not only app-specific traffic, but also numerous ambiguous flows. Ambiguous flows are flows generated by common functionality modules, such as advertisement and traffic analytics. Because such flows are common among many different apps, identifying the source app that generates ambiguous flows is challenging. To address this challenge, we propose a CNN+LSTM model that uses adjacent flows to learn the order and pattern of multiple flows, to better identify the app that generates them. We show that such flow association considerably improves the accuracy, particularly for ambiguous flows. Furthermore, we show that our approach is robust to mixed traffic scenarios where some unrelated flows may appear in adjacent flows. To the best of our knowledge, this is the first work that identifies the source app for ambiguous flows.","2169-3536","","10.1109/ACCESS.2019.2962018","National Science Foundation(grant numbers:CNS-1547461,CNS-1718901,IIS-1838207); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8941027","Convolutional neural network;deep learning;flow association;mobile app identification;occlusion analysis;recurrent neural network;smartphone app fingerprinting;traffic classification","Deep learning;Mobile applications;Protocols;Payloads;Routing;Support vector machines","","71","","56","CCBY","24 Dec 2019","","","IEEE","IEEE Journals"
"On Neural Network Activation Functions and Optimizers in Relation to Polynomial Regression","J. Pomerat; A. Segev; R. Datta","University of South Alabama, Mobile, Alabama; University of South Alabama, Mobile, Alabama; University of South Alabama, Mobile, Alabama",2019 IEEE International Conference on Big Data (Big Data),"24 Feb 2020","2019","","","6183","6185","Recently, research in machine learning has become more reliant on data-driven approaches. However, understanding the general theory behind optimal neural network architecture is, arguably, just as important. With the proliferation of deep learning and neural networks, finding optimal neural network architecture is vital for both accuracy and performance. Recently, extensive research on neural network architecture has been performed [3],[4]. Additionally, while there has been plenty of research on hidden layer neural network architecture [2], activation functions are often not considered. In a network, an activation function defines the output of a neuron and introduces non-linearities into the neural network, enabling it to be a universal function approximator [12]. In terms of activation functions, one significant paper is Krizhevsky's seminole work on ImageNet classification and the creation of the ReLU activation function [1]. In the paper, Krizhevsky outlines the construction of an image recognition model using the Rectified Linear Unit activation function (ReLU) for the ImageNET LSVRC-2010 competition which outperformed the state-of-the-art image recognition systems at the time [1]. Since then, ReLU has increased in popularity. In their 2018 conference paper, Bircanoğlu and Arica, with the assistance of 231 distinct training procedures, found ReLU to be the best general activation function [12]. In addition to comparisons of activation functions, Nwankpa, Ijomah, Gachagan, and Marshall conducted a meta analysis of the field of research centered around activation functions and found ReLU to be the most popular activation function choice [5]. In terms of optimizers, gradient descent has historically been the most popular loss optimization algorithm, but with Kingma and Ba's 2014 paper [8], Adam: A Method for Stochastic Optimization, Adam optimizer is slowly becoming the industry standard [11]. In their paper, Kingma and Ba cleverly combine momentum descent, RMSprop, and Adagrad optimization into one algorithm, Adam (or adaptive moment estimation) [8]. In addition to Adam, there are plenty of other optimizers to choose from, including gradient descent, RMSprop [9], Adagrad [10], and Adadelta [7]. Recently, many breakthroughs have been made in terms of neural network performance, improved GPU performance and adaptation to deep learning tasks has created massive efficiency increases for the whole field of machine learning. Furthermore, as machine learning becomes increasingly optimized, the importance of efficiency improvements will continue to rise. Thus, understanding the optimal activation function and optimizer choice for a neural network is relevant. The goal of this paper is to make comparisons between activation functions, optimizers, and, more generally, entire neural network architectures, through measured error in a training environment. In this paper, we examine the performance of a wide variety of neural network configurations on randomly generated polynomial data sets of fixed degree. To do this, we compare various neural network activation functions and optimizers while controlling for hidden layer configurations and degree of the underlying polynomial dataset. Curiously, we find that the Sigmoid activation function is more accurate than ReLU and Tanh for regression tasks on low-featured polynomial data. We also reach the same conclusion regarding Stochastic Gradient Descent (SGD) in comparison to the Adam optimization function and Root Mean Square Propagation (RMSprop). Additionally, we observe that SGD is more efficient in the short term for finding local minimums than Adam or RMSprop; however, after sufficiently many epochs, performance differences between the optimizers vanished.","","978-1-7281-0858-2","10.1109/BigData47090.2019.9005674","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9005674","","Biological neural networks;Machine learning;Optimization;Training;Stochastic processes;Neurons","","28","","12","IEEE","24 Feb 2020","","","IEEE","IEEE Conferences"
"RLCkt II: Deep Reinforcement Learning via Attention-Aware Sampling for Analog Integrated Circuit Transistor Sizing Automation","W. Zuo; W. Sun; B. Lan; J. Wan","School of Information Science and Technology, Fudan University, Shanghai, China; School of Information Science and Technology, Fudan University, Shanghai, China; Suzhou Foohu Technology Co., Ltd., Shanghai, China; School of Information Science and Technology, Fudan University, Shanghai, China",2024 2nd International Symposium of Electronics Design Automation (ISEDA),"8 Aug 2024","2024","","","177","181","The sizing of circuit transistors that meet design specifications in analog integrated circuit (IC) traditionally relies on the intuition and experience of human experts, posing challenges that are labor-intensive and time-consuming, particularly as circuit complexity increases. Compared to mainstream optimization algorithms that exhibit slow optimization speeds and unstable solutions when dealing with large-scale analog IC, this study introduces RLCktII, a breakthrough improvement built upon the advanced RLCkt. For the first time, it incorporates an attention mechanism combined with deep reinforcement learning into the domain of automated analog integrated circuit transistor sizing. Through the attention mechanism, it dynamically discerns the most distinctive input data, enhancing the deep reinforcement learning model's capability to handle complex tasks. RLCkt II was evaluated on two industrial-scale analog integrated circuits: LDO and R2R. After one and a half days of training, our RLCkt II agent achieved an average improvement of 28.71 % and 7.94% in convergence accuracy over the state-of-the-art RLCkt. In the same design tasks, RLCkt II demonstrated a speed advantage nearly a hundred times faster than the Genetic Algorithm (GA) while also ensuring greater design precision.","","979-8-3503-5203-0","10.1109/ISEDA62518.2024.10617708","National Key R&D Program of China(grant numbers:2021YFA120050); Natural Science Foundation of Shanghai(grant numbers:23ZR1405900); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10617708","Transistor Sizing;Attention Mechanism;Reinforcement Learning;Automation of Analog Design","Training;Attention mechanisms;Design automation;Deep reinforcement learning;Transistors;Task analysis;Integrated circuit modeling","","","","10","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Image classification optimization and results analysis based on VGG network","J. Shi; Y. Zhao","School of Computer and Information, Anhui Polytechnic university, Anhui, China; School of Computer and Information, Anhui Polytechnic university, Anhui, China","2020 IEEE International Conference on Information Technology,Big Data and Artificial Intelligence (ICIBA)","14 Dec 2020","2020","1","","1090","1099","Classification accuracy and network performance are affected by many factors while classifying Image by convolution neural network. The paper takes garbage image classification as an example, the common performance optimization methods of garbage image classification by using VGGNet model were summarized. And VGG-16 was used to test the target data set of garbage image. The classification accuracy under different conditions was measured and compared. Finally, the performance of the parameters under different optimization schemes is analyzed.","","978-1-7281-5224-0","10.1109/ICIBA50161.2020.9277329","natural science pre research project of Anhui University of Technology(grant numbers:kz42019104); university level undergraduate teaching quality improvement plan project of Anhui University of technology in 2019(grant numbers:2019jyxm72); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9277329","image classification;parameter tuning;VGG;image augmentation;model;overfitting","Convolution;Kernel;Optimization;Image classification;Neurons;Classification algorithms;Biological neural networks","","","","19","IEEE","14 Dec 2020","","","IEEE","IEEE Conferences"
"An Attentional Interactive Deep Learning Model for Multi-source Electrical Consistency Checking in Power Grid Measurement Center","Y. Xu; L. Li; J. Tan; S. Li; Z. Ma; M. Liu; L. Wei; J. Tu","State Grid Jiangsu Electric Power Co., Ltd., Nanjing, China; Nari Group Corporation / State Grid Electric Power Research Institute, Nanjing, China; State Grid Jiangsu Electric Power Co., Ltd., Nanjing, China; Nari Group Corporation / State Grid Electric Power Research Institute, Nanjing, China; State Grid Jiangsu Electric Power Co., Ltd., Nanjing, China; State Grid Jiangsu Electric Power Co., Ltd., Nanjing, China; State Grid Jiangsu Electric Power Co., Ltd., Nanjing, China; Nari Group Corporation / State Grid Electric Power Research Institute, Nanjing, China",2023 IEEE 7th Information Technology and Mechatronics Engineering Conference (ITOEC),"30 Oct 2023","2023","7","","880","885","With the rapid development of the Internet and big data, artificial intelligence technology represented by deep neural network, DNN, has ushered in a golden period of development. The application of deep learning method in the field of power grid data checking has also attracted more and more scholars' attention. A deep learning model based on attentional interaction module is proposed for multi-source electrical consistency checking. Firstly, the circuit switch and the grid data are mapped into a fixed length vector, and the corresponding embedding matrix of the switch and data is obtained. Then, the embedded matrix is sent into the attentional interaction module to obtain an embedded matrix that comprehensively considers all elements of the switch and data, and is sent to the deep neural network after adding the previous embedded matrix to extract the intrinsic characteristics of the switch and data. Finally, the vector representation of switch and data is obtained and the matching score is calculated. Experimental results show that the proposed model is effective for multi-source electrical consistency checking.","2693-289X","979-8-3503-3421-0","10.1109/ITOEC57671.2023.10291942","State Grid Corporation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10291942","Artificial intelligence;DNN;Deep learning;Attention;Embedding;Multi-source electrical consistency checking","Deep learning;Power measurement;Mechatronics;Switches;Artificial neural networks;Power grids;Internet","","","","27","IEEE","30 Oct 2023","","","IEEE","IEEE Conferences"
"Big Vibration Data Diagnosis of Bearing Fault Base on Feature Representation of Autoencoder and Optimal LSSVM-CRO Classifier Model","V. Nguyen; T. Dung Hoang; V. Thai; X. Nguyen","Mechanical Engineering Department, Hanoi University of Industry, HaNoi, VietNam; Mechanical Engineering Department, Hanoi University of Industry, HaNoi, VietNam; Mechanical Engineering Department, Hunan University, Changsha, China; Mechanical Engineering Department, Hanoi University of Industry, HaNoi, VietNam",2019 International Conference on System Science and Engineering (ICSSE),"5 Sep 2019","2019","","","557","563","In this paper, based on deep learning method for the high-dimensional feature representation of vibration signal and optimal machine learning model, a new diagnosis technique is proposed for identifying the big vibration data of multi-level fault of roller bearing. Firstly, a deep learning network based on stacked autoencoders (SAE) with hidden layers is exploited for vibration feature representation (VFR) of roller bearing data, named as VFR-SAE, in which the unsupervised learning algorithm is used to reveal the significant properties in the data such as nonlinear, non-station properties. The represented features can provide good discriminability for fault diagnosis task. Secondly, an optimal classifier model based on least square support vector machine (LSSVM) classifier and chemical reaction optimization (CRO) algorithm, named as LSSVM-CRO, is formed to perform supervised fine-turning and classification. In this work, the transfer learning performance can be especially tuned in this diagnosis technique. That is, the features of target vibration data will be extracted by the learning of feature representation which depends on the weight matrix of hidden layers of VFR-SAE method. The experimental results by analyzing the roller bearing vibration signals with multi-status of fault have demonstrated that the VFR-SAE based feature extraction in conjunction with the LSSVM-CRO classifier model can achieve higher accuracies than the other popular classifier models.","2325-0925","978-1-7281-0525-3","10.1109/ICSSE.2019.8823332","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8823332","Bearing fault diagnosis;Feature representation;Deep learning network;Autoencoder;Multi-level Fault;LSSVM-CROmodel","Feature extraction;Vibrations;Training;Deep learning;Optimization;Rolling bearings;Classification algorithms","","","","30","IEEE","5 Sep 2019","","","IEEE","IEEE Conferences"
"Deep Learning Based Joint Hybrid Precoding and Combining Design for mmWave MIMO Systems","F. Liu; X. Li; X. Yang; H. Shi; B. Shi; R. Du","Lab of Key Technology of Millimeter-wave Large-scale MIMO System, Hebei Key Laboratory of Marine Perception Network and Data Processing, Northeastern University at Qinhuangdao, Qinhuangdao, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; Lab of Key Technology of Millimeter-wave Large-scale MIMO System, Hebei Key Laboratory of Marine Perception Network and Data Processing, Northeastern University at Qinhuangdao, Qinhuangdao, China",IEEE Systems Journal,"18 Mar 2024","2024","18","1","560","567","To maximally enhance the hybrid precoding performance, an innovative deep learning-based optimization algorithm is given for a multiuser fully connected (MUFC) structure in this article. First, a joint optimization framework based on convolutional neural network (CNN) is suggested, namely, JOCNN, which can synchronously optimize the hybrid precoder and combiner. In the JOCNN, analog precoding neural network (AP-NN), digital precoding neural network (DP-NN), analog combining neural network (AC-NN), and digital combining neural network (DC-NN) are designed to satisfy the nonconvex constant modulus constraint and the power constraint. Then, using a training strategy based on unsupervised learning, the developed JOCNN can be trained to maximize the spectral efficiency (SE). At last, the thoroughly trained JOCNN model can take in the estimated channel matrix as the input and immediately output the analog precoding matrix, the digital precoding matrix, the analog combining matrix, and the digital combining matrix. Simulation results demonstrate that the suggested method can outperform the previous works in terms of SE while requiring less computation time.","1937-9234","","10.1109/JSYST.2024.3357712","National Natural Science Foundation of China(grant numbers:61971117); Natural Science Foundation of Hebei Province(grant numbers:F2020501007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10430125","Convolutional neural network (CNN);deep learning (DL);hybrid precoding;multiple-input multiple-output (MIMO);millimeter-wave (mmWave)","Radio frequency;Precoding;Optimization;MIMO communication;Training;Millimeter wave communication;Convolutional neural networks","","1","","23","IEEE","8 Feb 2024","","","IEEE","IEEE Journals"
"A Hybrid Anomaly Detection Model Based on GANomaly in Cloud Environment","W. Yang; C. Zeng","School of Electronic Information Engineering Beijing Jiaotong University, Beijing, China; School of Electronic Information Engineering Beijing Jiaotong University, Beijing, China",2022 IEEE 5th International Conference on Big Data and Artificial Intelligence (BDAI),"26 Aug 2022","2022","","","51","56","Anomaly detection technology, which analyzes network data in detail and provides strategies for deploying security tools to ensure the integrity, confidentiality, and reliability of computer systems, is an important part of network information security infrastructure. However, in the cloud environment, the massive network traffic data contains many irrelevant or redundant features, which not only consumes many computing resources but also makes the detection accuracy lower. To this end, this paper proposes a hybrid model based on NSGA-III and GANomaly. Among them, NSGA-III can optimize the model in terms of the number of features, accuracy and false positive rate, while GAN omaly is a classifier that can be trained without negative samples. The experimental results show that the model reduces the number of features by nearly 74% on the basis of an accuracy rate of 99.71 % and a false alarm rate of 0.2% on the CSE-CIC-IDS2018 dataset.","","978-1-6654-7081-0","10.1109/BDAI56143.2022.9862656","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9862656","cloud environment;redundant features;NSGA-III;GANomaly","Training;Computational modeling;Information security;Telecommunication traffic;Feature extraction;Generative adversarial networks;Robustness","","1","","17","IEEE","26 Aug 2022","","","IEEE","IEEE Conferences"
"Comparative Study of Deep Learning Techniques for Detecting Tomato Plant Leaf Diseases Using Transfer Learning","P. K. Nalli; D. P. Garapati; M. V. Subbarao; E. Katta; A. S. Krishna; N. Deevi","Department of AI, Shri Vishnu Engineering College for Women, Bhimavaram, India; Department of AI, Shri Vishnu Engineering College for Women, Bhimavaram, India; Department of ECE, Shri Vishnu Engineering College for Women, Bhimavaram, India; Department of AI, Shri Vishnu Engineering College for Women, Bhimavaram, India; Department of AI, Shri Vishnu Engineering College for Women, Bhimavaram, India; Department of ECE, BVRIT Hyderabad College of Engineering for Women, Hyderabad, India",2024 International Conference on Distributed Computing and Optimization Techniques (ICDCOT),"8 May 2024","2024","","","1","7","Plant leaf diseases pose significant threats to crop yield and agricultural sustainability, making early and accurate detection crucial for effective disease management. In recent years, deep neural network (DNN) techniques have shown remarkable potential in the field of image classification, including plant disease detection. This study aims to investigate the performance of two popular deep learning architectures, namely, Xception, and InceptionResNetV2, for the detection of tomato plant leaf disease. The proposed methodology involves acquiring a diverse dataset comprising high-resolution images of healthy and diseased leaves from the target crops. Preprocessing techniques such as image augmentation and normalization are applied to enhance the generalization ability of the models and mitigate overfitting. Transfer learning is employed to initialize the deep learning architectures with weights pre-trained on large-scale image datasets to accelerate convergence and improve the models' performance in limited data scenarios. To evaluate performance of proposed networks various metrics such as validation and test accuracies, precision and recall, F1 score, and the area under the curve (AUC) are considered. From the investigations, the classification accuracy of the finest architectures is as follows: 99.8 percent for Xception and 99.4 percent for InceptionResNetV2 on Tomato Leaves. The results suggest that the models developed during the investigation phase to identify the leaf disease were superior to any existing Deep Neural Networks (DNNs).","","979-8-3503-8295-2","10.1109/ICDCOT61034.2024.10516123","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10516123","Plant Leaf Disease;Deep Learning;Neural Networks;Xception;InceptionResNetV2;Tomato","Deep learning;Training;Plant diseases;Plants (biology);Transfer learning;Artificial neural networks;Computer architecture","","","","18","IEEE","8 May 2024","","","IEEE","IEEE Conferences"
"From EMBER to FIRE: predicting high resolution baryon fields from dark matter simulations with deep learning","M. Bernardini; R. Feldmann; D. Anglés-Alcázar; M. Boylan-Kolchin; J. Bullock; L. Mayer; J. Stadel","Center for Theoretical Astrophysics and Cosmology, Institute for Computational Science, University of Zurich, Winterthurerstrasse 190, CH-8057 Zürich, Switzerland; mauro.bernardini@uzh.ch; Center for Theoretical Astrophysics and Cosmology, Institute for Computational Science, University of Zurich, Winterthurerstrasse 190, CH-8057 Zürich, Switzerland; Department of Physics, University of Connecticut, 196 Auditorium Road, U-3046, Storrs, CT 06269-3046, USA; Center for Computational Astrophysics, Flatiron Institute, 162 5th Avenue, New York, NY 10010, USA; Department of Astronomy, The University of Texas at Austin, 2515 Speedway, Stop C1400, Austin, TX 78712, USA; Department of Physics and Astronomy, University of California, 4129 Reines Hall, Irvine, CA 92697, USA; Center for Theoretical Astrophysics and Cosmology, Institute for Computational Science, University of Zurich, Winterthurerstrasse 190, CH-8057 Zürich, Switzerland; Center for Theoretical Astrophysics and Cosmology, Institute for Computational Science, University of Zurich, Winterthurerstrasse 190, CH-8057 Zürich, Switzerland",Monthly Notices of the Royal Astronomical Society,"13 Dec 2021","2021","509","1","1323","1341","Hydrodynamic simulations provide a powerful, but computationally expensive, approach to study the interplay of dark matter and baryons in cosmological structure formation. Here, we introduce the Emulating Baryonic En Richment (EMBER) Deep Learning framework to predict baryon fields based on dark matter-only simulations thereby reducing computational cost. EMBER comprises two network architectures, U-Net and Wasserstein Generative Adversarial Networks (WGANs), to predict 2D gas and H i densities from dark matter fields. We design the conditional WGANs as stochastic emulators, such that multiple target fields can be sampled from the same dark matter input. For training we combine cosmological volume and zoom-in hydrodynamical simulations from the Feedback in Realistic Environments (FIRE) project to represent a large range of scales. Our fiducial WGAN model reproduces the gas and H i power spectra within 10 per cent accuracy down to ∼10 kpc scales. Furthermore, we investigate the capability of EMBER to predict high resolution baryon fields from low resolution dark matter inputs through upsampling techniques. As a practical application, we use this methodology to emulate high-resolution H i maps for a dark matter simulation of a $L=100\, \text{Mpc}\, h^{-1}$ comoving cosmological box. The gas content of dark matter haloes and the H i column density distributions predicted by EMBER agree well with results of large volume cosmological simulations and abundance matching models. Our method provides a computationally efficient, stochastic emulator for augmenting dark matter only simulations with physically consistent maps of baryon fields.","1365-2966","","10.1093/mnras/stab3088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9648137","methods: numerical;methods: statistical;galaxies: haloes;dark matter;large-scale structure of Universe","","","","","","","13 Dec 2021","","","OUP","OUP Journals"
"Aircraft Detection in Airport Remote Sensing Images based on YOLOv5","C. Ji; L. Zhang; J. Li","School of Computer Science and Information Security, Guilin University of Electronic Technology; School of Computer Science and Information Security, Guilin University of Electronic Technology; Guilin Huigu Institute of Artificial Intelligence Industrial Technology, Guilin, China",2022 9th International Conference on Digital Home (ICDH),"19 Dec 2022","2022","","","13","18","With the development of remote sensing, aerospace, and other related technologies, remote sensing image application scenarios are more and more extensive. In this paper, the training of remote sensing data in different airports is improved based on the YOLOv5 (You Only Look Once) model. Based on the analysis of existing deep learning frameworks and models, a remote sensing image segmentation program is designed to solve the problems of large-scale, large variation, and multiple data channels of remote sensing images. To improve the performance of small object detection, the original image is segmented on the detector. When the size of the original image is large, it will be reduced in the detector input, and the number of pixel representatives of small targets will be seriously reduced, leading to the degradation of detector detection performance. So, we propose the purpose of image segmentation and separation detection is to reduce the loss of small target pixels during image reduction. It shows great advantages in solving the problem of precision and speed of aircraft identification, and it is feasible to identify aircraft with a computer quickly and efficiently.","","978-1-6654-5478-0","10.1109/ICDH57206.2022.00009","Research and Development; Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9978347","Deep learning;Remote sensing image processing;Aircraft detection","Training;Image segmentation;Atmospheric modeling;Detectors;Object detection;Airports;Data models","","","","21","IEEE","19 Dec 2022","","","IEEE","IEEE Conferences"
"Multimodal Emotion Recognition Based on Hybrid Fusion","G. Yang; D. Yang; J. Li; G. Wang","School of Software, Shenyang University of Technology, Shenyang, China; School of Software, Shenyang University of Technology, Shenyang, China; School of Software, Shenyang University of Technology, Shenyang, China; Beijing Research Institute of Telemetry, Beijing, China","2023 5th International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)","2 Apr 2024","2023","","","85","88","Accurate emotion recognition can greatly improve the reliability of interpersonal communication and mental illness diagnosis. Aiming at the problem that the accuracy of single-modal emotion recognition is difficult to improve, a multimodal emotion recognition method is proposed by fusing speech, facial expression and EEG. The attention mechanism is introduced in the feature layer fusion, and the recognition accuracy is improved by designing the optimal weight allocation algorithm of the decision layer fusion. The results show that the recognition accuracy is 94.53% on the training set MAHNOB-HCI, 92.89% on the SEED dataset, and 91.54% on the self-built dataset, which indicates that the recognition accuracy is improved compared with the single-modal emotion recognition, and has good generalization ability.","2994-2977","979-8-3503-5993-0","10.1109/MLBDBI60823.2023.10481889","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10481889","Attention mechanism;Decision-layer;Multi-modal","Training;Emotion recognition;Visualization;Machine learning algorithms;Face recognition;Speech recognition;Machine learning","","","","10","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Using Machine Learning Regression Model to Predict the Optimum Election Algorithm for Parallel and Distributed Computing Systems","N. Dhariwal","Department of Software Systems, Vellore Institute of Technology, Vellore, India","2023 Third International Conference on Smart Technologies, Communication and Robotics (STCR)","22 Jan 2024","2023","1","","1","5","Parallel and distributed computing has become a necessity when it comes to high-performance computing, artificial intelligence, big data analytics, machine learning, deep learning, signal processing and bioengineering. System engineers connect distributed systems, with large number of nodes, that then compute the solution for a problem concurrently. The distributed system has a leader that manages the overall coordination of these connected nodes. This research provides a novel approach to predict the most efficient and time-feasible election algorithm, that must be ran on a distributed system, using linear regression machine learning approach. This research implements the prediction process for three election algorithms and successfully predicts the most efficient and least time-consuming algorithm with 94.98% accuracy. Initially, the dataset for the regression model training was created by executing the three election algorithms parallelly using MPI running 16 parallel processors (cores). The research also presents three regression line equations, trained and modelled on this dataset, that can be used to predict the execution time for the three algorithms for a system with any number of nodes. The results are duly visualized, and regression lines are plotted for comparison.","","979-8-3503-7086-7","10.1109/STCR59085.2023.10396874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10396874","linear regression;machine learning;parallel and distributed compution;election algorithms;prediction;optimization","Training;Machine learning algorithms;Voting;Robot kinematics;Signal processing algorithms;Predictive models;Signal processing","","","","17","IEEE","22 Jan 2024","","","IEEE","IEEE Conferences"
"Optimization of Wireless Ad Hoc Network Node Layout Self-play Based on AlphaZero Algorithm","X. Zou; R. Yang; C. Yin; X. Wang","Information and Communications College, National University of Defense Technology, Wuhan, Hubei, China; Information and Communications College, National University of Defense Technology, Wuhan, Hubei, China; Information and Communications College, National University of Defense Technology, Wuhan, Hubei, China; Information and Communications College, National University of Defense Technology, Wuhan, Hubei, China",2019 2nd International Conference on Artificial Intelligence and Big Data (ICAIBD),"16 Sep 2019","2019","","","334","337","The success of AlphaZero algorithm in chess games provides ideas and methods for other fields. This research designs an intelligent deployment model of wireless ad hoc network node based on the deep reinforcement learning framework of AlphaZero algorithm. Aiming at the difficult problems of setting learning rate and preventing model overfitting in the process of model self-training optimization, this paper chooses gradually decreasing dynamics. Learning rate and regularization method are increased to ensure fast convergence in model training and improve the accuracy of model prediction.","","978-1-7281-0831-5","10.1109/ICAIBD.2019.8837036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8837036","AlphaZero;wireless ad hoc network;Learning rate;styling over-fitting;deployment","Neural networks;Ad hoc networks;Wireless communication;Training;Data models;Optimization;Training data","","","","11","IEEE","16 Sep 2019","","","IEEE","IEEE Conferences"
"Distributed Parameter Optimization Scheduling Strategy and System Design Based on Mesos","S. Li; Z. Lu; Y. Sun; S. Deng; B. Niu","University of Chinese Academy of Sciences, Beijing, China; Computer Network Information Center of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Computer Network Information Center of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","2019 IEEE 5th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS)","29 Aug 2019","2019","","","218","224","This paper implements a distributed parameter optimization system based on Mesos and studies the scheduling strategy of this system. Using the resource interface of Mesos, the system packages a variety of common parameter optimization algorithms and task scheduling strategy into a framework software that can run on Mesos. Aiming at the two-level scheduling mechanism of Mesos, a dynamic scheduling strategy for distributed parameter optimization system in multi-job environment on hybrid deployment cluster is proposed. This paper designs several experiments, and compares the resource scheduling strategy of the architecture software with the FIFO scheduling strategy in the hybrid deployment scenario. This work reduces the difficulty of optimizing distributed parameters in common scenarios such as deep learning in a cluster environment, and improves resource utilization efficiency in multi-task environment.","","978-1-7281-0006-7","10.1109/BigDataSecurity-HPSC-IDS.2019.00050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8819494","Apache Mesos, Distributed System, Scheduling, Parameter Optimization, Distributed Computing","Optimization;Task analysis;Processor scheduling;Training;Deep learning;Servers","","1","","21","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"Confrontational flight trajectory prediction based on attention mechanism","Y. Sun; D. Wang; W. Wang; L. Xiong; X. Yang","Institute of Aeronautics Engineering, Air Force Engineering University, Xi'an, China; Institute of Aeronautics Engineering, Air Force Engineering University, Xi'an, China; School of Telecommunications Engineering Xidian University, Xi'an, China; School of Telecommunications Engineering Xidian University, Xi'an, China; School of Telecommunications Engineering Xidian University, Xi'an, China",2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),"23 Apr 2021","2020","","","211","214","Predicting the flight trajectory of the target fighter as accurately as possible to capture the fleeting time to attack is crucial for close-range air combat, so the research has always been a focus in related fields. Without exception, it is as hard as other prediction problems due to the large degree of freedom of the solution space. All the previous methods only used the time correlation in a single trajectory data to predict and ignored the spatial correlation between the trajectory data of both sides. Therefore, not only the traditional machine learning methods but the current popular deep learning methods have less accurate predictions of trajectories, especially in the case of quick-changing, which greatly limits the practical application of this technology. In this paper, both the time and space correlation of flight trajectories are considered to express the tactical intentions in close-range air combat. Thus, the LSTM trajectory prediction method is proposed based on the combination of confrontational data and attention mechanism. Two or more flight trajectory data are used for learning and training of encoder network parameters, and then the target's trajectory is predicted by a corresponding decoder module. The method also takes the advantage of attention mechanism to strengthen the prediction accuracy. The experimental results show that the accuracy of target trajectory prediction is greatly improved, especially when the trajectory changes quickly, so it is very suitable for practical application.","","978-1-7281-9619-0","10.1109/ICBASE51474.2020.00052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9403789","Trajectory prediction;combat flight;long and short-term memory network;attention mechanism","Training;Deep learning;Correlation;Prediction methods;Feature extraction;Trajectory;Decoding","","2","","14","IEEE","23 Apr 2021","","","IEEE","IEEE Conferences"
"MOPSO-Based CNN for Keyword Selection on Google Ads","J. Liang; H. Yang; J. Gao; C. Yue; S. Ge; B. Qu","School of Electrical Engineering, Zhengzhou University, Zhengzhou, China; School of Electrical Engineering, Zhengzhou University, Zhengzhou, China; School of Statistics and Big Data, Henan University of Economics and Law, Zhengzhou, China; School of Electrical Engineering, Zhengzhou University, Zhengzhou, China; School of Electrical Engineering, Zhengzhou University, Zhengzhou, China; School of Electric and Information Engineering, Zhongyuan University of Technology, Zhengzhou, China",IEEE Access,"12 Sep 2019","2019","7","","125387","125400","Google Ads is an advertising agency that provides ads to advertisers. Advertisers match the user's search terms and push ads by selecting keywords related to their ad content. Keywords can determine the type of users an advertiser pushes, the effectiveness of the ad promotion, and the sales of the ad product. Automatically selecting keywords that are satisfactory to advertisers from a large number of keywords provided by Google Ads is the main task of this paper. But there is not too much time for the model to judge whether keywords are selected, choosing correct keywords in the shortest time is another task of this paper. Therefore, a structure of the model that can get some useful keywords for advertisers is designed and an improved multi-objective particle swarm optimization algorithm is proposed to achieve this multi-objective task. These are also the main contributions of this paper. To accomplish this multi-objective task, many technical issues need to be overcome, such as the mixed language problem, the imbalance problem, the problem of extracting features from corpora and so on. This paper proposes a corpus selection method to solve the mixed problem of Chinese and English in keywords, word embedding method to solve the representation of keywords, re-sampling to solve data imbalance problem, improved convolutional neural network (CNN) to solve classification problem, and a multi-objective particle swarm optimization algorithm (MOPSO) to achieve neural structure search of CNN so that the effect of the classification is improved and the training time is reduced. The keyword selection problem is solved with the combination of evolutionary computing, deep learning, machine learning, and text processing techniques. Experimental results show that the proposed algorithm greatly improved the accuracy of keyword selection and shortened the time of selecting keywords. Therefore, this algorithm has a good application value.","2169-3536","","10.1109/ACCESS.2019.2937339","National Natural Science Foundation of China(grant numbers:61876169,61473266,61673404); Scientific and Technological Project of Henan Province(grant numbers:152102210153); China Postdoctoral Science Foundation(grant numbers:2017M622373); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812731","Google Ads;corpus selection;word embedding;re-sampling;CNN;MOPSO;keyword selection;neural structure search","Google;Advertising;Task analysis;Economics;Deep learning;Particle swarm optimization;Feature extraction","","9","","42","CCBY","26 Aug 2019","","","IEEE","IEEE Journals"
"Multi-scale Feature Fusion in Wireless Propagation Model Optimization Algorithms","L. Zheng; J. Min; C. Guo; T. Yan","School of Electronic and Information Engineering, Lanzhou Jiaotong University, Lanzhou, China; School of Electronic and Information Engineering, Lanzhou Jiaotong University, Lanzhou, China; School of Electronic and Information Engineering, Lanzhou Jiaotong University, Lanzhou, China; School of Electronic and Information Engineering, Lanzhou Jiaotong University, Lanzhou, China","2021 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)","3 Sep 2021","2021","","","692","696","In order to address the effects of various environmental variables on radio propagation and improve the quality of radio propagation. Inspired by the traditional empirical classification method and based on an artificial environment, this paper improves the experiments on the path well prediction results of the wireless propagation model according to the optimization algorithm with satellite remote sensing images as the input data of the network, and selects RESNet50 with the highest recognition efficiency by comparing the experimental data of eight deep learning networks such as VGG16 and RESNet50 under the self-built wireless transmission environment. By adding a feature pyramid to the network, the recognition accuracy of environmental variables reaches 96.4%, and the recognition time is 1.29 seconds per 1000 samples.","","978-1-6654-4854-3","10.1109/ICITBS53129.2021.00174","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9526056","Deep Learning;Radio Propagation Model;Characteristic Gold Tower","Wireless communication;Wireless sensor networks;Smart cities;Satellite broadcasting;Radio propagation;Data models;Convolutional neural networks","","","","13","IEEE","3 Sep 2021","","","IEEE","IEEE Conferences"
"Research on Task Scheduling Optimi-Zation of Data Center Under Double Carbon Target","Z. Mei; C. Yang; W. Yang; Z. Wang; Y. Yang","Information System Integration Branch, Nari Technology Co.Ltd.,, Nanjing, China; Information System Integration Branch, Nari Technology Co.Ltd.,, Nanjing, China; Information System Integration Branch, Nari Technology Co.Ltd.,, Nanjing, China; Information System Integration Branch, Nari Technology Co.Ltd.,, Nanjing, China; Information System Integration Branch, Nari Technology Co.Ltd.,, Nanjing, China","2022 International Conference on Industrial IoT, Big Data and Supply Chain (IIoTBDSC)","23 Mar 2023","2022","","","294","299","When a data center is running, the efficiency of task scheduling directly affects its operating efficiency. Aiming at the efficiency of scheduling operation, aiming at low-carbon operation, the optimization algorithm and resource allocation technology of task scheduling are studied. The traditional scheduling optimization algorithm is relatively lacking in the training speed and the state and motion capture ability in the face of complex environments. Therefore, a scheduling optimization learning model is proposed, which accelerates the model training through the experience knowledge base, and establishes a deep reinforcement learning model to obtain more effi-cient and accurate scheduling policy information, and design a coding scheme using idle nodes to speed up task scheduling. Simulation experiments show that it effectively reduces the turnaround time and delay parameters of task scheduling, maximizes the amount of parallel tasks, and has better scheduling efficiency and high-load coping capabilities.","","978-1-6654-5455-1","10.1109/IIoTBDSC57192.2022.00061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10077177","Scheduling optimization;Resource allocation;Deep reinforcement Learning;experience knowledge base;Coding acceleration","Training;Data centers;Job shop scheduling;Knowledge based systems;Supply chains;Reinforcement learning;Motion capture","","","","16","IEEE","23 Mar 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Dynamic Demand in Shared Bicycle Scheduling Problem","Z. Jiao; W. Xiao; H. Xin; D. Liu; H. Yan; C. Chen","Laboratory for Big Data and Decision, National University of Defense Technology, Changsha, China; Academy of Military Sciences, Beijing, China; College of Systems Engineering, National University of Defense Technology, Changsha, China; Software College, Liaoning Technical University, Liaoning, China; Laboratory for Big Data and Decision, National University of Defense Technology, Changsha, China; School of Traffic and Transportation Engineering, Central South University, Changsha, China",2023 9th International Conference on Big Data and Information Analytics (BigDIA),"15 Feb 2024","2023","","","557","564","In contemporary shared bicycle scheduling systems, a common occurrence is a severe imbalance between regions, with one area suffering from a dearth of bicycles while another area is burdened with an excess. The task of balancing the distribution of vehicles can be abstracted as The Vehicle Routing Problem with Pickup and Delivery and Time Windows (VRPSPDTW). The primary objective is to efficiently schedule vehicles between multiple sites to minimize the overall transportation cost. However, in practical scheduling scenarios, the demand at each site undergoes dynamic fluctuations over time, thereby presenting a formidable challenge for problem-solving. Hence, this study employs a deep reinforcement learning algorithm equipped with a pointer network, constructing a computational environment capable of accommodating the dynamic nature of demand and lifting the constraints on the number of scheduled vehicles. Furthermore, the integration of a node masking mechanism enables the acquisition of high-quality scheduling strategies for addressing the shared bicycle scheduling predicament. Two distinct scheduling strategies are proposed, one prioritizing cost optimization and the other emphasizing user demand fulfillment. Empirical findings substantiate that the proposed model surpasses other conventional algorithms in terms of performance efficacy and computational efficiency.","2771-6902","979-8-3503-3007-6","10.1109/BigDIA60676.2023.10429476","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10429476","Deep Reinforcement Learning;shared bicycle scheduling;VRPSPDTW;Dynamic Demand","Deep learning;Processor scheduling;Heuristic algorithms;Bicycles;Reinforcement learning;Dynamic scheduling;Vehicle dynamics","","","","27","IEEE","15 Feb 2024","","","IEEE","IEEE Conferences"
"An Efficient Computation Offloading Approach in Multi-access Edge Computing Using Deep Reinforcement Learning","J. Wang; Q. Yin; X. Zhang; M. Zhang; H. Xu; Q. Fang","College of Systems Engineering, National University of Defense Technology, Changsha, China; College of Systems Engineering, National University of Defense Technology, Changsha, China; College of Systems Engineering, National University of Defense Technology, Changsha, China; College of Systems Engineering, National University of Defense Technology, Changsha, China; College of Systems Engineering, National University of Defense Technology, Changsha, China; College of Systems Engineering, National University of Defense Technology, Changsha, China",2022 8th International Conference on Big Data and Information Analytics (BigDIA),"9 Sep 2022","2022","","","67","72","Multi-access Edge Computing (MEC) is a promising paradigm to empower the internet of things (IoT) devices stronger ability for complicated applications and long hours of service. In this paper, we consider a binary offloading scenario with a MEC server and multiple smart devices, aiming to minimize the total energy consumption of smart devices. In order to obtain the offloading strategy quickly, we propose a deep reinforcement learning (DRL) based method which can directly output the solution without iterations that the conventional numerical optimization methods should have. To explore the offloading action space effectively, we propose a Hamming distance-based action exploration method to discover the optimal action for the update of policy networks. Numerical results show that the proposed method has good performance in the prediction accuracy and the exploring success rate.","2771-6902","978-1-6654-8796-2","10.1109/BigDIA56350.2022.9874103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9874103","Multi-access Edge Computing;deep reinforcement learning;ensemble learning;action exploration;computation offloading","Multi-access edge computing;Fluctuations;Computational modeling;Optimization methods;Reinforcement learning;Numerical models;Internet of Things","","","","17","IEEE","9 Sep 2022","","","IEEE","IEEE Conferences"
"Improved Grey Wolf Optimization Optimized Temporal Convolutional Networks - Bidirectional Long Short-Term Memory Networks Short-term Electric Load Forecasting Model","H. Xian; W. Bian; C. Xie; Y. Gao; C. Zheng","Linyi Power Supply Company, State Grid Shandong Electric Power Company, Linyi, China; Linyi Power Supply Company, State Grid Shandong Electric Power Company, Linyi, China; Linyi Power Supply Company, State Grid Shandong Electric Power Company, Linyi, China; Linyi Power Supply Company, State Grid Shandong Electric Power Company, Linyi, China; Linyi Power Supply Company, State Grid Shandong Electric Power Company, Linyi, China",2024 6th International Conference on Energy Systems and Electrical Power (ICESEP),"6 Sep 2024","2024","","","833","836","In the management of power systems, the accuracy of short-term electric load forecasting is crucial, especially against the backdrop of the ongoing energy structure transformation and increased market volatility. However, existing forecasting methods often show limitations when dealing with large-scale, highly dynamic data. To address this challenge, this study proposes an innovative model that combines Temporal Convolutional Networks (TCN) with Bidirectional Long Short-Term Memory Networks (BiLSTM) and introduces an improved Grey Wolf Optimization (GWO) algorithm for optimizing parameter configuration. The main innovations of the model include: 1) The adoption of an adaptive strategy-optimized grey wolf algorithm, which accelerates model convergence and improves prediction accuracy; 2) The combination of TCN and BiLSTM effectively captures the complex characteristics of time-series data; 3) The model demonstrates superior performance on multiple electric load datasets, especially outperforming existing advanced methods in key performance indicators. Through a series of experimental validations, the new model not only sets a new standard in accuracy but also shows significant advantages in generalization capability and stability.","","979-8-3503-5356-3","10.1109/ICESEP62218.2024.10652124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10652124","Short-term Electric Load Forecasting;Temporal Convolutional Networks;Bidirectional Long Short-Term Memory;Grey Wolf Optimization;Forecasting Performance Enhancement","Adaptation models;Accuracy;Recurrent neural networks;Load forecasting;Computational modeling;Predictive models;Prediction algorithms","","","","10","IEEE","6 Sep 2024","","","IEEE","IEEE Conferences"
"Emergency Rescue Decision-making System Based on Joint Algorithm of Remote Sensing Image and Unmanned Aerial Vehicle","F. He; H. Liu; M. Chen","School of Logistics and Management Engineering, Yunnan University of Finance and Economics, Kunming, Yunnan, China; College of Tourism and Geographical Science, Leshan Normal University, Leshan, Sichuan, China; School of Logistics and Management Engineering, Yunnan University of Finance and Economics, Kunming, Yunnan, China","2024 International Conference on Electrical Drives, Power Electronics & Engineering (EDPEE)","31 May 2024","2024","","","862","867","In this article, firstly, the related technologies of remote sensing image and UAV (Unmanned aerial vehicle) will be deeply studied, including the acquisition and processing of remote sensing image, the design and control of UAV, etc. Then, an efficient joint algorithm is designed and implemented, which can make full use of the advantages of remote sensing images and UAV to achieve comprehensive acquisition and accurate analysis of environmental information in disaster areas. Finally, the whole system is tested and evaluated comprehensively to verify its performance and effectiveness. Experimental results show that the proposed algorithm has obvious advantages in path planning error and accuracy. Compared with traditional ant colony algorithm and traditional LSTM algorithm, the path planning error of the proposed algorithm is reduced by more than 7%, and its accuracy is improved by more than 90%. In addition, the model shows a fast convergence speed in the training process, which further improves the efficiency of the algorithm. In actual operation, the algorithm proposed in this article also shows a low time consumption, thus improving the rapidity and accuracy of rescue operations. The research results of this article will provide new ideas and methods for the design and implementation of emergency auxiliary rescue decision system.","","979-8-3503-9563-1","10.1109/EDPEE61724.2024.00166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10539663","Remote sensing image;UAV joint algorithm;Emergency auxiliary rescue decision","Training;Disasters;Simulation;Decision making;Process control;Autonomous aerial vehicles;Path planning","","","","11","IEEE","31 May 2024","","","IEEE","IEEE Conferences"
"Predicting PD-L1 status of esophageal cancer from H&E images based on FusedNet model","M. Gao; C. Li; J. Zhang; H. Chen; W. Hu; H. Yang; L. Shi; Y. Jing; S. Tian; H. Sun; M. Grzegorzek; X. Li","College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, China; College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, China; College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, China; College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, China; College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, China; College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, China; College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, China; College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, China; College of Life and Health Sciences, Northeastern University, Shenyang, China; Shengjing Hospital of China Medical University, Shenyang, China; Institute of Medical Informatics, University of Luebeck, Luebeck, Germany; Cancer Hospital of China Medical University, Shenyang, China",2023 IEEE International Conference on Big Data (BigData),"22 Jan 2024","2023","","","4397","4405","For esophageal cancer immunotherapy, Programmed Death Ligand-l (PD-LI) is considered a predictive biomarker. However, immunehistochemistry (IHC) methods used to quantify PD-LI are challenged by high cost, time and variability. In contrast, hematoxylin and eosin (H&E) staining is a reliable method commonly used in cancer diagnosis. By employing advanced deep learning techniques, this study demonstrates the feasibility of predicting PD-LI expression from H&E stained images. With the help of pathologists, a dataset is constructed to evaluate the validity of PD-LI prediction in esophageal cancer by H&E using the FusedNet model. In 227 patients, PD-LI status is systematically predicted. Consistent prediction performance is demonstrated through validation of the validation set, proving that the system can be used as a decision support and quality assurance system in clinical practice.","","979-8-3503-2445-7","10.1109/BigData59044.2023.10386856","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10386856","PD-L1;Esophageal cancer;H&E;Deep learning;FusedNet","Deep learning;Quality assurance;Costs;Immunotherapy;Predictive models;Big Data;Data models","","","","23","IEEE","22 Jan 2024","","","IEEE","IEEE Conferences"
"Learned Unmanned Vehicle Scheduling for Large-Scale Urban Logistics","M. Zhang; Y. Zeng; K. Wang; Y. Li; Q. Wu; M. Xu","School of Computer and Artificial Intelligent, Zhengzhou University, Zhengzhou, China; School of Computer and Artificial Intelligent, Zhengzhou University, Zhengzhou, China; School of Computer and Artificial Intelligent, Zhengzhou University, Zhengzhou, China; Department of Infectious Disease, Public Health Center, Zhengzhou University People’s Hospital, Zhengzhou, Henan, China; School of Computer and Artificial Intelligent, Zhengzhou University, Zhengzhou, China; School of Computer and Artificial Intelligent, Zhengzhou University, Zhengzhou, China",IEEE Transactions on Intelligent Transportation Systems,"2 Jul 2024","2024","25","7","7933","7944","The adoption of unmanned vehicles in urban logistics has gradually become a trend. It can effectively lower carbon emissions, reduce labor costs, and improve logistics efficiency. In this paper, we investigate a novel problem of unmanned vehicle scheduling (UVS) for large-scale urban logistics, where the logistics platform assigns unmanned vehicles to deliver parcels among stations under the constraints of time, capacity, and electricity to maximize the overall revenue of the logistics platform. Although the UVS problem is of practical usefulness, solving it requires non-trivial efforts, because we have proved that the UVS problem is NP-hard. To solve the UVS problem efficiently, we propose an efficient two-stage processing framework, including task assignment and vehicle reposition. Specifically, in the first stage, we propose an effective preference-aware matching (PAM) algorithm to deal with task assignments between unmanned vehicles and delivery tasks, which considers not only the electricity consumption of unmanned vehicles but also the supply-demand balance between delivery tasks and unmanned vehicles. In the second stage, we propose two vehicle repositioning algorithms based on deep reinforcement learning, termed restricted DQN repositioning algorithm (RDR) and restricted A2C repositioning algorithm (RAR), which can effectively refine the vehicle’s reposition stations based on vehicle supply and demand, electricity supply and demand, charging pile availability and collision avoidance restriction rules at current and neighbor stations, so that the vehicles can be efficiently relocated to stations with over-delivery tasks. Finally, extensive experiments have demonstrated that our proposed algorithms can achieve desirable efficiency and effectiveness.","1558-0016","","10.1109/TITS.2024.3351687","National Key Research and Development Program of China(grant numbers:2021YFB3301504); National Natural Science Foundation of China(grant numbers:61972362,62372416,62325602,62036010,61602420); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10414380","Location-based services;urban logistics;task assignment;vehicle scheduling;optimization","Task analysis;Autonomous vehicles;Logistics;Costs;Deep learning;Reinforcement learning;Heuristic algorithms","","","","32","IEEE","25 Jan 2024","","","IEEE","IEEE Journals"
"Joint Power Control and Pilot Assignment in Cell-Free Massive MIMO Using Deep Learning","M. Usman Khan; E. Testi; M. Chiani; E. Paolini","CNIT/WiLab, DEI, University of Bologna, Cesena, Italy; CNIT/WiLab, DEI, University of Bologna, Cesena, Italy; CNIT/WiLab, DEI, University of Bologna, Cesena, Italy; CNIT/WiLab, DEI, University of Bologna, Cesena, Italy",IEEE Open Journal of the Communications Society,"4 Sep 2024","2024","5","","5260","5275","Cell-free massive MIMO (CF-mMIMO) networks leverage seamless cooperation among numerous access points to serve a large number of users over the same time/frequency resources. This paper addresses the challenges of pilot and data power control, as well as pilot assignment, in the uplink of a cell-free massive MIMO (CF-mMIMO) network, where the number of users significantly exceeds that of the available orthogonal pilots. We first derive the closed-form expression of the achievable uplink rate of a user. Subsequently, harnessing the universal function approximation capability of artificial neural networks, we introduce a novel multi-task deep learning-based approach for joint power control and pilot assignment, aiming to maximize the minimum user rate. Our proposed method entails the design and unsupervised training of a deep neural network (DNN), employing a custom loss function specifically tailored to perform joint power control and pilot assignment, while simultaneously limiting the total network power usage. Extensive simulations demonstrate that our method outperforms the existing power control and pilot assignment strategies in terms of achievable network throughput, minimum user rate, and per-user energy consumption. The model versatility and adaptability are assessed by simulating two different scenarios, namely a urban macro (UMa) and an industrial one.","2644-125X","","10.1109/OJCOMS.2024.3447839","Consorzio Nazionale Inter-Universitario per le Telecomunicazioni (CNIT) Wireless Communications Laboratory (WiLab) and the WiLab-Huawei Joint Innovation Center; European Union through the Italian National Recovery and Resilience Plan of NextGenerationEU, Partnership on “Telecommunications of the Future”(grant numbers:PE00000001-RESTART); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10643563","Cell-free massive MIMO;deep learning;pilot assignment;power control","Power control;Optimization;Uplink;Interference;Fading channels;Throughput;Signal to noise ratio","","","","39","CCBY","22 Aug 2024","","","IEEE","IEEE Journals"
"Semantic Segmentation Optimization Algorithm Based on Knowledge Distillation and Model Pruning","W. Yao; J. Zhang; C. Li; S. Li; L. He; B. Zhang","School of Mechanical Engineering, Southwest Jiaotong University, Chengdu, China; School of Mechanical Engineering, Southwest Jiaotong University, Chengdu, China; Data and Intelligence R&D Center, Zhuzhou CRRC Times Electric Co., Ltd., Zhuzhou, China; Affiliated Hospital, Chengdu University, Chengdu, China; School of Mechanical Engineering, Southwest Jiaotong University, Chengdu, China; School of Mechanical Engineering, Southwest Jiaotong University, Chengdu, China",2019 2nd International Conference on Artificial Intelligence and Big Data (ICAIBD),"16 Sep 2019","2019","","","261","265","This paper proposes a semantic segmentation optimization method combining knowledge distillation and model pruning to reduce the total calculation and volume of deep learning models. Compared with traditional pruning, the unstructured channel pruning of semantic segmentation neural networks combined with knowledge distillation consumes less total training in model compression and less inference time in prediction. The sparse training method of knowledge distillation proposed in this paper improves the whole pruning process while keeping the accuracy of the model basically unchanged. This method improves the speed of the entire pruning process, and reduces the total number of parameters and the total amount of calculations of the model significantly.","","978-1-7281-0831-5","10.1109/ICAIBD.2019.8836989","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8836989","deep learning;semantic segmentation;model compression;knowledge distillation","Training;Semantics;Image segmentation;Solid modeling;Convolution;Computational modeling;Knowledge engineering","","","","14","IEEE","16 Sep 2019","","","IEEE","IEEE Conferences"
"Unlabeled Text Classification Optimization Algorithm Based on Active Self-Paced Learning","T. Zheng; L. Wang","College of Computer Science and Technology, Taiyuan University of Technology, Taiyuan, China; Electrical and Power Engineering, Shanxi Institute of Energy, Jinzhong, China",2018 IEEE International Conference on Big Data and Smart Computing (BigComp),"28 May 2018","2018","","","404","409","This paper aims to introduce an algorithm for learning from unlabeled and very few labeled text based on the combination of convolutional neural network (CNN), one-vs-all SVM classifier and Active Self-Paced Learning(ASPL). The algorithm first initialize the classifier using a few annotated samples, and extract text features using CNN. It then rank the unlabeled samples according to their importance weight v. The top-ranked samples will be get, and form these samples into high-confidence sample set. In addition, we consider that a few annotations may contain incorrectly annotated samples, then, we re-rank them and select Top-6 ones with lowest prediction scores to verify these annotations. Experimental results shows that the accuracy of optimized text classifier can be improved by unlabeled or very few labeled text data. This algorithm can provide a more effective way for unlabeled or few labeled text samples classification.","2375-9356","978-1-5386-3649-7","10.1109/BigComp.2018.00066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367146","text classification;unlabeled text data;CNN;ASPL;One-vs-all SVM","Text categorization;Classification algorithms;Feature extraction;Training;Optimization;Support vector machines;Machine learning","","2","","28","IEEE","28 May 2018","","","IEEE","IEEE Conferences"
"Black-Box Optimization Based Adaptive Image Anonymization","A. Llanza; N. Shvai; A. Nakib","Cyclope.ai, University Paris Est Créteil Laboratoire LISSI, Paris, France; Cyclope.ai, National University of Kyiv-Mohyla Academy, Paris, France; Cyclope.ai, University Paris Est Créteil Laboratoire LISSI, Paris, France",2024 IEEE Congress on Evolutionary Computation (CEC),"8 Aug 2024","2024","","","1","8","In the last decade, Convolutional Neural Networks became an industry standard achieving state-of-the-art results for many computer vision tasks. This unprecedented success has been possible due to the use of massive amounts of visual and multimodal data. However, management of these data must comply with the regulations on privacy protection, i.e. personal data should be anonymized. Traditional image anonymization methods such as blurring, masking, pixelating are efficient in the obfuscation of the sensitive data. Still, recent research has indicated that these methods impact in the negative way the performance of computer vision models. Numerous deep learning anonymization methods have been proposed as an alternative, in particular for human face and body anonymization. Unfortunately, the vast majority of these approaches are task-specific and require training. Other methods, although general, rely on full access to the computer vision model (the so-called white-box methods). Here, we propose a novel adaptive image anonymization method that allows one to achieve high concordance of the classification model predictions on the original and anonymized image. It is gradient-free, agnostic to anonymized objects, and to the particular architecture and weights of the computer vision model used. Finally, the proposed method does not require modifications to the computer vision model. The main idea of the approach introduced in this paper is to consider image anonymization as an optimization problem and to solve it using the iFDA metaheuristics algorithm. Experiments conducted on the large-scale benchmark image dataset ImageNet convincingly demonstrate the efficiency of our approach. When applying the proposed adaptive image anonymization method, the class concordance rate obtained was 98.11%, as opposed to 74.47% obtained by traditional anonymization.","","979-8-3503-0836-5","10.1109/CEC60901.2024.10612195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10612195","optimization;metaheuristics;image classification;privacy protection;neural network","Image quality;Data privacy;Computer vision;Adaptation models;Computational modeling;Metaheuristics;Information filtering","","","","19","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Water-Body Type Classification in Dual PolSAR Imagery Using a Two-Step Deep-Learning Method","Q. Yuan; J. Lu; L. Wu; Y. Huang; Z. Guo; N. Li","School of Computer and Information Engineering, Henan University, Kaifeng, China; Land Satellite Remote Sensing Application Center, Ministry of Natural Resources, Beijing, China; School of Computer and Information Engineering, Henan University, Kaifeng, China; School of Computer and Information Engineering, Henan University, Kaifeng, China; School of Computer and Information Engineering, Henan University, Kaifeng, China; School of Computer and Information Engineering, Henan University, Kaifeng, China",IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"22 Feb 2024","2024","17","","4966","4985","Water-body type problems classification plays a vital role in ecological conservation, water resource management, and urban planning. Accurate classification can aid decision-makers in understanding the functions of different water-body types, providing key information for urban planning and promoting harmony between human activities and the natural environment. Despite extensive research in the field of water-body segmentation, exploration in the water-body type classification community is not as widespread. Therefore, this article proposes a novel water-body type classification method based on a two-step deep-learning model, decomposing water-body type classification into water-body segmentation and water-body type identification. Especially, this method constructs a unique data strategy by organically integrating backscatter features, polarimetric features, and DEM features, providing the model with rich and comprehensive information. In the first step, the segmentation network uses the fused feature to extract all water-body from synthetic aperture radar images. Subsequently, the extracted water-body are combined with the input data, forming a multifeature input for the identification network to distinguish between natural and artificial water-body. During this process, a swarm intelligence optimization algorithm is employed to explore the optimal hyperparameters of the network, including those of the segmentation and identification networks. Finally, the proposed method is assessed using extensive experiments on water-body segmentation tasks, water-body type identification tasks, and joint water-body type classification tasks. This article not only provides a new perspective in the field of water-body type classification but also demonstrates the immense potential of deep-learning network hyperparameter optimization and feature fusion in solving such.","2151-1535","","10.1109/JSTARS.2024.3361025","Plan of Science and Technology of Henan Province(grant numbers:232102211043); Key R&D Project of Science and Technology of Kaifeng City(grant numbers:22ZDYF006); Key Laboratory of Natural Resources Monitoring and Regulation in Southern Hilly Region; Ministry of Natural Resources of the People's Republic of China(grant numbers:NRMSSHR2022Z01); Key Laboratory of Land Satellite Remote Sensing Application; Ministry of Natural Resources of the People's Republic of China(grant numbers:KLSMNR-202302); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10418484","Deep learning;feature fusion;hyperparameter optimize;polarization decomposition method;synthetic aperture radar (SAR);water-body types classification","Feature extraction;Image segmentation;Remote sensing;Deep learning;Task analysis;Radar polarimetry;Backscatter","","2","","54","CCBYNCND","1 Feb 2024","","","IEEE","IEEE Journals"
"Particle Swarm Optimization-Based Deep Neural Network for Digital Modulation Recognition","W. Shi; D. Liu; X. Cheng; Y. Li; Y. Zhao","College of Information Science and Engineering, China University of Petroleum, Beijing, China; College of Information Science and Engineering, China University of Petroleum, Beijing, China; College of Information Science and Engineering, China University of Petroleum, Beijing, China; College of Information Science and Engineering, China University of Petroleum, Beijing, China; College of Information Science and Engineering, China University of Petroleum, Beijing, China",IEEE Access,"9 Aug 2019","2019","7","","104591","104600","Modulation recognition is a major task in many wireless communication systems including cognitive radio and signal reconnaissance. The diversification of modulation schemes and the increased complexity of the channel environment put higher requirements on the correct identification of modulated signals. Deep learning (DL) is considered as a potential solution to solve these problems due to the superior big data processing and classification capabilities. This paper proposes an efficient digital modulation recognition method based on deep neural network (DNN) model. Furthermore, we present the particle swarm optimization (PSO) algorithm to optimize the number of hidden layer nodes of the DNN so as to solve the problem that the traditional DNN is trapped in local minimum values and the number of hidden layer nodes needs selecting manually. In this paper, we utilize the proposed PSO-DNN method to learn characteristics extracted from the modulated signal added by additive white Gaussian noise (AWGN) and to train the network, which can improve the performance of recognition under the condition of low signal-to-noise ratio (SNR). The experimental results demonstrate that the recognition rate on this algorithm has improved by 9.4% and 8.8% compared with methods that adopt conventional DNN and support vector machine (SVM) when SNR equals 0 and 1 dB, respectively. Besides, another experiment compared with the genetic algorithm (GA) also proves that our proposed algorithm is more effective in optimizing the DNN. The proposed method is easy to be implemented so that it has a broad development prospect in modulation recognition.","2169-3536","","10.1109/ACCESS.2019.2932266","Research and Development of Key Instruments and Technologies for Deep Resources Prospecting (National Research and Development Projects for Key Scientific Instruments)(grant numbers:ZDYZ2012-1-07-02-01); National Natural Science Foundation of China(grant numbers:41374151,41074099); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8782525","Additive white Gaussian noise;deep neural network;digital modulation recognition;particle swarm optimization algorithm","Feature extraction;Digital modulation;Signal to noise ratio;Neural networks;Particle swarm optimization;Phase shift keying","","40","","28","CCBY","31 Jul 2019","","","IEEE","IEEE Journals"
"Scalable and Cost Efficient Resource Allocation Algorithms Using Deep Reinforcement Learning","M. Laroui; M. A. Cherif; H. I. Khedher; H. Moungla; H. Afifi","UMR 5157, CNRS, Institut Polytechnique de Paris, Telecom SudParis Saclay, France; Computer Science Departement, EEDIS Laboratory, Djillali Liabes University, Sidi Bel Abbes, Algeria; UMR 5157, CNRS, Institut Polytechnique de Paris, Telecom SudParis Saclay, France; UMR 5157, CNRS, Institut Polytechnique de Paris, Telecom SudParis Saclay, France; UMR 5157, CNRS, Institut Polytechnique de Paris, Telecom SudParis Saclay, France",2020 International Wireless Communications and Mobile Computing (IWCMC),"27 Jul 2020","2020","","","946","951","The emergence of a new generation of applications led to the appearance of new challenges that represent improvements in current communication technologies. For this, a new network paradigm's including edge computing that allows the process of data at the edge of the network. And the 5G network slicing that represents a new generation of communication increases the capacity of mobile networks by supporting the slicing technology that allows virtual “cutting” of a telecommunications network in several slices that provide high performance in terms of bandwidth and latency. Slice allocation and placement is an important networking optimization task that still painstakingly tune heuristics to get a sufficient solution. These algorithms use data as input and outputs near-optimal solutions. Thus, we are motivated by replacing this tedious process with the recent deep reinforcement learning algorithms. In this paper, we propose three approaches for Virtual Network Functions (VNFs) slices placement in edge computing (Integer linear programming (ILP), reinforcement learning (RL), and deep reinforcement learning (DRL)). Then they are implemented and evaluated. Several scenarios are considered to study the behavior of the algorithms and to quantify the impact of network size. The results show the feasibility and efficiency of the proposed techniques in terms of server utilization, placement time, and energy consumption.","2376-6506","978-1-7281-3129-0","10.1109/IWCMC48107.2020.9148286","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9148286","VNF;Edge Computing;Network Slicing;Optimization;Deep Learning;Deep Reinforcement Learning","Servers;5G mobile communication;Edge computing;Machine learning;Computational modeling;Network slicing;Optimization","","3","","16","IEEE","27 Jul 2020","","","IEEE","IEEE Conferences"
"DeepLoc: A Deep Neural Network-based Indoor Positioning Framework","S. Liu; Q. Ren; J. Li; H. Xu","Department of Computer Science and Technology, Heilongjiang University, Harbin, China; Department of Computer Science and Technology, Heilongjiang University, Harbin, China; Shandong Artificial Intelligence Institute, Qilu University of Technology, Jinan, China; Department of Computer Science and Technology, Heilongjiang University, Harbin, China","2021 IEEE 23rd Int Conf on High Performance Computing & Communications; 7th Int Conf on Data Science & Systems; 19th Int Conf on Smart City; 7th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)","30 May 2022","2021","","","1735","1740","The growing attention on location-based services has promoted the development of indoor localization studies. Existing techniques mainly use Received Signal Strength Indicator (RSSI) of wireless signals as location fingerprint. Inspired by deep learning techniques for signal processing, we propose a deep neural network-based framework (DeepLoc) to implement Wi-Fi fingerprint positioning. In order to improve localization performance, we further design a network division based optimization algorithm. We first adopt greedy algorithm to locate the user in a sub-area, and then reconstruct a smaller fingerprint database, which is fed into the training model. Finally, we evaluate the proposed framework. Experimental results show that DeepLoc can improve the localization accuracy efficiently and obtain better performance.","","978-1-6654-9457-1","10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9781040","positioning;greedy algorithm;deep learning;sub-area","Location awareness;Training;Greedy algorithms;Deep learning;Signal processing algorithms;Fingerprint recognition;Received signal strength indicator","","1","","23","IEEE","30 May 2022","","","IEEE","IEEE Conferences"
"Literature Review On Metaheuristics Techniques In The Health Care Industry","A. Gjecka; M. Fetaji","Faculty of Contemporary Sciences and Technologies, SEE University, Tetovo, North Macedonia; Faculty of Contemporary Sciences and Technologies, SEE University, Tetovo, North Macedonia",2023 12th Mediterranean Conference on Embedded Computing (MECO),"26 Jun 2023","2023","","","1","8","In recent times, machine learning has provided increasingly satisfying results in the field of medicine, providing results with very high accuracy while helping to reduce costs and diagnose the disease in real time. To achieve this, it is necessary to develop different deep machine learning techniques. Some of these are metaheuristic techniques that offer practical solutions for different types of chronic diseases. These types of algorithms have received the most attention in solving optimization problems. Therefore, this paper presents a wide review of the literature for solving the problems of feature selection using metaheuristic algorithms and selecting those that have had the highest performance compared to the results given by other algorithms. In this paper, a study of 71 articles from a research database was carried out, from which metaheuristic algorithms were analyzed and evidenced on the optimization and selection of features for the prediction of chronic diseases using numerical, binary, or even imaging data. The efficiency of the algorithms is measured based on the accuracy results, error rate, F-means, or other parameters or graphical representations found in this study. This work will help researchers to improve any of the methods, hybridize them, or even build applications for predicting diseases in the future. Gaps in this field have also been identified, and future studies should be conducted.","2637-9511","979-8-3503-2291-0","10.1109/MECO58584.2023.10155079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10155079","Metaheuristics;chronic diseases;optimization methods;machine learning","Industries;Machine learning algorithms;Metaheuristics;Measurement uncertainty;Medical services;Prediction algorithms;Feature extraction","","","","70","IEEE","26 Jun 2023","","","IEEE","IEEE Conferences"
"A Communication-Efficient Federated Learning Scheme for IoT-Based Traffic Forecasting","C. Zhang; L. Cui; S. Yu; J. J. Q. Yu","Department of Computer Science and Engineering, Guangdong Provincial Key Laboratory of Brain-inspired Intelligent Computation, Southern University of Science and Technology, Shenzhen, China; Faculty of Engineering and Information Technology, University of Technology Sydney, Sydney, NSW, Australia; Faculty of Engineering and Information Technology, University of Technology Sydney, Sydney, NSW, Australia; Department of Computer Science and Engineering, Guangdong Provincial Key Laboratory of Brain-inspired Intelligent Computation, Southern University of Science and Technology, Shenzhen, China",IEEE Internet of Things Journal,"6 Jul 2022","2022","9","14","11918","11931","Federated learning (FL) is widely adopted in traffic forecasting tasks involving large-scale IoT-enabled sensor data since its decentralization nature enables data providers’ privacy to be preserved. When employing state-of-the-art deep learning-based traffic predictors in FL systems, the existing FL frameworks confront overlarge communication overhead when transmitting these models’ parameter updates since the modeling depth and breadth renders them incorporating an enormous number of parameters. In this article, we propose a practical FL scheme, namely, Clustering-based hierarchical and Two-step-optimized FL (CTFed), to tackle this issue. The proposed scheme follows a divide et impera strategy that clusters the clients into multiple groups based on the similarity between their local models’ parameters. We integrate the particle swarm optimization algorithm and devises a two-step approach for local model optimization. This scheme enables only one but representative local model update from each cluster to be uploaded to the central server, thus reduces the communication overhead of the model updates transmission in FL. CTFed is orthogonal to the gradient compression- or sparsification-based approaches so that they can orchestrate to optimize the communication overhead. Extensive case studies on three real-world data sets and three state-of-the-art models demonstrate the outstanding training efficiency, accurate prediction performance, and robustness to unstable network environments of the proposed scheme.","2327-4662","","10.1109/JIOT.2021.3132363","Stable Support Plan Program of Shenzhen Natural Science Fund(grant numbers:20200925155105002); General Program of Guangdong Basic and Applied Basic Research Foundation(grant numbers:2019A1515011032); Guangdong Provincial Key Laboratory of Brain-inspired Intelligent Computation(grant numbers:2020B121201001); Australia ARC(grant numbers:DP200101374,LP190100676); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9634121","Communication efficiency;federated learning (FL);graph neural networks (GNNs);industrial IoT;traffic forecasting","Forecasting;Predictive models;Training;Servers;Data models;Organizations;Convolutional neural networks","","18","","53","IEEE","3 Dec 2021","","","IEEE","IEEE Journals"
"Research on Automatic Essay Scoring of Composition Based on CNN and OR","Z. Chen; Y. Zhou","School of Computer Science and Software Engineering, East China Normal University, Shanghai, China; School of Computer Science and Software Engineering, East China Normal University, Shanghai, China",2019 2nd International Conference on Artificial Intelligence and Big Data (ICAIBD),"16 Sep 2019","2019","","","13","18","Manual scoring may be affected by unconscious factors such as subjective judgment of the reviewer, which may lead to the deviation of scores. Establishing an objective, systematic automatic comment system is of great significance to solve this problem. Most of the existing automatic scoring systems are based on a single neural network or based on feature extraction. The accuracy of the formers need to be improved, and the latter require a lot of time to manually extract features. After exploring and analyzing convolutional neural networks (CNN) and ordinal regression (OR), we propose an automatic essay scoring approach based on a combination of CNN and OR for the characteristics of automatic scoring mechanism. Through the deep learning framework Keras to realize the designing, the experimental results demonstrate that the proposed model has a great improvement on the accuracy and efficiency of the automatic essay scoring than existing methods.","","978-1-7281-0831-5","10.1109/ICAIBD.2019.8837007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8837007","deep learning;automatic essay scoring;convolutional neural networks;Keras;ordinal regression","Feature extraction;Convolution;Convolutional neural networks;Manuals;Encoding;Data models","","9","","18","IEEE","16 Sep 2019","","","IEEE","IEEE Conferences"
"CT Image Classification of Pulmonary Nodules Based on Deep Transfer Learning and Inception-Res-V2","R. Wang","School of Computer and Information Engineering, Harbin University of Commerce, Harbin, China",2023 International Conference on Advances in Electrical Engineering and Computer Applications (AEECA),"9 May 2024","2023","","","576","582","The CT image classification method of pulmonary nodules based on deep convolutional neural network has a great advantage in performance compared with the traditional method. However, deep convolutional neural networks require a large number of labeled images for training. In practical applications, the number of labeled lung nodules CT images is often difficult to meet the requirements of network training. In this paper, an inception-res-v2 deep convolutional neural network model based on transfer learning is proposed to classify CT images of pulmonary nodules. First, the deep convolutional neural network model is pre-trained on the large scale dataset ImageNet, then the transfer learning method is used to fine tune the network with few pulmonary nodule CT images to solve the problem of insufficient training data. In this paper, the classify accuracy of CT images of pulmonary nodules of two deep neural networks: googlenet and inception-res-v2 were compared under the condition of transfer learning. In addition, the effects of different optimization algorithms on the classification results of transfer learning are also studied. Experiments show that the main factor affecting the final classification accuracy of CT images of pulmonary nodules is the network structure. Regardless of the optimization algorithm used, the inceptionres-v2 network has higher classification accuracy. The inception-res-v2 deep convolutional neural network model based on transfer learning proposed in this paper can be applied to other image classification and target detection tasks, and has important application value.","","979-8-3503-0808-2","10.1109/AEECA59734.2023.00108","Harbin University of Commerce; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10512063","deep learning;transfer learning;inception-res-v2;CT images;pulmonary nodules","Training;Computed tomography;Transfer learning;Lung;Training data;Classification algorithms;Convolutional neural networks","","","","11","IEEE","9 May 2024","","","IEEE","IEEE Conferences"
"Meta-WF: Meta-Learning-Based Few-Shot Wireless Impersonation Detection for Wi-Fi Networks","T. Li; Z. Hong; L. Liu; Z. Wen; L. Yu","Institute of Cyberspace Security, Zhejiang University of Technology, Hangzhou, China; Institute of Cyberspace Security, Zhejiang University of Technology, Hangzhou, China; Institute of Cyberspace Security, Zhejiang University of Technology, Hangzhou, China; Institute of Cyberspace Security, Zhejiang University of Technology, Hangzhou, China; Institute of Cyberspace Security, Zhejiang University of Technology, Hangzhou, China",IEEE Communications Letters,"10 Nov 2021","2021","25","11","3585","3589","As the common wireless open medium of the Internet of Things (IoT), Wi-Fi networks are often suffering from impersonation attacks. For the purpose of Wi-Fi impersonation attack detection (WIAD), recently, deep learning-based techniques provide superior detection performance. However, considering the dynamics of the WIAD task, these methods require retraining deep learning network (DNN) through large-scale samples and long training time to adapt to new WIAD tasks. In this letter, we propose a meta-learning-based Wi-Fi impersonation detection (Meta-WF) algorithm, which learns knowledge from historical multiple WIAD scenarios and quickly adapts to new WIAD scenarios with few-shot samples. Numerical results show that the proposed algorithm is capable of achieving 98% accuracy for new WIAD tasks by 1-step fine-tune using less than 0.1% (50-shot) of the training set samples.","1558-2558","","10.1109/LCOMM.2021.3112518","National Natural Science Foundation of China(grant numbers:62072408,62073292); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LY20F020030); New Century 151 Talent Project of Zhejiang Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551984","Internet of Things (IoT);Wi-Fi network;impersonation attack;deep learning;meta-learning","Task analysis;Feature extraction;Wireless fidelity;Training;Protocols;Deep learning;Heuristic algorithms","","5","","22","IEEE","29 Sep 2021","","","IEEE","IEEE Journals"
"Fault Diagnosis Method of Link Control System for Gravitational Wave Detection","A. Gao; S. Xu; Z. Zhao; H. Shang; R. Xu","School of Aerospace Engineering, Beijing Institute of Technology, Beijing, China; School of Aerospace Engineering, Beijing Institute of Technology, Beijing, China; School of Aerospace Engineering, Beijing Institute of Technology, Beijing, China; School of Aerospace Engineering, Beijing Institute of Technology, Beijing, China; School of Aerospace Engineering, Beijing Institute of Technology, Beijing, China",Journal of Systems Engineering and Electronics,"21 Aug 2024","2024","35","4","922","931","To maintain the stability of the inter-satellite link for gravitational wave detection, an intelligent learning monitoring and fast warning method of the inter-satellite link control system failure is proposed. Different from the traditional fault diagnosis optimization algorithms, the fault intelligent learning method proposed in this paper is able to quickly identify the faults of inter-satellite link control system despite the existence of strong coupling nonlinearity. By constructing a two-layer learning network, the method enables efficient joint diagnosis of fault areas and fault parameters. The simulation results show that the average identification time of the system fault area and fault parameters is 0.27 s, and the fault diagnosis efficiency is improved by 99.8% compared with the traditional algorithm.","1004-4132","","10.23919/JSEE.2024.000048","National Key Research and Development Program Topics(grant numbers:2020YFC2200902); National Natural Science Foundation of China(grant numbers:11872110); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10643183","large scale multi-satellite formation;gravitational wave detection;laser link monitoring;fault diagnosis;deep learning","Fault diagnosis;Couplings;Space vehicles;Simulation;Gravitational waves;Control systems;Stability analysis","","","","31","","21 Aug 2024","","","BIAI","BIAI Journals"
"Research on Dung Beetle Optimization Based Stacked Sparse Autoencoder for Network Situation Element Extraction","Y. Yang; P. Zhao","School of Big Data and Artificial Intelligence, Chizhou University, Chizhou, China; School of Big Data and Artificial Intelligence, Chizhou University, Chizhou, China",IEEE Access,"21 Feb 2024","2024","12","","24014","24026","Network security situation awareness enables networks to actively and effectively defend against network attacks, relying on the extraction of network situation elements as an initial and decisive step. In existing studies, the stacked sparse autoencoder (SSAE) has been employed to extract features from unlabeled network flows. However, obtaining the optimal hyperparameter combination is challenging due to its numerous hyperparameters. To address this issue, we propose a novel approach named DBO-SSAE that leverages dung beetle optimization (DBO) to select the optimal hyperparameters for SSAE automatically. Applied to the well-known UNSW-NB15 dataset, our model yields an optimal feature subset, which is evaluated across various binary classifiers with different metrics. Experimental results demonstrate that our approach improves accuracy and  $\textit{F}_{1}$ -measure by 0.2% to 1.5% while reducing the false negative rate (FNR) and false positive rate (FPR) by 0.06% to 7%, surpassing other feature extraction methods on the same classifier for the UNSW-NB15 dataset. Particularly, in conjunction with a lightweight bidirectional long short-term memory (BiLSTM), our model achieves metrics of 98.84% accuracy, 98.96%  $\textit{F}_{1}$ -measure, 1.86% FNR, and 0.6% FPR. This study could provide novel insights into the effective representation of network situation elements and lay the groundwork for a high-efficiency intrusion detection system.","2169-3536","","10.1109/ACCESS.2024.3365495","Chizhou University(grant numbers:CZ2022ZRZ05); Natural Science Research Key Project of the Education Department of Anhui Province, China(grant numbers:2022AH051828); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10433445","Dung beetle optimization;network security;network situation element extraction;stacked sparse autoencoder","Feature extraction;Principal component analysis;Optimization;Intrusion detection;Telecommunication traffic;Classification algorithms;Support vector machines;Encoding;Network security","","1","","67","CCBYNCND","13 Feb 2024","","","IEEE","IEEE Journals"
"The Comparison of Some Algorithm Based on Ceemdan","S. Wang; Y. Shao; J. Qian; S. Sun; S. Yu","School of Computer Science, University of Nottingham Ningbo China, Ningbo, Zhejiang, China; International Department, Affiliated High School of South China Normal University, Guangzhou, Guangdong, China; Ningbo Huamao International School, Ningbo, Zhejiang, China; Suzhou Foreign Language School, Suzhou, Jiangsu, China; Justin-Siena High School, Napa Valley, California, the United States of America",2021 2nd International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),"3 Feb 2022","2021","","","210","217","Stock price prediction is an indispensable part of the investment market and lots of approaches have been proposed. This article implements Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN) to decompose the dataset, because the time series data were usually nonlinear and nonstationary. Meanwhile, our algorithms do not have to decide the basic function. In addition, different algorithms are adopted to process different levels of Intrinsic Mode Function (IMF) in order to capture their different characteristics. In that case, a combined algorithm containing CEEMDAN, MLP-BP, LSTM/GRU and Linear Regression is used. The combined algorithm works well on Tesla’s price series from 2010 to 2020, and it can not only process the data with a high accuracy, but also has a short running time because some of the algorithms in the combination are very simple. Then the combined algorithm is adopted on a more representative dataset called S&P 500 index. It turns out it is still very accurate and fast.","","978-1-6654-2709-8","10.1109/ICBASE53849.2021.00047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9696159","Stock forecasting;CEEMDAN;MLP-BP;LSTM;Time-Series","Runtime;Software algorithms;Linear regression;Time series analysis;Prediction algorithms;Market research;Indexes","","","","15","IEEE","3 Feb 2022","","","IEEE","IEEE Conferences"
"A PID Controller Approach for Stochastic Optimization of Deep Networks","W. An; H. Wang; Q. Sun; J. Xu; Q. Dai; L. Zhang","Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong, China; Graduate School at Shenzhen, Tsinghua University, Shenzhen, China; Stanford University, CA, USA; Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong, China; Graduate School at Shenzhen, Tsinghua University, Shenzhen, China; Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong, China",2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,"16 Dec 2018","2018","","","8522","8531","Deep neural networks have demonstrated their power in many computer vision applications. State-of-the-art deep architectures such as VGG, ResNet, and DenseNet are mostly optimized by the SGD-Momentum algorithm, which updates the weights by considering their past and current gradients. Nonetheless, SGD-Momentum suffers from the overshoot problem, which hinders the convergence of network training. Inspired by the prominent success of proportional-integral-derivative (PID) controller in automatic control, we propose a PID approach for accelerating deep network optimization. We first reveal the intrinsic connections between SGD-Momentum and PID based controller, then present the optimization algorithm which exploits the past, current, and change of gradients to update the network parameters. The proposed PID method reduces much the overshoot phenomena of SGD-Momentum, and it achieves up to 50% acceleration on popular deep network architectures with competitive accuracy, as verified by our experiments on the benchmark datasets including CIFAR10, CIFAR100, and Tiny-ImageNet.","2575-7075","978-1-5386-6420-9","10.1109/CVPR.2018.00889","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8578987","","Optimization;Training;Acceleration;PD control;PI control;Neural networks","","60","","38","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"The Key Technologies to Realize the Profiling and Positioning of Key Personnel Based on Public Information Such as Social Accounts and IP","K. Cheng; Q. Wang; Z. Wu; L. Tan; D. Xiao; C. Zou","Central China Branch of State Grid Corporation of China, Wuhan, Hubei, China; Central China Branch of State Grid Corporation of China, Wuhan, Hubei, China; Central China Branch of State Grid Corporation of China, Wuhan, Hubei, China; Central China Branch of State Grid Corporation of China, Wuhan, Hubei, China; Central China Branch of State Grid Corporation of China, Wuhan, Hubei, China; Central China Branch of State Grid Corporation of China, Wuhan, Hubei, China",2022 IEEE 2nd International Conference on Mobile Networks and Wireless Communications (ICMNWC),"7 Feb 2023","2022","","","1","4","In recent years, with the rapid development and application of emerging technologies such as the Internet, cloud computing, and big data, there has been a substantial increase in threats to intrusions into networks and information systems. Plays an important role in preventing security threats and protecting the network from attacks. Over the years, attackers and defenders have been at war with each other, with new, combined or higher-level attack patterns emerging. Based on public information such as social accounts and IP, this paper describes network intrusion attacks from multiple dimensions such as attackers, attack targets, and attack methods. The key technology for the complete construction and positioning of the portrait. At the same time, this paper combines the advantages of deep learning and generative methods to build a deep generative model to detect and identify attacks. The construction of attacker portraits has become a network intrusion detection and defense process, analyzing the attacker’s intention, and effectively restoring and predicting the attack process. It is a good solution to the problems that traditional machine learning methods suffer from low detection rate and high false alarm rate. The final results of the research show that the processing time can be greatly reduced after the algorithm is optimized. The processing time of IP addresses is determined by The increase from 266s to 636s is because each word segment needs to be compared with all addresses in the address library, and the processing time of all programs increases rapidly with the number of statements.","","978-1-6654-9111-2","10.1109/ICMNWC56175.2022.10031850","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10031850","security threat;public information;attacker portrait;key personnel","Wireless communication;Face recognition;Network intrusion detection;Resists;Network security;Prediction algorithms;Real-time systems","","","","12","IEEE","7 Feb 2023","","","IEEE","IEEE Conferences"
"Intelligent Control of Automated Microelectronic Production Lines Based on Artificial Intelligence","H. Zhang","School of Information, North China University of Technology, Beijing, China",2023 International Conference on Applied Intelligence and Sustainable Computing (ICAISC),"9 Aug 2023","2023","","","1","6","Artificial intelligence is currently the most popular technology. With the advent of the era of big data, information technology based on artificial intelligence has become a new type of technology capable of most of the human labor force. Its scope of application is mainly in the comprehensive processing of improving efficiency, and those companies that need to improve efficiency do need corresponding technical support. Through artificial intelligence technology, the production line can analyze and predict the production process, so as to optimize and upgrade the production process. At the same time, intelligent production lines can also realize real-time monitoring and analysis of production data through big data analysis technology, thereby improving the reliability and stability of the production process. Automated control and optimization adjustments can also be realized in the production process. Through the intelligent control system, the production line can automatically complete the processing, assembly, testing and other work of the product, and the production process and process parameters can be automatically adjusted according to the production situation, so as to realize the autonomy and intelligence of the production process. This article first introduces the basic framework of artificial intelligence in detail, then applies artificial intelligence to the research of intelligent control of automated microelectronics production lines, conducts data tests on the overall efficiency, and finally uses questionnaires on the production personnel of the waterline and the overall efficiency of the company. It can be shown that artificial intelligence is of great help to improve the efficiency of production lines. Therefore, making good use of artificial intelligence is a very worthy topic for the research of intelligent control of automated microelectronics production lines.","","979-8-3503-2379-5","10.1109/ICAISC58445.2023.10200046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10200046","artificial intelligence;automation;electronic production line;intelligent control","Ethics;Government;Process control;Production;Companies;Big Data;Microelectronics","","1","","19","IEEE","9 Aug 2023","","","IEEE","IEEE Conferences"
"Evolutionary Algorithm-Based and Network Architecture Search-Enabled Multiobjective Traffic Classification","X. Wang; X. Wang; L. Jin; R. Lv; B. Dai; M. He; T. Lv","School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Department of Statistics, Colorado State University, Fort Collins, CO, USA; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China",IEEE Access,"8 Apr 2021","2021","9","","52310","52325","Network traffic classification technology plays an important role in network security management. However, the inherent limitations of traditional methods have become increasingly obvious, and they cannot address existing traffic classification tasks. Very recently, neural architecture search (NAS) has aroused widespread interest as a tool to automate the manual architecture construction process. To this end, this paper proposes NAS based on multiobjective evolutionary algorithms (MOEAs) to classify malicious network traffic. The main purpose is to simplify the search space by reducing the spatial ratio and number of channels of the model. In addition, the search strategy is changed in the effective search space, and the utilized strategies include EAs with the nondominated sorting genetic algorithm with the elite retention strategy (NSGA-II), strength Pareto evolutionary algorithm (SPEA-II) and multiobjective particle swarm optimization (MOPSO) to solve the formulated multiobjective NAS. Through comprehensive comparison of the population convergence times, model accuracies, Pareto optimality sets, model complexities and running speeds of the strategies, it is concluded that the model based on NSGA-II search has the best performance. The experimental results of the current machine learning algorithms and artificial learning methods based on the network are compared, showing that our method achieved better classification performance on two public datasets with a lower computational complexity, as mainly measured by FLOPs. Our approach is able to achieve 99.806% and 99.369% F1-score with 11.501 MB and 4.718 MB FLOPs on both IDS2012 and ISCX VPN dataset respectively.","2169-3536","","10.1109/ACCESS.2021.3068267","National Natural Science Foundation of China(grant numbers:61871046); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9383257","Deep learning;multiobjective;neural architecture search;traffic classification","Computer architecture;Search problems;Task analysis;Feature extraction;Network architecture;Microprocessors;Data models","","7","","72","CCBYNCND","23 Mar 2021","","","IEEE","IEEE Journals"
"Bone age assessment using convolutional neural networks","S. Wang; Y. Shen; D. Zeng; Y. Hu","Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Department of Orthopaedics and Traumatology, The University of Hong Kong, Hong Kong",2018 International Conference on Artificial Intelligence and Big Data (ICAIBD),"28 Jun 2018","2018","","","175","178","In this work, an automated skeletal maturity recognition system is proposed. It first accurately detects the distal radius and ulna (DRU) areas from hand and wrist X-ray images by a faster region-based convolutional neural network model. Then, a well-tuned convolutional neural network (CNN) classification model is applied to estimate the bone ages. We discussed the model performance according to various network configurations. After parameter optimization, the proposed model finally achieved 92% and 88% accuracy for radius and ulna, respectively.","","978-1-5386-6987-7","10.1109/ICAIBD.2018.8396189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8396189","convolutional neural network;skeletal maturity;classification","Urban areas;Bones;Machine learning;Classification algorithms;Convolutional neural networks;Wrist;Radiography","","28","","20","IEEE","28 Jun 2018","","","IEEE","IEEE Conferences"
"A Intrusion Detection Model Based on Convolutional Neural Network and Feature Selection","L. Zhang; C. Xu","School of Cyber Security and information law, Chongqing University of Posts and Telecommunications, ChongQing, China; School of Cyber Security and information law, Chongqing University of Posts and Telecommunications, ChongQing, China",2022 5th International Conference on Artificial Intelligence and Big Data (ICAIBD),"12 Jul 2022","2022","","","162","167","With the rapid development of the Internet, the network security situation has become increasingly severe. Attackers around the world have caused thousands of economic losses to government and enterprises. Novel attack methods have brought huge challenges to traditional intrusion detection systems. This paper proposes an intrusion detection model based on deep learning. First it uses the Sigmoid pigeon optimization algorithm(SPIO) to select the features of the data samples, then uses a Three-layer convolutional neural network trains and validates the dataset. The model is tested on NSL-KDD dataset. Experimental results show that this model has a higher accuracy rate than the existing traditional detection models.","","978-1-6654-9913-2","10.1109/ICAIBD55127.2022.9820384","Chongqing Municipal Education Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820384","SPIO;convolutional neural network;NSL-KDD;IDS","Economics;Deep learning;Government;Intrusion detection;Network security;Feature extraction;Data models","","4","","20","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"An Algorithm–Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers","C. Fang; A. Zhou; Z. Wang","School of Electronic Science and Engineering, Nanjing University, Nanjing, China; CUHK-Sensetime Joint Laboratory, The Chinese University of Hong Kong (CUHK), Hong Kong, Shatin, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China",IEEE Transactions on Very Large Scale Integration (VLSI) Systems,"20 Oct 2022","2022","30","11","1573","1586","The Transformer has been an indispensable staple in deep learning. However, for real-life applications, it is very challenging to deploy efficient Transformers due to the immense parameters and operations of models. To relieve this burden, exploiting sparsity is an effective approach to accelerate Transformers. Newly emerging Ampere graphics processing units (GPUs) leverage a 2:4 sparsity pattern to achieve model acceleration, while it can hardly meet the diverse algorithm and hardware constraints when deploying models. By contrast, we propose an algorithm–hardware co-optimized framework to flexibly and efficiently accelerate Transformers by utilizing general N:M sparsity patterns. First, from an algorithm perspective, we propose a sparsity inheritance mechanism along with inherited dynamic pruning (IDP) to obtain a series of N:M sparse candidate Transformers rapidly. A model compression scheme is further proposed to significantly reduce the storage requirement for deployment. Second, from a hardware perspective, we present a flexible and efficient hardware architecture, namely, STA, to achieve significant speedup when deploying N:M sparse Transformers. STA features not only a computing engine unifying both sparse–dense and dense–dense matrix multiplications with high computational efficiency but also a scalable softmax module eliminating the latency from intermediate off-chip data communication. Experimental results show that, compared to other methods, N:M sparse Transformers, generated using IDP, achieves an average of 6.7% improvement on accuracy with high training efficiency. Moreover, STA can achieve  $14.47\times $  and  $11.33\times $  speedups compared to Intel i9-9900X and NVIDIA RTX 2080 Ti, respectively, and perform  $2.00 \,\,\sim 19.47 \times $  faster inference than the state-of-the-art field-programmable gate array (FPGA)-based accelerators for Transformers.","1557-9999","","10.1109/TVLSI.2022.3197282","National Natural Science Foundation of China(grant numbers:62174084,62104097); High-Level Personnel Project of Jiangsu Province(grant numbers:JSSCBS20210034); Key Research Plan of Jiangsu Province of China(grant numbers:BE2019003-4); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857911","Algorithm–hardware codesign;hardware accelerator;model compression;pruning;Transformer","Transformers;Hardware;Computational modeling;Field programmable gate arrays;Optimization;Engines;Sparse matrices","","22","","42","IEEE","16 Aug 2022","","","IEEE","IEEE Journals"
"Privacy-preserving deep learning","R. Shokri; V. Shmatikov","UT, Austin; Cornell Tech","2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton)","7 Apr 2016","2015","","","909","910","Deep learning based on artificial neural networks is a very popular approach to modeling, classifying, and recognizing complex data such as images, speech, and text. The unprecedented accuracy of deep learning methods has turned them into the foundation of new AI-based services on the Internet. Commercial companies that collect user data on a large scale have been the main beneficiaries of this trend since the success of deep learning techniques is directly proportional to the amount of data available for training. Massive data collection required for deep learning presents obvious privacy issues. Users' personal, highly sensitive data such as photos and voice recordings is kept indefinitely by the companies that collect it. Users can neither delete it, nor restrict the purposes for which it is used. Furthermore, centrally kept data is subject to legal subpoenas and extrajudicial surveillance. Many data owners-for example, medical institutions that may want to apply deep learning methods to clinical records-are prevented by privacy and confidentiality concerns from sharing the data and thus benefitting from large-scale deep learning. In this paper, we present a practical system that enables multiple parties to jointly learn an accurate neural-network model for a given objective without sharing their input datasets. We exploit the fact that the optimization algorithms used in modern deep learning, namely, those based on stochastic gradient descent, can be parallelized and executed asynchronously. Our system lets participants train independently on their own datasets and selectively share small subsets of their models' key parameters during training. This offers an attractive point in the utility/privacy tradeoff space: participants preserve the privacy of their respective data while still benefitting from other participants' models and thus boosting their learning accuracy beyond what is achievable solely on their own inputs. We demonstrate the accuracy of our privacy-preserving deep learning on benchmark datasets.","","978-1-5090-1824-6","10.1109/ALLERTON.2015.7447103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7447103","","Training;Privacy;Machine learning;Data models;Decision support systems;Companies;Data privacy","","152","1","1","IEEE","7 Apr 2016","","","IEEE","IEEE Conferences"
"Leverage Learning Behaviour Data for Students' Learning Performance Prediction and Influence Factor Analysis","Q. Ni; Y. Zhu; L. Zhang; X. Lu; L. Zhang","Key Laboratory of Multilingual Education with AI, Shanghai International Studies University, Shanghai, China; College of Information Science and Technology, Donghua University, Shanghai, China; Shanghai Engineering Research Center of Intelligent Education and Bigdata and Lab for Educational Big Data and Policymaking, Shanghai Normal University, Shanghai, China; College of Information Science and Technology, Donghua University, Shanghai, China; College of Information Science and Technology, Donghua University, Shanghai, China",IEEE Transactions on Artificial Intelligence,"16 May 2024","2024","5","5","2422","2433","Online education has become increasingly significant for university students and faculty, especially in the context of the modern remote education landscape. However, the inherent space-time separation in online education can create communication delays between teachers and students, making it challenging to monitor students' behaviors effectively. In an effort to understand the connection between students' online engagement and their learning performance, data annotation has been implemented to address the issue of accurately representing students' learning behaviors. Taking the online teaching data of Shanghai Normal University platform as the research object, the primary online education problem is explored through data mining, which includes correlation analysis, Gini importance ranking, and principal component analysis (PCA). Then, constructing the learning performance prediction model using random forest (RF) based on PCA by comparing various machine learning algorithms. As a consequence, the most influential online learning behaviors are course duration time, document learning time, test average score, and video completion rate. The overall classification accuracy of the learning performance prediction model is 87.45%, and the highest prediction accuracy for a single category is 96.52%.","2691-4581","","10.1109/TAI.2023.3320118","National Natural Science Foundation of China(grant numbers:62371118,6210020445,61901104); Natural Science Foundation of Shanghai(grant numbers:21ZR1446900); Shanghai Baoshan Future Learning Research and Development Centre(grant numbers:21511100102); Science and Technology Research Project of Shanghai Songjiang District(grant numbers:20SJKJGG4C); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10266676","Classification prediction;correlation analysis;data mining;learning behavior representation;random forest (RF)","Behavioral sciences;Education;Data mining;Predictive models;Analytical models;Artificial intelligence;Task analysis","","","","27","IEEE","28 Sep 2023","","","IEEE","IEEE Journals"
"Dueling-DDQN Based Virtual Machine Placement Algorithm for Cloud Computing Systems","J. Yan; J. Xiao; X. Hong","School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China",2021 IEEE/CIC International Conference on Communications in China (ICCC),"8 Nov 2021","2021","","","294","299","Virtual machine placement (VMP) in large-scale cloud computing clusters is a challenging problem with practical importance. Deep Q-learning (DQN) based algorithm is a promising means to solve difficult VMP problems with complex optimization goals and dynamically changing environments. However, native DQN algorithms suffer from shortcomings such as Q value overestimation, difficulty in convergence, and failure to maximize long-term reward. To overcome these shortcomings, this paper proposes an advanced VMP algorithm based on Dueling-DDQN. Moreover, specific optimization techniques are introduced to enhance the exploration strategy and the capability of achieving long-term reward. Experiment results show that the proposed algorithm outperforms native DQN in terms of convergence speed, Q-value estimation accuracy and stability. Meanwhile, the proposed algorithm can achieve multiple optimization goals such as reducing power consumption, ensuring resource load balance and Improving user service Quality.","2377-8644","978-1-6654-4385-2","10.1109/ICCC52777.2021.9580393","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9580393","cloud computing;virtual machine placement;deep reinforcement learning","Cloud computing;Power demand;Heuristic algorithms;Estimation;Clustering algorithms;Reinforcement learning;Load management","","2","","20","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"Faster Person Re-Identification: One-Shot-Filter and Coarse-to-Fine Search","G. Wang; X. Huang; S. Gong; J. Zhang; W. Gao","School of Electronic and Computer Engineering, Peking University, Beijing, China; Beijing Jiaotong University, Beijing, China; School of Electronic Engineering and Computer Science, Queen Mary University of London, London, U.K.; School of Electronic and Computer Engineering, Peking University, Beijing, China; School of Electronic and Computer Engineering, Peking University, Beijing, China",IEEE Transactions on Pattern Analysis and Machine Intelligence,"3 Apr 2024","2024","46","5","3013","3030","Fast person re-identification (ReID) aims to search person images quickly and accurately. The main idea of recent fast ReID methods is the hashing algorithm, which learns compact binary codes and performs fast Hamming distance and counting sort. However, a very long code is needed for high accuracy (e.g., 2048), which compromises search speed. In this work, we introduce a new solution for fast ReID by formulating a novel Coarse-to-Fine (CtF) hashing code search strategy, which complementarily uses short and long codes, achieving both faster speed and better accuracy. It uses shorter codes to coarsely rank broad matching similarities and longer codes to refine only a few top candidates for more accurate instance ReID. Specifically, we design an All-in-One (AiO) module together with a Distance Threshold Optimization (DTO) algorithm. In AiO, we simultaneously learn and enhance multiple codes of different lengths in a single model. It learns multiple codes in a pyramid structure, and encourage shorter codes to mimic longer codes by self-distillation. DTO solves a complex threshold search problem by a simple optimization process, and the balance between accuracy and speed is easily controlled by a single parameter. It formulates the optimization target as a $F_{\beta }$Fβ score that can be optimised by Gaussian cumulative distribution functions. Besides, we find even short code (e.g., 32) still takes a long time under large-scale gallery due to the $O(n)$O(n) time complexity. To solve the problem, we propose a gallery-size-free latent-attributes-based One-Shot-Filter (OSF) strategy, that is always $O(1)$O(1) time complexity, to quickly filter major easy negative gallery images, Specifically, we design a Latent-Attribute-Learning (LAL) module supervised a Single-Direction-Metric (SDM) Loss. LAL is derived from principal component analysis (PCA) that keeps largest variance using shortest feature vector, meanwhile enabling batch and end-to-end learning. Every logit of a feature vector represents a meaningful attribute. SDM is carefully designed for fine-grained attribute supervision, outperforming common metrics such as Euclidean and Cosine metrics. Experimental results on 2 datasets show that CtF+OSF is not only 2% more accurate but also $5\times$5× faster than contemporary hashing ReID methods. Compared with non-hashing ReID methods, CtF is $50\times$50× faster with comparable accuracy. OSF further speeds CtF by $2\times$2× again and upto $10\times$10× in total with almost no accuracy drop.","1939-3539","","10.1109/TPAMI.2023.3340923","National Natural Science Foundation of China(grant numbers:62202041); Shenzhen General Research Project(grant numbers:JCYJ20220531093215035); Fundamental Research Funds for the Central Universities(grant numbers:2023JBMC057); China Scholarship Council(grant numbers:201904910606); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10354027","Person re-identification;hashing;coarse-to-fine;latent attribute;one-shot-filter;computer vision;deep learning","Optimization;Search problems;Binary codes;Principal component analysis;Measurement;Hamming distances;Complexity theory","","","","85","IEEE","12 Dec 2023","","","IEEE","IEEE Journals"
"Enhancing Brain Tumor Classification with Optimized Convolutional Neural Networks","S. M. Saranya; D. Komarasamy; R. Dharshini; R. Gurudeepa; S. Mohanapriya; R. Dharani","Dept of CSE, Kongu Engineering College, Erode, India; Dept of CSE, Kongu Engineering College, Erode, Tamil Nadu, India; Dept of CSE, Kongu Engineering College, Erode, India; Dept of CSE, Kongu Engineering College, Erode, India; Dept of CSE, Kongu Engineering College, Erode, India; Dept of CSE, Kongu Engineering College, Erode, India",2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT),"23 Nov 2023","2023","","","1","6","Using software to recognize and categorize objects by using computer methods are technologies that are becoming more and more significant in a variety of fields, including the medical sector. The proven results show that accuracy in the medical field is improving through deep learning. Deep convolution networks are performing incredibly well at classifying and detecting objects. In our daily life, diseases keep on increasing which causes human death. In that way, Brain tumors are one among them. Around 28000 instances of brain tumors are recorded in India each year, by the International Association of Cancer Registries, and about 24000 individuals are said to die from brain tumors each year. So it is a significant requirement to perform classification in brain tumors. For better improvement of results, we used CNN to classify brain tumor images. Here the diseases are classified by utilizing various deep neural networks based on Convolutional Neural Networks such as EfficientNet, and DenseNet.To shape and mold the CNN model into its accurate form, we also used optimizers. It helps in reducing the overall loss and improving the accuracy. The commonly used optimizers are Adam, AdaGrad, AdaDelta, AdaMax, Nadam, Stochastic Gradient Descent, etc. The dataset consists of 4 classes of tumors and each category contains around 300 images for testing and training. The EfficientNet results in 97.62% and DenseNet with 90.77% of accuracy.","2473-7674","979-8-3503-3509-5","10.1109/ICCCNT56998.2023.10307287","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10307287","Deep Learning;CNN;EfficientNet;DenseNet;Optimizers","Training;Convolution;Shape;Stochastic processes;Computer architecture;Software;Convolutional neural networks","","","","10","IEEE","23 Nov 2023","","","IEEE","IEEE Conferences"
"Boosting Grid Efficiency and Resiliency by Releasing V2G Potentiality Through a Novel Rolling Prediction-Decision Framework and Deep-LSTM Algorithm","S. Li; C. Gu; J. Li; H. Wang; Q. Yang","Laboratory for Electric Vehicles, Beijing Institute of Technology, Beijing, China; Department of Electronic and Electrical Engineering, Centre for Sustainable Power Distribution, Bath, U.K.; Department of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; Department of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; Faculty of Engineering, Environment and Computing, Coventry University, Coventry, U.K.",IEEE Systems Journal,"7 Jun 2021","2021","15","2","2562","2570","The bidirectional link between the power grid and electric vehicles enables the flexible, cheap, and fast-responding application of vehicle batteries to provide services to the grid. However, in order to realize this, a critical issue that should be addressed first is how to predict and utilize vehicle-to-grid (V2G) schedulable capacity accurately and reasonably. This article proposes a novel V2G scheduling approach that considers predicted V2G capacity. First of all, with the concept of dynamic rolling prediction and deep long short term memory (LSTM) algorithm, a novel V2G capacity modeling and prediction method is developed. Then, this article designs a brand-new rolling prediction-decision framework for V2G scheduling to bridge the gap between optimization and forecasting phases, where the predicted information can be more reasonably and adequately utilized. The proposed methodologies are verified by numerical analysis, which illustrates that the efficiency and resiliency of the grid can be significantly enhanced with V2G services managed by the proposed methods.","1937-9234","","10.1109/JSYST.2020.3001630","National Natural Science Foundation of China(grant numbers:U1864202,51807008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9127494","Deep learning and vehicle to grid (V2G) schedulable capacity;electric vehicle (EV);long-short-term memory;rolling prediction and decision;V2G","Vehicle-to-grid;Prediction algorithms;Predictive models;Heuristic algorithms;Resilience;Scheduling","","45","","43","IEEE","29 Jun 2020","","","IEEE","IEEE Journals"
"Improved Multi-Agent Deep Deterministic Policy Gradient for Path Planning-Based Crowd Simulation","S. Zheng; H. Liu","Shandong Provincial Key Laboratory for Distributed Computer Software Novel Technology, Jinan, China; Shandong Provincial Key Laboratory for Distributed Computer Software Novel Technology, Jinan, China",IEEE Access,"21 Oct 2019","2019","7","","147755","147770","Deep reinforcement learning (DRL) has been proved to be more suitable than reinforcement learning for path planning in large-scale scenarios. In order to more effectively complete the DRL-based collaborative path planning in crowd evacuation, it is necessary to consider the space expansion problem brought by the increase of the number of agents. In addition, it is often faced with complicated circumstances, such as exit selection and congestion in crowd evacuation. However, few existing works have integrated these two aspects jointly. To solve this problem, we propose a planning approach for crowd evacuation based on the improved DRL algorithm, which will improve evacuation efficiency for large-scale crowd path planning. First, we propose a framework of congestion detection-based multi-agent reinforcement learning, the framework divides the crowd into leaders and followers and simulates leaders with a multi-agent system, it considers the congestion detection area is set up to evaluate the degree of congestion at each exit. Next, under the specification of this framework, we propose the improved Multi-Agent Deep Deterministic Policy Gradient (IMADDPG) algorithm, which adds the mean field network to maximize the returns of other agents, enables all agents to maximize the performance of a collaborative planning task in our training period. Then, we implement the hierarchical path planning method, which upper layer is based on the IMADDPG algorithm to solve the global path, and lower layer uses the reciprocal velocity obstacles method to avoid collisions in crowds. Finally, we simulate the proposed method with the crowd simulation system. The experimental results show the effectiveness of our method.","2169-3536","","10.1109/ACCESS.2019.2946659","National Natural Science Foundation of China(grant numbers:61876102,61472232,61272094); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8865095","Deep reinforcement learning;multi-agent reinforcement learning;path planning;crowd simulation for evacuation;improved multi-agent deep deterministic policy gradient algorithm","Path planning;Reinforcement learning;Planning;Collaboration;Multi-agent systems;Task analysis;Training","","54","","48","CCBY","14 Oct 2019","","","IEEE","IEEE Journals"
"Hybrid Optimization Algorithm for Detection of Security Attacks in IoT-Enabled Cyber-Physical Systems","A. Sagu; N. S. Gill; P. Gulia; I. Priyadarshini; J. M. Chatterjee","Department of Computer Science & Applications, Maharshi Dayanand University, Rohtak, India; Department of Computer Science & Applications, Maharshi Dayanand University, Rohtak, India; Department of Computer Science & Applications, Maharshi Dayanand University, Rohtak, India; School of Information, University of California, Berkeley, USA; Department of CSE, Graphic Era University, Dehradun, India",IEEE Transactions on Big Data,"","2024","PP","99","1","12","The Internet of Things (IoT) is being prominently used in smart cities and a wide range of applications in society. The benefits of IoT are evident, but cyber terrorism and security concerns inhibit many organizations and users from deploying it. Cyber-physical systems that are IoT-enabled might be difficult to secure since security solutions designed for general information/operational technology systems may not work as well in an environment. Thus, deep learning (DL) can assist as a powerful tool for building IoT-enabled cyber-physical systems with automatic anomaly detection. In this paper, two distinct DL models have been employed i.e., Deep Belief Network (DBN) and Convolutional Neural Network (CNN), considered hybrid classifiers, to create a framework for detecting attacks in IoT-enabled cyber-physical systems. However, DL models need to be trained in such a way that will increase their classification accuracy. Therefore, this paper also aims to present a new hybrid optimization algorithm called “Seagull Adapted Elephant Herding Optimization” (SAEHO) to tune the weights of the hybrid classifier. The “Hybrid Classifier + SAEHO” framework takes the feature extracted dataset as an input and classifies the network as either attack or benign. Using sensitivity, precision, accuracy, and specificity, two datasets were compared. In every performance metric, the proposed framework outperforms conventional methods.","2332-7790","","10.1109/TBDATA.2024.3372368","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10457946","Deep Learning;IoT Enabled Cyber-Physical Systems;Optimization Technique;cyberattacks;cyber-security;cyber-physical infrastructure","Internet of Things;Feature extraction;Security;Cyber-physical systems;Support vector machines;Metaheuristics;Convolutional neural networks","","1","","","IEEE","1 Mar 2024","","","IEEE","IEEE Early Access Articles"
"Two-Stage Short-Term Wind Speed Prediction Based on LSTM-LSSVM-CFA","L. Zhang; B. Wang; B. Fang; H. Ma; Z. Yang; Y. Xu","School of Electrical Engineering, Wuhan University, Wuhan, China; School of Electrical Engineering, Wuhan University, Wuhan, China; China Southern Power Grid, Power Dispatching and Control Center, Guangzhou, China; Tus-Institute for Renewable Energy, Qinghai University, Xining, China; School of Electrical Engineering, Wuhan University, Wuhan, China; School of Electrical Engineering, Wuhan University, Wuhan, China",2018 2nd IEEE Conference on Energy Internet and Energy System Integration (EI2),"20 Dec 2018","2018","","","1","6","Accurately predicting wind speed is vital to the large scale connection of wind power grids. This paper proposed a two-stage prediction way called LSTM-LSSVM-CFA (LSTMs-long short term memory networks, LSSVM-least squares support vector machine and CFA-chaos firefly algorithm) which may be an effective method to improve the accuracy of short-term wind speed prediction. Integrated learning of wind speed sequence based on LSTMs and LSSVM-CFA is the core technology in this new method. The prediction section include two main procedures. In the first stage, a couple of LSTMs with different hidden layers and different neurons processing guarantee the implied information of wind speed time series can be fully collected and avoid the weak generalization capability of single deep neural network learning. In the second stage, the prediction result of LSTMs-layer is transmitted to the LSSVM-based regression layer and its parameters are optimized by the CFA which has global search capabilities. Finally, the final integrated learning result of prediction is given by the LSSVM-layer. Based on actual historical wind speed data, we made use of single-LSTM, LSTM-LSSVM-PSO and LSTM-LSSVM-CFA to perform short-term wind speed prediction one hour ahead. Compared with single learning and other popular integrated learning models, experimental results show that the proposed LSTM-LSSVM-CFA can perform better.","","978-1-5386-8549-5","10.1109/EI2.2018.8582618","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8582618","two-stage wind speed prediction;integrated learning;long short term memory networks;least squares support vector machine;chaotic firefly algorithm","Wind speed;Autoregressive processes;Predictive models;Prediction algorithms;Optimization;Logic gates;Support vector machines","","5","","16","IEEE","20 Dec 2018","","","IEEE","IEEE Conferences"
"Improved Double Deep Q Network-Based Task Scheduling Algorithm in Edge Computing for Makespan Optimization","L. Zeng; Q. Liu; S. Shen; X. Liu","School of Computer Science, Nanjing University of Information Science and Technology, Nanjing, China; School of Software, Nanjing University of Information Science and Technology, Nanjing, China; School of Information Engineering, Huzhou University, Huzhou, China; School of Computing, Edinburgh Napier University, Edinburgh, UK",Tsinghua Science and Technology,"4 Dec 2023","2024","29","3","806","817","Edge computing nodes undertake an increasing number of tasks with the rise of business density. Therefore, how to efficiently allocate large-scale and dynamic workloads to edge computing resources has become a critical challenge. This study proposes an edge task scheduling approach based on an improved Double Deep Q Network (DQN), which is adopted to separate the calculations of target Q values and the selection of the action in two networks. A new reward function is designed, and a control unit is added to the experience replay unit of the agent. The management of experience data are also modified to fully utilize its value and improve learning efficiency. Reinforcement learning agents usually learn from an ignorant state, which is inefficient. As such, this study proposes a novel particle swarm optimization algorithm with an improved fitness function, which can generate optimal solutions for task scheduling. These optimized solutions are provided for the agent to pre-train network parameters to obtain a better cognition level. The proposed algorithm is compared with six other methods in simulation experiments. Results show that the proposed algorithm outperforms other benchmark methods regarding makespan.","1007-0214","","10.26599/TST.2023.9010058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10339723","edge computing;task scheduling;reinforcement learning;makespan;Double Deep Q Network (DQN)","Knowledge engineering;Scheduling algorithms;Reinforcement learning;Load management;Cognition;Virtual machining;Task analysis","","9","","34","","4 Dec 2023","","","TUP","TUP Journals"
"Research on feature extraction and classification algorithm of Marine environmental noise based on deep learning","S. Yang; L. Bo; L. Zhenwang; M. Zhaoran; W. Zixin","Dalian Scientific Test and Control Technology Institute, Dalian, China; Dalian Scientific Test and Control Technology Institute, Dalian, China; Dalian Scientific Test and Control Technology Institute, Dalian, China; Dalian Scientific Test and Control Technology Institute, Dalian, China; Dalian Scientific Test and Control Technology Institute, Dalian, China","2024 IEEE 4th International Conference on Electronic Technology, Communication and Information (ICETCI)","18 Jul 2024","2024","","","342","346","In the field of feature extraction and classification of Marine environmental noise, an innovative research work deeply integrates Long Short-Term Memory (LSTM) neural network with Variational Quantum Algorithms (VQA). This builds a new hybrid model known as Long Short Quantum Algorithms Memory (LSQAM). The aim of this research is to solve the problem of feature extraction and classification of high-dimensional and nonlinear noise signals in complex Marine environments. As a special type of recurrent neural network, LSTM has significant advantages in processing sequence data and capturing time-dependent relationships, and is especially suitable for analyzing Marine environmental noise signals containing long-term correlation and dynamic change characteristics. Through its unique gating mechanism, LSTM is able to efficiently store long-term information and filter out irrelevant interference, thereby extracting key spatio-temporal features from noisy signals. However, in the face of increasing data dimensions and highly complex noise patterns, traditional LSTM models may be limited by low computational efficiency and insufficient model generalization ability. To overcome these challenges, researchers introduce variable component subalgorithms. By making full use of the parallel computing power and quantum entanglement effect of quantum computers, VQA can significantly improve the solving speed of specific optimization problems, and show unique advantages for processing large-scale high-dimensional data. LSQAM model cleverly combines the advantages of LSTM and VQA. LSTM is responsible for extracting and capturing important local and global spatiotemporal features from the original ocean noise signals. Then, variable component subcircuits are used to encode and compress the high-dimensional features extracted by LSTM efficiently. At the same time, the feature space is deeply explored and optimized by combining quantum optimization algorithms to improve the learning efficiency and generalization performance of the model in the task of noise source classification. Practice has proved that the LSQAM model has successfully realized the in-depth mining and accurate classification of Marine environmental noise characteristics, which not only improves the accuracy of noise source identification, but also greatly reduces the need for computing resources. This novel approach of integrating artificial intelligence and quantum computing technology marks a strong combination of the two in practical problem solving, and has great significance for promoting technological innovation in many fields, such as Marine scientific research, environmental monitoring, and military applications. With the development of quantum computing hardware technology and theoretical research, LSQAM model is expected to play a more critical role in the future large-scale and more complex noise recognition scenarios, leading the study of Marine environmental noise into a new stage of development.","","979-8-3503-6164-3","10.1109/ICETCI61221.2024.10594535","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10594535","Noise extraction;short and long time memory neural network;data extraction","Computers;Working environment noise;Computational modeling;Noise;Feature extraction;Classification algorithms;Data mining","","","","10","IEEE","18 Jul 2024","","","IEEE","IEEE Conferences"
"Improving the Classification Performance of Asphalt Cracks After Earthquake With a New Feature Selection Algorithm","M. Yılmaz; E. Yalçın; S. Kifah; F. Demir; A. Şengür; R. Demir; R. M. Mehmood","Civil Engineering Department, Engineering Faculty, Fırat University, Elâzığ, Turkey; Civil Engineering Department, Engineering Faculty, Fırat University, Elâzığ, Turkey; School of Computing and Data Science, Xiamen University Malaysia, Sepang, Malaysia; Software Engineering Department, Engineering Faculty, Fırat University, Elâzığ, Turkey; Electric and Electronic Department, Fýrat University, Elâzığ, Turkey; Civil Engineering Department, Engineering Faculty, Fırat University, Elâzığ, Turkey; School of Computing and Data Science, Xiamen University Malaysia, Sepang, Malaysia",IEEE Access,"15 Jan 2024","2024","12","","6604","6614","Large-scale earthquakes can cause huge loss of life and material losses. After an earthquake, highways are the most commonly used type of transportation for the delivery of the necessary aid teams and materials to the scene of the event. If the highways are not well maintained, it may cause serious disruption of transportation after the earthquake or aftershocks. In this study, field studies were conducted in the provinces where the earthquake was felt severely after the earthquakes in Turkey on February 6, 2023. In these studies, images were collected according to the condition of asphalt cracks on the highways. These images were labeled as in need of urgent maintenance (Major) and not in need of urgent maintenance (Minor) and a new dataset was created. The classification performance of popular pre-trained CNN models is evaluated on this dataset. First, classification algorithms other than softmax were used to improve the classification performance. The Combined Metaheuristic Optimization-Relieff (CMO-R) algorithm was designed to improve the classification performance by one more level. Extensive experiments were conducted on the dataset, and the VGG16 model demonstrated superior performance, reaching an accuracy of 80.32% without encountering overfitting.","2169-3536","","10.1109/ACCESS.2023.3343619","Xiamen University Malaysia Research Fund (XMUMRF)(grant numbers:XMUMRF/2022-C9/IECE/0035); Tubitak 1002C Project(grant numbers:123D058); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10388226","Classification;deep learning;new asphalt cracks dataset;new feature selection algorithm","Asphalt;Feature extraction;Transfer learning;Earthquakes;Metaheuristics;Classification algorithms;Road transportation;Deep learning","","1","","37","CCBYNCND","10 Jan 2024","","","IEEE","IEEE Journals"
"Research on a Mine Gas Concentration Forecasting Model Based on a GRU Network","P. Jia; H. Liu; S. Wang; P. Wang","Department of Computer Science and Technology, Xi’an University of Science and Technology, Xi’an, China; Department of Computer Science and Technology, Xi’an University of Science and Technology, Xi’an, China; Shaanxi Coal Industry Chemical Technology Research Institute, Xi’an, China; Department of Safety Science and Engineering, Xi’an University of Science and Technology, Xi’an, China",IEEE Access,"2 Mar 2020","2020","8","","38023","38031","To improve the level of safety in coal mine production, it is important to enhance the accuracy of coal mine gas concentration prediction. In the context of deep learning, we proposed a mine gas concentration prediction model based on gated recurrent units (GRUs). The GRU model is not only simple in structure but also offers high prediction accuracy, and it can make full use of the time-series characteristic of mine gas concentration data. First, we apply the Pauta criterion and Lagrange interpolation to preprocess mine gas concentration monitoring data. Then, a spatial reconstruction method is used to construct the training set for the prediction model. Finally, the mean square error (MSE) is used as the loss function and adaptive moment estimation (Adam) is used as the optimization algorithm to determine the learning parameters of the GRU model for predicting gas concentration values. Experimental results show that compared with models based on support vector regression (SVR), a backpropagation neural network (BPNN), a recurrent neural network (RNN) and a long short-term memory (LSTM) network, the proposed GRU-based model for gas concentration prediction achieves reduced error on the test set, and moreover, the GRU model is more efficient than the LSTM model in terms of run time. Thus, the accuracy and efficiency of gas concentration prediction are both improved, showing that the proposed model is of high practical value.","2169-3536","","10.1109/ACCESS.2020.2975257","State Key Research Development Program of China(grant numbers:2018YFC0808303); National Natural Science Foundation of China(grant numbers:51974236); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9004607","Deep learning;gated recurrent unit;gas concentration;prediction","Predictive models;Fuel processing industries;Forecasting;Logic gates;Monitoring;Neurons;Coal mining","","33","","21","CCBY","20 Feb 2020","","","IEEE","IEEE Journals"
"Application of ELM Algorithm Incorporating AE Principles in Wireless Communication Signal Detection","S. Zhao; G. Yu; Y. Feng","College of Information Engineering, Fuyang Normal University, Fuyang, China; College of Information Engineering, Fuyang Normal University, Fuyang, China; College of Information Engineering, Fuyang Normal University, Fuyang, China",IEEE Access,"28 Aug 2023","2023","11","","89720","89732","With the development of society, the requirements for communication are getting higher and higher, but the detection performance of wireless communication signals is insufficient. To solve this problem, this paper combines the principle of Auto-encoder (AE) and the feature classification function of Extreme Learning Machine (ELM) algorithm to obtain AE-ELM algorithm, and builds a new wireless communication signal detection model based on this algorithm, hoping to improve the detection performance of wireless communication signals. The performance comparison experiment of the proposed AE-ELM algorithm shows that the bit error rate and average relative time complexity of the algorithm are 0.0004 and 0.5292s respectively, which is better than the comparison algorithm. Additionally, the effectiveness of the fusion algorithm-based signal detection model for wireless communication is investigated, and the findings demonstrate that this model outperforms the others with the lowest detection error rate of 0.0031. The above results demonstrate the high application potential of the novel wireless communication signal detection model and its ability to significantly improve the performance of wireless communication signal detection. Its application to wireless communication signal detection can greatly promote the development of the signal detection field in China.","2169-3536","","10.1109/ACCESS.2023.3306470","Anhui Province Universities Outstanding Talented Person Support Plan Project: Research on Channel Allocation and Resource Optimization of D2D Communication in 5G Networks(grant numbers:gxyq2021253); Key Project of Natural Science Research in Universities of Anhui Province: Research on Adaptive Isomeric Group Intelligent Optimization Algorithm and Its Application in Large-Scale Production Scheduling(grant numbers:2022AH052819); Key Projects of Natural Science Research in Universities of Anhui Province: Research on Vehicle Trajectory Extraction and Behavior Recognition Based on Complex Traffic Video Scene(grant numbers:KJ2020A1216); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10224237","AE;ELM;wireless communication;signal detection;deep learning","Wireless communication;Signal detection;Feature extraction;Classification algorithms;Training;Deep learning;Machine learning algorithms","","","","22","CCBYNCND","18 Aug 2023","","","IEEE","IEEE Journals"
"Learning-Based Application-Agnostic 3D NoC Design for Heterogeneous Manycore Systems","B. K. Joardar; R. G. Kim; J. R. Doppa; P. P. Pande; D. Marculescu; R. Marculescu","Washington State University, Pullman, WA, USA; Colorado State University, Fort Collins, CO, USA; Washington State University, Pullman, WA, USA; Washington State University, Pullman, WA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA",IEEE Transactions on Computers,"8 May 2019","2019","68","6","852","866","The rising use of deep learning and other big-data algorithms has led to an increasing demand for hardware platforms that are computationally powerful, yet energy-efficient. Due to the amount of data parallelism in these algorithms, high-performance three-dimensional (3D) manycore platforms that incorporate both CPUs and GPUs present a promising direction. However, as systems use heterogeneity (e.g., a combination of CPUs, GPUs, and accelerators) to improve performance and efficiency, it becomes more pertinent to address the distinct and likely conflicting communication requirements (e.g., CPU memory access latency or GPU network throughput) that arise from such heterogeneity. Unfortunately, it is difficult to quickly explore the hardware design space and choose appropriate tradeoffs between these heterogeneous requirements. To address these challenges, we propose the design of a 3D Network-on-Chip (NoC) for heterogeneous manycore platforms that considers the appropriate design objectives for a 3D heterogeneous system and explores various tradeoffs using an efficient machine learning (ML)-based multi-objective optimization (MOO) technique. The proposed design space exploration considers the various requirements of its heterogeneous components and generates a set of 3D NoC architectures that efficiently trades off these design objectives. Our findings show that by jointly considering these requirements (latency, throughput, temperature, and energy), we can achieve 9.6 percent better Energy-Delay Product on average at nearly iso-temperature conditions when compared to a thermally-optimized design for 3D heterogeneous NoCs. More importantly, our results suggest that our 3D NoCs optimized for a few applications can be generalized for unknown applications as well. Our results show that these generalized 3D NoCs only incur a 1.8 percent (36-tile system) and 1.1 percent (64-tile system) average performance loss compared to application-specific NoCs.","1557-9956","","10.1109/TC.2018.2889053","National Science Foundation(grant numbers:CNS-1564014,CNS-1564022,CCF 1514269); Army Research Office(grant numbers:W911NF-17-1-0485); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8585068","Heterogeneous architectures;manycore systems;multi-objective optimization;network-on-chip","Three-dimensional displays;Graphics processing units;Traffic control;Optimization;Throughput;Data transfer;Integrated circuits","","47","","44","IEEE","21 Dec 2018","","","IEEE","IEEE Journals"
"Hash Bit Selection With Reinforcement Learning for Image Retrieval","X. Yao; M. Wang; W. Zhou; H. Li","Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, China; Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, Hefei, China; Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, China; Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, China",IEEE Transactions on Multimedia,"3 Nov 2023","2023","25","","6678","6687","In recent years, binary hashing methods have been widely used in large-scale multimedia retrieval because of the low computational complexity and memory cost. Generally, better retrieval accuracy can be achieved with a longer hash code, which, however, may suffer redundancy. In this paper, we propose a novel hash bit selection method, called Hash Bit Selection with Reinforcement Learning (HBS-RL), which aims to adaptively select the most informative bits from the database binary codes. In our approach, the hash bit selection problem is firstly modeled as a Markov Decision Process (MDP), which is solved with reinforcement learning. HBS-RL learns a policy for bit selection, which effectively identifies the most informative bits by directly maximizing mean Average Precision (mAP) during training. Specially, given a generated bit pool, our HBS-RL can sequentially select bits with different code lengths with a very lightweight fully-connected policy network. The proposed method is evaluated on the MNIST, CIFAR-10, ImageNet and NUS-WIDE datasets, and the results show that it significantly improves the retrieval performance of the existing unsupervised and deep supervised hashing methods. It also outperforms the state-of-the-art bit selection methods. For convenience of repeating our results, we release our source code at: https://github.com/xyez/HBS-RL.","1941-0077","","10.1109/TMM.2022.3213476","National Natural Science Foundation of China(grant numbers:62102128,61836011,62021001); GPU cluster built; MCC Laboratory of Information Science and Technology Institution; University of Science and Technology of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9915428","Hashing;bit selection;reinforcement learning;image retrieval","Reinforcement learning;Binary codes;Databases;Hash functions;Redundancy;Quantization (signal);Optimization","","7","","51","IEEE","10 Oct 2022","","","IEEE","IEEE Journals"
"Balancing Awareness Fast Charging Control for Lithium-Ion Battery Pack Using Deep Reinforcement Learning","Y. Yang; J. He; C. Chen; J. Wei","School of Management and Engineering, Nanjing University, Nanjing, China; School of Management and Engineering, Nanjing University, Nanjing, China; School of Management and Engineering, Nanjing University, Nanjing, China; School of Management and Engineering, Nanjing University, Nanjing, China",IEEE Transactions on Industrial Electronics,"31 Oct 2023","2024","71","4","3718","3727","Minimizing charging time without damaging the batteries is significantly crucial for the large-scale penetration of electric vehicles. However, charging inconsistency caused by inevitable manufacture and usage inconsistencies can lead to lower efficiency, capacity, and shorter durability due to the “cask effect.” This goal can be achieved by solving a series of constrained optimization problems with the model-based framework. Nevertheless, the high computational complexity, identifiability, and observability issues still limit their fidelity and robustness. To overcome these limitations and provide end-to-end learning strategies, this article proposes a balancing-aware fast-charging control framework based on deep reinforcement learning. In particular, a cell-to-pack equalization topology is first introduced to dispatch energy among in-pack cells. Then, the balancing awareness fast charging problem is formulated as a multiobjective optimization problem by considering charging time, consistency, and over-voltage safety constraints. Further, a deep reinforcement learning framework using a deep Q-network is established to find the optimal policy. By using the generalization of a neural network, the learned policies can be transferred to real-time charging and balancing control, thus improving the applicability of the proposed strategy. Finally, numerous comparative simulations and experimental results illustrate its effectiveness and superiority in terms of charging rapidity and balancing.","1557-9948","","10.1109/TIE.2023.3274853","National Natural Science Foundation of China(grant numbers:62203209); Fundamental Research Funds for the Central Universities(grant numbers:0221-14380010); Natural Science Foundation of Jiangsu Province(grant numbers:BK20200333); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10124815","Cell balancing;fast charging;lithium-ion battery pack;reinforcement learning","Batteries;Computational modeling;Optimization;Integrated circuit modeling;Safety;Electric vehicles;Behavioral sciences","","7","","24","IEEE","15 May 2023","","","IEEE","IEEE Journals"
"Data Analytics for Price Forecasting in Smart Grids: A Survey","S. Mujeeb; N. Javaid; S. Javaid","Computer Science Department, COMSATS University Islamabad, Islamabad, Pakistan; Computer Science Department, COMSATS University Islamabad, Islamabad, Pakistan; Computer Science Department, COMSATS University Islamabad, Islamabad, Pakistan",2018 IEEE 21st International Multi-Topic Conference (INMIC),"30 Dec 2018","2018","","","1","10","Electricity Price Forecasting (EPF) plays a significant role in competitive electricity markets. Market participants rely on price forecast for generation, assets scheduling and effective bidding plan formulation. The uncertainty and volatility of energy market makes price forecasting a very challenging task. This paper gives a survey on methods and techniques used in literature for price forecasting in last four years. This work provides general background of price forecasting and brief discussion of the models and approaches implemented in the area of price forecasting. A comparison of price forecasting techniques with respect to different categories is presented that draws out the important aspects of the implemented methodologies.","","978-1-5386-7536-6","10.1109/INMIC.2018.8595571","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595571","Data Analytics;Big Data;Price Forecasting;Artificial intelligent Forecasters;Deep Learning","Forecasting;Predictive models;Support vector machines;Smart grids;Time series analysis;Neural networks","","2","","30","IEEE","30 Dec 2018","","","IEEE","IEEE Conferences"
"Binary Classification of Criminal Tools from the Images of the Case Using CNN","M. Kaya; B. A. Karakuş; S. Karakuş","Digital Forensic Engineering Dept., Firat University, Elazig, TURKEY; Computer Engineering Dept., Firat University, Elazig, TURKEY; Digital Forensic Engineering Dept., Firat University, Elazig, TURKEY",2018 International Conference on Artificial Intelligence and Data Processing (IDAP),"24 Jan 2019","2018","","","1","6","Recent advances in object detection and classification has accelerated by virtue of Convolutional Neural Networks (CNN) and large-scale image datasets like ImageNet. In this study, we aim to classify objects such as knives and weapons on the image data contained in electronic evidence, which may be criminal objects in judicial cases. To this end, we make this process autonomous by contributing to the examination of electronic evidence via artificial intelligence. For training of the CNN model presented in this paper, we use 10000 training data handled from ImageNet. The model with best parameter settings has been achieved 94.37% classification accuracy on 2500 test samples. We also investigate the effect of different parameters used in the network to increase the classification accuracy and try to achieve a set of optimal hyper parameters by tuning best possible parameters. As a result of the work, we present a detailed comparative study on binary object classification.","","978-1-5386-6878-8","10.1109/IDAP.2018.8620886","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8620886","CNN;Digital Evidence;Image Classification;Convolutional Neural Network","Forensics;Feature extraction;Convolutional neural networks;Software;Convolution;Artificial intelligence","","1","","21","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"Research and Application of Image Captioning Based on the CNN-LSTM Method","J. Liu; M. Gao","College of Science, China University of Petroleum, Shandong, Qingdao, China; SDU-ANU Joint Science College, Shandong University, Shandong, Jinan, China","2023 IEEE International Conference on Electrical, Automation and Computer Engineering (ICEACE)","28 Feb 2024","2023","","","1614","1620","Image captioning refers to the generation of concise descriptions for input images by computers. Serving as a bridge between computer vision and natural language processing, image captioning finds extensive applications in fields like accessibility assistance, intelligent search, and autonomous driving. In this paper, this research propose a CNN-LSTM encoder-decoder architecture. Building upon the pre-trained ResNet-101, this research modify the network structure to serve as an encoder for image feature extraction. Subsequently, this research introduce an LSTM decoder with a Soft-Attention module to generate image descriptions. Lastly, this research optimize hyperparameters using the Optuna framework. Our approach demonstrates satisfactory results on the Flicker30k dataset, achieving a Bleu-4 score of 0.187, cross-entropy loss of 6.15, and a top-5 accuracy of 72%. The experimental outcomes compellingly establish the effectiveness of this algorithm.","","979-8-3503-0961-4","10.1109/ICEACE60673.2023.10442537","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10442537","CNN-LSTM;Soft-Attention;image captioning;deep learning;hyperparameter optimization","Computers;Computer vision;Computer architecture;Feature extraction;Natural language processing;Decoding;Optimization","","","","10","IEEE","28 Feb 2024","","","IEEE","IEEE Conferences"
"Deep learning-bat high-dimensional missing data estimator","C. Leke; A. R. Ndjiongue; B. Twala; T. Marwala","School of Electrical and Electronic Engineering, University of Johannesburg; School of Electrical and Electronic Engineering, University of Johannesburg; School of Electrical and Electronic Engineering, University of Johannesburg; School of Electrical and Electronic Engineering, University of Johannesburg","2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","30 Nov 2017","2017","","","483","488","In recent years, several new methods for missing data estimation have been developed. Real world datasets possess the properties of big data being volume, velocity and variety. With an increase in volume which includes sample size and dimensionality, existing imputation methods have become less effective and accurate. Much attention has been given to narrow Artificial Intelligence frameworks courtesy of their efficiency in low dimensional settings. However, with an increase in dimensionality, these methods yield unrepresentative imputations with an impact on decision making processes. Therefore in this paper, we present a new framework for missing data imputation in high dimensional datasets. A Deep Learning technique is used in conjunction with a swarm intelligence algorithm. The performance of the proposed technique was experimentally tested and compared against other existing methods on an off-line dataset. The results obtained have shown promising potential with slightly longer execution times, which are a worthy tradeoff when accuracy is of importance.","","978-1-5386-1645-1","10.1109/SMC.2017.8122652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8122652","","Mathematical model;Estimation;Training;Optimization;Algorithm design and analysis;Image reconstruction","","4","","22","IEEE","30 Nov 2017","","","IEEE","IEEE Conferences"
"Attention-LSTM-Based Prediction Model of PM2.5 Hourly Concentration","L. Yang; D. Zhang; J. Li; L. Kang; S. Zhao","Shenyang Institute of Computing Technology, Chinese Academy of Sciences, Shenyang, China; Shenyang Institute of Computing Technology, Chinese Academy of Sciences, Shenyang, China; Liaoning Ecological and Environmental Monitoring Center, Shenyang, China; Liaoning Ecological and Environmental Monitoring Center, Shenyang, China; Shenyang No.22 Middle School, Shenyang, China",2023 12th International Conference of Information and Communication Technology (ICTech),"29 Sep 2023","2023","","","137","141","As an important indicator to measure the concentration of air pollutants, the monitoring and prediction of PM2.5 concentration can effectively protect the atmospheric environment and further reduce the harm caused by air pollution. With the large-scale establishment of automatic air quality monitoring stations, the air quality prediction model built by traditional machine learning can no longer meet the current needs. In this paper, an attention-LSTM model based on Long Short Term Memory network (LSTM) and multi-head attention mechanism is proposed to predict PM2.5 hourly concentration, and the data from a monitoring station in Shenyang are trained and tested. This model takes into account the influence of PM2.5 concentration on other air quality data, uses the two-layer LSTM network to capture the complex time series correlation features between air quality data, and uses the multi-head attention mechanism to extract the time series correlation features of different subspaces, which can obtain more complete and effective characteristic information. Experiments prove that compared with mainstream air quality prediction methods such as ARIMA, AutoEncoder and LSTM, the Attention-LSTM model proposed in this paper effectively improves the prediction accuracy and can accurately predict PM2.5 concentration.","","979-8-3503-3291-9","10.1109/ICTech58362.2023.00037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10261093","PM2.5;LSTM;Attentional mechanis;Prediction method;Time series data","Correlation;Atmospheric measurements;Atmospheric modeling;Time series analysis;Predictive models;Feature extraction;Air pollution","","","","17","IEEE","29 Sep 2023","","","IEEE","IEEE Conferences"
"Economic dispatch of power system based on neural network algorithm","C. Zhu; X. Wan; W. Gu; F. Li; J. Liu; K. Zhang; K. Yuan","Key Laboratory of Measurement and Control of CSE, Southeast University, Nanjing, China; National Electric Power Dispatching and Control Center of State Grid Corporation of China, Beijing, China; State Grid Zhejiang Electric Power Company, Hangzhou, China; State Grid Zhejiang Electric Power Co., Ltd. Jinhua Power Supply Company, Jinhua, China; China Electric Power Research Institute (Nanjing), Nanjing, China; Key Laboratory of Measurement and Control of CSE, Southeast University, Nanjing, China; Key Laboratory of Measurement and Control of CSE, Southeast University, Nanjing, China","2023 IEEE 3rd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)","6 Jul 2023","2023","3","","1343","1349","In order to improve the absorption of new energy and quickly solve the economic dispatching problem of the power system, this paper uses neural network algorithm to build a neural network model framework to solve the unit power generation plan, cluster the load forecasting data and photovoltaic power generation data, use the improved six-node system to construct a data set, train the neural network algorithm, and finally verify the accuracy of the algorithm, providing a basis for the framework requirements in the actual project.","","978-1-6654-9079-5","10.1109/ICIBA56860.2023.10165485","State Grid Corporation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10165485","Optimal dispatch;deep learning;neural networks;long short term memory","Economics;Photovoltaic systems;Load forecasting;Neural networks;Clustering algorithms;Predictive models;Dispatching","","","","14","IEEE","6 Jul 2023","","","IEEE","IEEE Conferences"
"Unsupervised Land Cover Classification of Hybrid and Dual-Polarized Images Using Deep Convolutional Neural Network","A. Chatterjee; J. Saha; J. Mukherjee; S. Aikat; A. Misra","Department of Computer Science and Engineering, IIT Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, IIT Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, IIT Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, IIT Kharagpur, Kharagpur, India; Space Applications Centre, Indian Space Research Organisation, Ahmedabad, India",IEEE Geoscience and Remote Sensing Letters,"21 May 2021","2021","18","6","969","973","Enormous volumes of data made available by the high-resolution satellite imagery enable us to use a deep framework in the field of remote sensing for image classification. Recently, deep learning has been an area of interest for the researchers in the computer vision domain due to its high efficiency toward large-scale, high-dimensional data. In this letter, we propose an unsupervised learning algorithm to cluster hybrid polarimetric SAR images, and dual-polarized SAR images using the deep framework. We use feature extraction layers of the VGG16 model with batch normalization, which is trained with small patches derived from the hybrid polarimetric SAR images. It uses an entropy-based loss function and an adaptive learning rate optimization algorithm, Adam, for training. Broadly, the patches are segmented into three classes, namely, surface, volume, and double-bounce, which are defined with reference to the SAR scattering characteristics. Furthermore, we classify volume into dense forest region and agricultural crop fields. We also observe mixed classes between volume and double-bounce, mainly covering the settlements surrounded by areas covered by tall trees. Furthermore, we use transfer learning for generating the labels for dual-polarized images by using the learned weights of a hybrid polarized image model. Such a technique renders an average accuracy of 89.70% and 86.08% for hybrid polarized SAR images and dual-polarized SAR images, respectively. Hence, this method explores the spatial characteristics of remotely sensed images to distinguish urban settlements, water bodies, agricultural, and forest areas from the underlying scene in an unsupervised fashion.","1558-0571","","10.1109/LGRS.2020.2993095","Indian Space Research Organisation (ISRO) through the project Polarimetric Analysis of SAR Images for Segmentation, Classification, and Recognition of Objects; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9094063","Deep learning;dual-polarized SAR images;hybrid polarimetric SAR images;unsupervised classification","Synthetic aperture radar;Feature extraction;Data models;Clustering algorithms;Forestry;Scattering;Entropy","","16","","13","IEEE","15 May 2020","","","IEEE","IEEE Journals"
"Reliability Analysis of Power Distribution Network Based on PSO-DBN","H. Shan; Y. Sun; W. Zhang; A. Kudreyko; L. Ren","School of Electrical and Electronic Engineering, Shanghai University of Engineering Science, Shanghai, China; School of Electrical and Electronic Engineering, Shanghai University of Engineering Science, Shanghai, China; School of Electrical and Electronic Engineering, Shanghai University of Engineering Science, Shanghai, China; Department of Medical Physics and Computer Science, Bashkir State Medical University, Ufa, Russia; School of Electrical and Electronic Engineering, Shanghai University of Engineering Science, Shanghai, China",IEEE Access,"23 Dec 2020","2020","8","","224884","224894","The main problem dealt with in this paper is to find a method to improve the performance of the reliability analysis of power distribution networks. With the help of deep learning, which has the characteristics of large-scale parallel processing and self-learning, a deep belief network (DBN) simulation model for power distribution network reliability analysis is established. After training RBM layer by layer and extracting feature information from complex data, DBN model parameters are adaptively adjusted by particle swarm optimization (PSO) algorithm. The results of power distribution network reliability analysis based on PSO-DBN model is compared with those of Monte Carlo model. In order to evaluate the performance of the proposed model, the coefficient R2, the mean absolute error and the root mean square error are used to evaluate the model. The results show that the reliability analysis model based on PSO-DBN is more accurate, and the reliability analysis efficiency of the trained PSO-DBN model is higher, which to some extent proves the superiority of applying deep neural network to the reliability analysis of distribution network.","2169-3536","","10.1109/ACCESS.2020.3007776","13th Five-Year Plan Project of National Education Science(grant numbers:DGA160245); National Nature Science Foundation of China(grant numbers:U1811462,71971031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9134746","DBN;power distribution network;PSO-DBN;RBM","Reliability;Power system reliability;Training;Analytical models;Computer network reliability;Classification algorithms","","11","","44","CCBY","7 Jul 2020","","","IEEE","IEEE Journals"
"A Constraint Satisfaction Service Composition Method Supporting One to Many Task Pattern","W. Chu; Y. Wang; T. Mo; W. Li","School of Software and Microelectronics, Peking University, Beijing, China; School of Software and Microelectronics, Peking University, Beijing, China; School of Software and Microelectronics, Peking University, Beijing, China; School of Software and Microelectronics, Peking University, Beijing, China",2021 IEEE International Conference on Services Computing (SCC),"15 Nov 2021","2021","","","382","387","The current service composition problem is usually aimed at the constrained service composition problem of Web services. However, this kind of scheme is not suitable for the situation that one service corresponds to multiple tasks. In order to solve the problem of service composition meeting business constraints and users’ needs in a specific field, this paper uses Markov decision process to model the problem, proposes a method based on deep Q-learning to solve the problem, and uses random sampling method for the inference. This method calculates the candidate services that can be used for composition, and takes the maximum cumulative rewards represented by the degree of constraint satisfaction as the optimization objective, and globally optimizes the results to meet the needs of users to the greatest extent. The experimental results show that: in this problem, compared with the existing methods, this method has higher combination efficiency.","2474-2473","978-1-6654-1683-2","10.1109/SCC53864.2021.00054","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9592518","DQN Algorithm;Service Composition;Constraint Satisfaction;MDP;Deep Exploration","Web services;Conferences;Computational modeling;Service computing;Reinforcement learning;Markov processes;Sampling methods","","2","","25","IEEE","15 Nov 2021","","","IEEE","IEEE Conferences"
"A Comprehensive Survey on Training Acceleration for Large Machine Learning Models in IoT","H. Wang; Z. Qu; Q. Zhou; H. Zhang; B. Luo; W. Xu; S. Guo; R. Li","School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Key Laboratory of Water Resources Big Data Technology of Ministry of Water Resources and the School of Computer and Information, Hohai University, Nanjing, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; School of Physics, University of Electronic Science and Technology of China, Chengdu, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China",IEEE Internet of Things Journal,"6 Jan 2022","2022","9","2","939","963","The ever-growing artificial intelligence (AI) applications have greatly reshaped our world in many areas, e.g., smart home, computer vision, natural language processing, etc. Behind these applications are usually machine learning (ML) models with extremely large size, which require huge data sets for accurate training to mine the value contained in the big data. Large ML models, however, can consume tremendous computing resources to achieve decent performance and thus, it is difficult to train them in resource-constrained Internet of Things (IoT) environments, which would prevent further development and application of AI techniques in the future. To deal with such challenges, there are many efforts on accelerating the training process for large ML models in IoT. In this article, we provide a comprehensive review on the recent advances toward reducing the computing cost during the training stage while maintaining comparable model accuracy. Specifically, the optimization algorithms that aim to improve the convergence rate are emphasized over various distributed learning architectures that exploit ubiquitous computing resources. Then, the article elaborates the computation hardware acceleration and communication optimization for collaborative training among multiple learning entities. Finally, the remaining challenges, future opportunities, and possible directions are discussed.","2327-4662","","10.1109/JIOT.2021.3111624","National Key Research and Development Program of China(grant numbers:2016YFB0800402); Hong Kong RGC Research Impact Fund (RIF)(grant numbers:R5060-19); General Research Fund (GRF)(grant numbers:152221/19E,15220320/20E); Collaborative Research Fund (CRF)(grant numbers:C5026-18G); National Natural Science Foundation of China(grant numbers:61872310,U1836204,U1936108); Major Projects of the National Social Science Foundation(grant numbers:16ZDA092); Shenzhen Science and Technology Innovation Commission(grant numbers:R2020A045); Fundamental Research Funds for the Central Universities(grant numbers:200202176,210202079); China Postdoctoral Science Foundation(grant numbers:2019M661709); Research Grants Council of the Hong Kong Special Administrative Region, China(grant numbers:PolyU15222621); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9534784","Distributed machine learning (ML);hardware-aided acceleration;large model training;training acceleration","Training;Computational modeling;Data models;Computer architecture;Optimization;Internet of Things;Task analysis","","19","","256","IEEE","10 Sep 2021","","","IEEE","IEEE Journals"
"Cooperative Control for Multi-Intersection Traffic Signal Based on Deep Reinforcement Learning and Imitation Learning","Y. Huo; Q. Tao; J. Hu","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China",IEEE Access,"9 Nov 2020","2020","8","","199573","199585","Traffic signal control has long been considered as a critical topic in intelligent transportation systems. Most existing related methods either suffer from inefficient training or mainly focus on isolated intersections. This article aims at the cooperative control for multi-intersection traffic signal, in which a novel end-to-end learning model is established and an efficient training method is proposed analogously, which is capable of adapting to large-scale scenarios. In the proposed method, the input traffic status in multi-intersection are expressed by a tensor without information loss, which significantly reduces model complexity than using a huge matrix, since additional convolutional layers can be required to extract features from a huge matrix. For the output, a multidimensional boolean vector is employed to simplify the control policy with abiding the practical phase changing rules, and then a multi-task learning structure is used to get the cooperative policy. Instead of only using the reinforcement learning to train the model, we employ imitation learning to integrate a rule based model to do the pre-training, which greatly accelerates the convergence. Afterwards, the reinforcement learning method is adopted to continue the fine training, where proximal policy optimization algorithm is incorporated to solve the policy collapse problem in multi-dimensional output situation. Numerical experiments demonstrate the distinctive advantages of the proposed method with comparison to the efficiency and accuracy of the related state-of-the-art methods.","2169-3536","","10.1109/ACCESS.2020.3034419","National Key R&D Program in China(grant numbers:2019YFF0303102); National Natural Science Foundation of China(grant numbers:61673232); Tsinghua University Initiative Scientific Research Program(grant numbers:2018Z05JDX005,20183080016)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9241814","Deep reinforcement learning;imitation learning;multi-intersection;proximal policy optimization;tensor","Training;Numerical models;Reinforcement learning;Feature extraction;Convergence;Optimization;Indexes","","16","","42","CCBYNCND","28 Oct 2020","","","IEEE","IEEE Journals"
"Locational Marginal Electricity Price Forecasting-Based Self-Attention Mechanism and Simulated Annealing Optimizer using Big Data","M. Massaoudi; H. Abu-Rub; S. S. Refaat; A. Ali Al-Kuwari; T. Huang","Department of Electrical and Computer Engineering, Texas A&M University at Qatar, Doha, Qatar; Department of Electrical and Computer Engineering, Texas A&M University at Qatar, Doha, Qatar; Department of Electrical and Computer Engineering, Texas A&M University at Qatar, Doha, Qatar; Qatar General Electricity and Water Corporation (KAHRAMAA), Qatar; Science Department, Texas A&M University at Qatar, Doha, Qatar",2021 10th International Conference on Renewable Energy Research and Application (ICRERA),"16 Nov 2021","2021","","","391","396","Effective short-term Locational Marginal Price Forecasting (LMPF) is difficult in view of the high sensibility of the electricity price in deregulated markets. This paper proposes an accurate forecasting algorithm for LMPF using the latest breakthroughs in deep learning. Specifically, the proposed strategy is composed of a Hybrid Feature Selector (HFS), hyperparameter tuning using Simulated Annealing (SA)-based multi-objective optimization algorithm, and self-Attention-based Long Short-Term Memory (ALSTM). The proposed HFS includes Extreme Gradient Boosting, Elastic Net, and random forest models to rank the features based on their relevance. The experimental results are compared with multiple benchmark algorithms to demonstrate the robustness and efficiency of the proposed framework. The main contributions of this paper include 1) An efficient model perfectly tailored for LMPF is introduced; 2) The effectiveness superiority of the proposed SA-ALSTM is verified on publicly available electricity market data. Extensive experimental results validate the competitive performance of the proposed SA-ALSTM in terms of score measures.","2572-6013","978-1-6654-4524-5","10.1109/ICRERA52334.2021.9598604","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9598604","Attention mechanism;Deep Learning;Electricity price forecasting;Long Short Term Memory;Simulated Annealing;Big Data","Deep learning;Sensitivity;Simulated annealing;Hafnium;Benchmark testing;Electricity supply industry;Robustness","","1","","22","IEEE","16 Nov 2021","","","IEEE","IEEE Conferences"
"Online Sequential Extreme learning machine-based hybrid model for forecasting of COVID-19 cases in India","R. P. Shetty; P. Srinivasa Pai","Department of Mechanical Engineering, NMAM Institute of Technology, Udupi, Karnataka, India; Department of Mechanical Engineering, NMAM Institute of Technology, Udupi, Karnataka, India","2022 International Conference on Distributed Computing, VLSI, Electrical Circuits and Robotics ( DISCOVER)","12 Dec 2022","2022","","","271","276","COVID-19 has posed high stress on government and people with its disruptive effects on every sector of the nation. Accurate and reliable forecasting models are of great need to handle this unprecedented situation. A hybrid model, which is a combination of, cuckoo search optimization algorithm, variational mode decomposition and online sequential extreme learning machine has been proposed in this work for multistep forecasting of COVID-19 cases. The model showed reasonable accuracy of 1.363%, 1.596% and 1.933% for one, three and five days ahead forecasting. The model gave superior results when compared with partial autocorrelation function (PACF) for selection of number of input parameters. The robustness of the proposed model has been evident in comparison with other similar state of the art techniques discussed in the literature.","","978-1-6654-8716-0","10.1109/DISCOVER55800.2022.9974919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9974919","COVID-l9 forecasting;Cuckoo search algorithm;Optimized variational mode decomposition;Online sequential extreme learning machine","COVID-19;Extreme learning machines;Predictive models;Very large scale integration;Feature extraction;Data models;Robustness","","","","25","IEEE","12 Dec 2022","","","IEEE","IEEE Conferences"
"1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB’s Convergence Speed","C. Li; A. A. Awan; H. Tang; S. Rajbhandari; Y. He",Microsoft; Microsoft; University of Rochester; Microsoft; Microsoft,"2022 IEEE 29th International Conference on High Performance Computing, Data, and Analytics (HiPC)","26 Apr 2023","2022","","","272","281","To train large machine learning models (like BERT and GPT-3) on hundreds of GPUs, communication has become a significant bottleneck, especially on commodity systems with limited-bandwidth TCP networks. On one side, large batch-size optimization such as the LAMB algorithm was proposed to reduce the frequency of communication. On the other side, communication compression algorithms such as 1-bit Adam help to reduce the volume of each communication. However, we find that simply using one of the techniques is not sufficient to solve the communication challenge, especially under low network bandwidth. Motivated by this we aim to combine the power of large-batch optimization and communication compression but we find that existing compression strategies cannot be directly applied to LAMB due to its unique adaptive layerwise learning rates. To this end, we design a new communication-efficient optimization algorithm, 1-bit LAMB, which introduces a novel way to support adaptive layerwise learning rates under compression. In addition to the algorithm and corresponding theoretical analysis, we propose three novel system implementations in order to achieve actual wall clock speedup: a momentum fusion mechanism to reduce the number of communications, a momentum scaling technique to reduce compression error, and a NCCL-based compressed communication backend to improve both usability and performance. For the BERT-Large pre-training task with batch sizes from 8K to 64K, our evaluations on up to 256 GPUs demonstrate that our optimized implementation of 1-bit LAMB is able to achieve up to 4.6x communication volume reduction, up to 2.8x end-to-end time-wise speedup, and the same sample-wise convergence speed (and same fine-tuning task accuracy) compared to uncompressed LAMB. Furthermore, 1-bit LAMB achieves the same accuracy as LAMB on computer vision tasks like ImageNet and CIFAR100.","2640-0316","978-1-6654-9423-6","10.1109/HiPC56025.2022.00044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10106313","optimization;communication compression;natural language processing;language model pre training;distributed deep learning","Training;Solid modeling;Machine learning algorithms;Bit error rate;Machine learning;Task analysis;Usability","","5","","21","IEEE","26 Apr 2023","","","IEEE","IEEE Conferences"
"Semi-Supervised Learning Based on Generative Adversarial Network and Its Applied to Lithology Recognition","G. Li; Y. Qiao; Y. Zheng; Y. Li; W. Wu","Laboratory of Oil and Gas Big Data, China University of Petroleum Beijing at Kelamayi, Xinjiang, China; Laboratory of Oil and Gas Big Data, China University of Petroleum Beijing at Kelamayi, Xinjiang, China; Laboratory of Oil and Gas Big Data, China University of Petroleum Beijing at Kelamayi, Xinjiang, China; Laboratory of Oil and Gas Big Data, China University of Petroleum Beijing at Kelamayi, Xinjiang, China; Laboratory of Oil and Gas Big Data, China University of Petroleum Beijing at Kelamayi, Xinjiang, China",IEEE Access,"4 Jun 2019","2019","7","","67428","67437","Lithology recognition is an essential part of reservoir parameter prediction. Compared to conventional algorithms, deep learning that needs a large amount of training data as support can extract features automatically. In the process of real data acquisition, the labeled data account for only a small portion due to high drilling cost, and it is difficult to achieve the data size required for deep learning training, resulting in a significant variance of the recognition model. In this paper, for this shortage, a semi-supervised algorithm based on generative adversarial network (GAN) with Gini-regularization is proposed, called SGAN_G, which takes borehole-side data as labeled data and seismic data as unlabeled data. First, the SGAN_G is trained by Adam (a method for stochastic optimization) algorithm and utilizes a discriminator to lithology recognition. And, we add the entropy regularization to the initial loss function which enhances the convergence speed and accuracy of the model. Eventually, we propose a novel sampling approach which employs multiple sampling points of seismic data as inputs to use the stratum information implicitly. Through the experimental comparison with a variety of supervised approaches, we can see that the SGAN_G can achieve higher prediction accuracy by using unlabeled data effectively.","2169-3536","","10.1109/ACCESS.2019.2918366","National Natural Science Foundation of China(grant numbers:60473125,61701213); Innovation Foundation of CNPC(grant numbers:05E7013); National Key Project Foundation of Science(grant numbers:G5800-08-ZS-WX); Science Foundation of China University of Petroleum-Beijing At Karamay(grant numbers:RCYJ2016B-03-001); Cooperative Education Project of Nation Education Ministry(grant numbers:201702098015); Natural Science Foundation of Fujian Province(grant numbers:2018J01545,2018J01546,2019J01748); Research Funds for the Educational Department of Fujian Province(grant numbers:JA15300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8720241","Entropy regularization;generative adversarial network;lithology recognition;semi-supervised learning","Entropy;Data models;Training;Generative adversarial networks;Semisupervised learning;Generators;Petroleum","","24","","30","OAPA","22 May 2019","","","IEEE","IEEE Journals"
"Randomized Iterative Methods for Low-Complexity Large-Scale MIMO Detection","Z. Wang; R. M. Gower; Y. Xia; L. He; Y. Huang","School of Information Science and Engineering, Southeast University, Nanjing, China; Center for Computational Mathematics, Flatiron Institute and Simons Foundation, New York, NY, USA; School of Information Science and Engineering, Southeast University, Nanjing, China; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Information Science and Engineering, Southeast University, Nanjing, China",IEEE Transactions on Signal Processing,"21 Jun 2022","2022","70","","2934","2949","In this paper, we introduce a randomized iterative method for signal detection in uplink large-scale multiple-input multiple-output (MIMO) systems, which not only achieves a low computational complexity but also enjoys a global and exponentially fast convergence. First of all, by adopting the random sampling into the iterations, the randomized iterative detection algorithm (RIDA) is proposed for large-scale MIMO systems. We show that RIDA converges exponentially fast in terms of mean squared error (MSE). Furthermore, this global convergence always holds, and does not depend on the standard requirements such as $N\gg K$, where $N$ and $K$ denote the numbers of antennas at the sides of base station and users. This broadly extends the applications of low-complexity detection in uplink large-scale MIMO systems. Then, based on a new conditional sampling, optimization and enhancements are given to further improve both the convergence and efficiency of RIDA, resulting in the modified randomized iterative detection algorithm (MRIDA). Meanwhile, with respect to MRIDA, further complexity reduction by exploiting the matrix structure is given while its implementation by deep neural networks (DNN) is also presented for a better detection performance.","1941-0476","","10.1109/TSP.2022.3180552","National Natural Science Foundation of China(grant numbers:61801216,61771124,61720106003); Natural Science Foundation of Jiangsu Province(grant numbers:BK20180420); State Key Laboratory of Integrated Services Networks Xidian University(grant numbers:ISN21-31); Zhi Shan Young Scholar Program of Southeast University; Fundamental Research Funds for the Central Universities(grant numbers:2242022k30002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9790338","Massive MIMO detection;low complexity;iterative methods;linear system solver;deep learning","Convergence;MIMO communication;Jacobian matrices;Uplink;Complexity theory;Antennas;Signal detection","","9","","57","IEEE","8 Jun 2022","","","IEEE","IEEE Journals"
"Convolutional Neural Networks on Apache Storm","W. Zhang; Y. Lu; Y. Li; H. Qiao","The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Information Engineering, Nanchang University, Nanchang, China; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China",2019 Chinese Automation Congress (CAC),"13 Feb 2020","2019","","","2399","2404","the performance of deep learning largely depends on the size of data. One data source is real-time streaming data, produced from mobile devices, sensors or social media, etc. Streaming data is high-speed and large-scale, which needs real-time processing. However, current mainstream frameworks are mainly designed for off-line data. To suit this, we first propose a deep learning framework based on Apache Storm, which is a distributed stream processing frame, fast and fault-tolerant. Our framework implements the distributed training of CNNs. which is different from MMLSpark or TensorFlowOnSpark that is a pure Java implementation. The design of message passing and synchronization is also suitable to other MapReduce-family distributed computing platforms. To validate our work, MNIST and Cifar -10 datasets are used for evaluation and comparison with similar architectures. The results show our framework, in resource-limited environment, realizes about 10 times speedup.","2688-0938","978-1-7281-4094-0","10.1109/CAC48633.2019.8996300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8996300","Computer Vision;Distributed Systems;Neural Networks;Speed up;Streaming Datoe","Fasteners;Storms;Topology;Parallel processing;Training;Neural networks;Machine learning","","1","","22","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"FarSick: A Persian Semantic Textual Similarity And Natural Language Inference Dataset","Z. Ghasemi; M. A. Keyvanrad","Faculty of Electrical & Computer Engineering, Malek-Ashtar University of Technology, Iran; Faculty of Electrical & Computer Engineering, Malek-Ashtar University of Technology, Iran",2021 11th International Conference on Computer Engineering and Knowledge (ICCKE),"28 Feb 2022","2021","","","194","199","Semantic textual similarity(STS) and natural language inference(NLI) are important tasks in natural language processing(NLP) such as information retrieval, text classification, subject extraction, text summarization, machine translation and plagiarism detection. Lack of appropriate datasets in the Persian language is a major obstacle to progress in this area. Therefore, in this paper, we present FarSick, a new dataset for STS and NLI tasks in the Persian language. FarSick is the first relatively large-scale STS dataset for the Persian language. It includes 9804 pairs of Persian sentences with labels for similarity and inference for each pair of sentences. This dataset is collected by translating and editing the sentences of SICK dataset. We also measured the performance of traditional, statistical and deep learning models on it, e.g. transformers, Convolution Neural Networks, Bidirectional LSTMs, weighted average of word vectors, etc. We used different pre-trained embeddings, word2vec, glove, fastText and Bert sentence transformer. We used accuracy metric to test NLI tasks and Pearson metric to test STS tasks. The dataset is available at https://github.com/ZahraGhasemi-AI/FarSick.","2643-279X","978-1-6654-0208-8","10.1109/ICCKE54056.2021.9721521","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9721521","Persian dataset;Semantic Textual Similarity;Natural Language Inference;paraphrase expressions;plagiarism detection;deep learning;Natural Language Processing","Measurement;Weight measurement;Plagiarism;Natural languages;Semantics;Text categorization;Neural networks","","2","","19","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Short-term Load Forecasting of CCHP System Based on PSO-LSTM","Y. -R. Zhu; J. -G. Wang; Y. -Q. Sun; J. -J. Wu; G. -Q. Zhao; Y. Yao; J. -L. Liu; H. -L. Chen","School of Mechatronical Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronical Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronical Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronical Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronical Engineering and Automation, Shanghai University, Shanghai, China; Department of Chemical Engineering, National Tsing-Hua University, Hsin-Chu, Taiwan; Shanghai Minghua Electric Power Science &Technology Co. Ltd, Shanghai, China; Ironmaking Plant, Baoshan Iron & Steel Co. Ltd., Shanghai, China",2023 IEEE 12th Data Driven Control and Learning Systems Conference (DDCLS),"7 Jul 2023","2023","","","639","644","With the inherent need to accelerate the high-quality development of China's economy, it is necessary to build a clean, low-carbon, safe and efficient modern energy system. The traditional energy system is centralized and large-scale, and the transmission and distribution system are complex, with low adaptability and reliability. The Combined cooling, heating and power system has been widely promoted and concerned for its advantages of improving energy efficiency, saving energy and reducing emissions. This paper takes the Combined cooling, heating and power system of Shanghai Qiantan Energy Station as the research object and establishes a load prediction model on the user side. This paper first introduces the Combined cooling, heating and power system of Shanghai Qiantan Energy Station, then explores the influencing factors of load data, builds the PSO-LSTM model and analyzes the prediction results, and finally draws a conclusion.","2767-9861","979-8-3503-2105-0","10.1109/DDCLS58216.2023.10167106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10167106","Combined cooling heating and power;short-term load forecasting;PSO-LSTM","Deep learning;Temperature distribution;Load forecasting;Neural networks;Time series analysis;Predictive models;Prediction algorithms","","1","","8","IEEE","7 Jul 2023","","","IEEE","IEEE Conferences"
"Bridging the Gap between Deep Learning and Frustrated Quantum Spin System for Extreme-Scale Simulations on New Generation of Sunway Supercomputer","M. Li; J. Chen; Q. Xiao; F. Wang; Q. Jiang; X. Zhao; R. Lin; H. An; X. Liang; L. He","School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; CAS Key Lab of Quantum Information, University of Science and Technology of China, Hefei, Anhui, China; CAS Key Lab of Quantum Information, University of Science and Technology of China, Hefei, Anhui, China",IEEE Transactions on Parallel and Distributed Systems,"26 May 2022","2022","33","11","2846","2859","Efficient numerical methods are promising tools for delivering unique insights into the fascinating properties of physics, such as the highly frustrated quantum many-body systems. However, the computational complexity of obtaining the wave functions for accurately describing the quantum states increases exponentially with respect to particle number. Here we present a novel convolutional neural network (CNN) for simulating the two-dimensional highly frustrated spin-$1/2$   1 / 2    $J_1-J_2$    J 1  -  J 2     Heisenberg model, meanwhile the simulation is performed at an extreme scale system with low cost and high scalability. By ingenious employment of transfer learning and CNN’s translational invariance, we successfully investigate the quantum system with the lattice size up to $24\times 24$   24 × 24   , within 30 million cores of the new generation of sunway supercomputer. The final achievement demonstrates the effectiveness of CNN-based representation of quantum-state and brings the state-of-the-art record up to a brand-new level from both aspects of remarkable accuracy and unprecedented scales.","1558-2183","","10.1109/TPDS.2022.3145163","National Key Research and Development Program of China(grant numbers:2016YFB1000403); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693260","Quantum system;deep learning;new generation sunway supercomputer;spin-1/2 J1 – J2 Heisenberg model","Quantum system;Lattices;Supercomputers;Stationary state;Wave functions;Monte Carlo methods;Convolutional neural networks","","10","","47","IEEE","25 Jan 2022","","","IEEE","IEEE Journals"
"Distance Metric Learning via Iterated Support Vector Machines","W. Zuo; F. Wang; D. Zhang; L. Lin; Y. Huang; D. Meng; L. Zhang","School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, Sun Yat-sen University, Guangzhou, China; Research Division, Educational Testing Service, Princeton, NJ, USA; Institute of Information and System Sciences, Faculty of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong",IEEE Transactions on Image Processing,"4 Aug 2017","2017","26","10","4937","4950","Distance metric learning aims to learn from the given training data a valid distance metric, with which the similarity between data samples can be more effectively evaluated for classification. Metric learning is often formulated as a convex or nonconvex optimization problem, while most existing methods are based on customized optimizers and become inefficient for large scale problems. In this paper, we formulate metric learning as a kernel classification problem with the positive semi-definite constraint, and solve it by iterated training of support vector machines (SVMs). The new formulation is easy to implement and efficient in training with the off-the-shelf SVM solvers. Two novel metric learning models, namely positive-semidefinite constrained metric learning (PCML) and nonnegative-coefficient constrained metric learning (NCML), are developed. Both PCML and NCML can guarantee the global optimality of their solutions. Experiments are conducted on general classification, face verification, and person re-identification to evaluate our methods. Compared with the state-of-the-art approaches, our methods can achieve comparable classification accuracy and are efficient in training.","1941-0042","","10.1109/TIP.2017.2725578","National Key R&D Program of China(grant numbers:2017YFC0113000,2016YFB1001004); NSFC(grant numbers:61671182); Guangdong Natural Science Foundation(grant numbers:2015A030313129); Hong Kong Research Grants Council General Research Fund(grant numbers:PolyU 152212/14E); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7973168","Metric learning;support vector machine;kernel method;Lagrange duality;alternating minimization","Measurement;Support vector machines;Kernel;Training;Optimization;Learning systems;Face","","30","","62","IEEE","11 Jul 2017","","","IEEE","IEEE Journals"
"Artificial Intelligence: New Frontiers in Real-Time Inverse Scattering and Electromagnetic Imaging","M. Salucci; M. Arrebola; T. Shan; M. Li","ELEDIA Research Center (ELEDIA@UniTN - University of Trento), DICAM - Department of Civil, Environmental, and Mechanical Engineering, Trento, Italy; Department of Electrical Engineering, Signal Theory and Communications Group, Universidad de Oviedo, Gijón, Spain; Department of Electronic Engineering, Beijing National Research Center for Information Science and Technology, Microwave and Antenna Institute, Tsinghua University, Beijing, China; Department of Electronic Engineering, Beijing National Research Center for Information Science and Technology, Microwave and Antenna Institute, Tsinghua University, Beijing, China",IEEE Transactions on Antennas and Propagation,"8 Sep 2022","2022","70","8","6349","6364","In recent years, artificial intelligence (AI) techniques have been developed rapidly. With the help of big data, massive parallel computing, and optimization algorithms, machine learning (ML) and (more recently) deep learning (DL) strategies have been equipped with enhanced learning and generalization capabilities. Besides becoming an essential framework in image and speech signal processing, AI has also been widely applied to solve several electromagnetic (EM) problems with unprecedented computational efficiency, including inverse scattering (IS) and EM imaging. In this article, a review of the most recent progresses in the application of ML and DL for such problems is given. We humbly hope a brief summary could help us better understand the pros and cons of this research topic and foster future research in using AI to address paramount challenges in the field of EM vision.","1558-2221","","10.1109/TAP.2022.3177556","Project “CYBER-PHYSICAL ELECTROMAGNETIC VISION: Context-Aware Electromagnetic Sensing and Smart Reaction (EMvisioning)”(grant numbers:2017HZJXSZ); Italian Ministry of Education, University, and Research within the PRIN2017 Program(grant numbers:CUP: E64I19002530001); “SMARTOUR - Piattaforma Intelligente per il Turismo”(grant numbers:SCN_00166); Italian Ministry of Education, University, and Research within the Program “Smart cities and communities and Social Innovation”(grant numbers:CUP: E44G14000040008); Ministerio de Ciencia e Innovación; Spanish Agency for Research within project ENHANCE-5G(grant numbers:PID2020-114172RBC21/AEI/10.13039/501100011033); Government of Principality of Asturias within project(grant numbers:AYUD/2021/51706); National Natural Science Foundation of China(grant numbers:61971263); National Key Research and Development Program of China(grant numbers:2018YFC0603604); Institute for Precision Medicine, Tsinghua University, Beijing, China, and THE XPLORER PRIZE; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9785455","Artificial intelligence (AI);deep learning (DL);electromagnetic (EM) imaging;inverse scattering (IS);learning by examples (LBE);machine learning (ML)","Imaging;Three-dimensional displays;Electromagnetics;Artificial intelligence;Inverse problems;Deep learning;Technological innovation","","40","","162","IEEE","30 May 2022","","","IEEE","IEEE Journals"
"A deep Spatio-temporal network for vision-based sexual harassment detection","M. S. Islam; M. M. Hasan; S. Abdullah; J. U. M. Akbar; N. H. M. Arafat; S. A. Murad","Dept. of CSE, Manarat International University, Dhaka, Bangladesh; IICT, BUET, Dhaka, Bangladesh; Dept. of CSE, Manarat International University, Dhaka, Bangladesh; Dept. of CSE, International Islamic University Chittagong, Chittagong, Bangladesh; Dept. of Computer Science and Technology, Henan Polytechnic University, 454003, Jiaozuo, Henan, P.R. China; Faculty of Computing, Universiti Malaysia Pahang, 26600, Pahang, Malaysia","2021 Emerging Technology in Computing, Communication and Electronics (ETCCE)","26 Jan 2022","2021","","","1","6","Smart surveillance systems can play a significant role in detecting sexual harassment in real-time for law enforcement which can reduce the sexual harassment activities. Real-time detecting of sexual harassment from video is a complex computer vision because of various factors such as clothing or carrying variation, illumination variation, partial occlusion, low resolution, view angle variation etc. Due to the advancement of convolutional neural networks (CNNs) and Long Short-Term Memory (LSTM), human action recognition tasks have achieved great success in recent years. But sexual harassment detection is addressed due to presences of large-scale harassment dataset. In this work, to address this problem, we build a video dataset of sexual harassment, namely Sexual harassment video (SHV) dataset which consists of harassment and non-harassment videos collected from YouTube. Besides, we build a CNN-LSTM network to detect the sexual harassment in which CNN and RNN are employed for extracting spatial features and temporal features, respectively. State-of-the-art pretrained models are also employed as a spatial feature extractor with an LSTM and three dense layer to classify harassment activities. Moreover, to find the robustness of our proposed model, we have conducted several experiments with our proposed method on two other benchmark datasets, such as Hockey Fight dataset and Movie Violence dataset and achieved state-of-the-art accuracy.","","978-1-6654-8364-3","10.1109/ETCCE54784.2021.9689891","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9689891","sexual harassment;surveillance systems;deep learning","Surveillance;Lighting;Streaming media;Feature extraction;Motion pictures;Real-time systems;Robustness","","4","","36","IEEE","26 Jan 2022","","","IEEE","IEEE Conferences"
"Prediction of Stock Price and Direction Using Neural Networks: Datasets Hybrid Modeling Approach","M. Al Aradi; N. Hewahi","Big Data Science & Analytics Master Program Collage of Science, Univestity of Bahain Sakhir, Kingdom of Bahrain; Department of Computer Science, Collage of Information Technology, Univestity of Bahain, Sakhir, Kingdom of Bahrain",2020 International Conference on Data Analytics for Business and Industry: Way Towards a Sustainable Economy (ICDABI),"20 Jan 2021","2020","","","1","6","Stock prices prediction is one of the most daunting tasks to achieve for day traders, investors, and data scientists. They are complex functions of a wide array of contributing factors that affects the movement dynamics. Politics, social outlook, the company's sales performance, and socio-economic factors all have an influence of how the stock is perceived globally and thus affecting the price movement based on the supply/demand balance. In this work a Framework is proposed to predict the price and movement direction by using multiple datasets that includes news sentiment, social sentiment, company earnings announcement, and technical indicators. A case study was applied on the Apple Inc. stock using Long short-term Memory Neural Networks (LSTM) and Deep Neural Networks (DNN). Results showed superiority of the LSTM model in both predicting stock value [ 75.4 MSE, +-2.52 PE] and the direction with [70.1% classification accuracy] over the DNN model which suffered from prediction lag and dependency on trend indicators achieving [53.1%] in predicting the direction of the stock.","","978-1-7281-9675-6","10.1109/ICDABI51230.2020.9325697","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9325697","ANN(s);Sentiment;LSTM;Deep Learning;Stock Price;Data science;AAPL;Apple stock price","Predictive models;Machine learning;Companies;Neural networks;Market research;Data models;Recurrent neural networks","","7","","21","IEEE","20 Jan 2021","","","IEEE","IEEE Conferences"
"3D Aerial Base Station Position Planning based on Deep Q-Network for Capacity Enhancement","J. Wu; P. Yu; L. Feng; F. Zhou; W. Li; X. Qiu","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China",2019 IFIP/IEEE Symposium on Integrated Network and Service Management (IM),"20 May 2019","2019","","","482","487","When the existing traditional terrestrial base station is insufficient to meet the sudden traffic demand or is not available, deploying the aerial base station(aerial-BS) is a fast and effective solution for achieving network capacity enhancement. How to plan the best 3D location of the aerial-BS according to the user's business needs and service scenarios is a key issue to be solved. At present, the conventional optimization algorithms that solve this problem have high time complexity and it is difficult to utilize experience. However, applying the deep reinforcement learning model can quickly get an optimal solution by historical experience feedback training. Therefore, it is suitable for solving the optimal 3D location planning problem of the aerial-BS. In this paper, firstly, aiming at the maximum spectral efficiency of the system, considering the effects of line-of-sight and non-line-of-sight path loss, a mathematical optimization model for the location planning of the aerial-BS is proposed. For this model, the model definition and training process of deep Q-Network are constructed, and through the large-scale pre-learning experience of different user layouts in the training process to gain experience, improve the timeliness of the training process. The simulation results show that the proposed method can achieve the spectral efficiency of more than 91% of the theoretical maximum spectral efficiency, which has lower time complexity than traditional genetic algorithms (such as hill climbing algorithm and simulated annealing algorithm).","1573-0077","978-3-903176-15-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8717805","aerial base station;deep reinforcement learning;DQN;mobility management","Three-dimensional displays;Spectral efficiency;Base stations;Mathematical model;Resource management;Planning;Optimization","","2","","11","","20 May 2019","","","IEEE","IEEE Conferences"
"Learning to Optimize Vehicle Routes Problem: A Two-Stage Hybrid Reinforcement Learning","W. Zhang; X. Wang","College of Information Science and Engineering, Henan University of Technology, Zhengzhou, China; College of Information Science and Engineering, Henan University of Technology, Zhengzhou, China","2023 International Conference on Sensing, Measurement & Data Analytics in the era of Artificial Intelligence (ICSMD)","15 Apr 2024","2023","","","1","6","The Vehicle Routing Problem (VRP) is a typical combinatorial optimization problem, aiming to determine the optimal customer visit routes while minimizing the total cost under certain constraints. However, for solving large-scale VRP instances, traditional heuristic optimization algorithms could hardly meet the demands of both solution accuracy and computational efficiency. Thus learning-based methods have recently garnered more attention. Some work attempts to solve the problems by reinforcement learning methods, which suffer from slow convergence issues during training and are still not accurate enough regarding the solutions. To that end, this paper proposes a two-stage hybrid algorithm based on imitation learning and reinforcement learning for solving the vehicle routing problem. Firstly, we introduce a novel imitation learning framework, where classical heuristic methods are treated as experts to encourage policy models to mimic and generate similar or better solutions. which accelerates the convergence more stably and accurately. Secondly, by employing deep reinforcement learning methods, we further train neural networks to improve their performance in generating superior solutions. Experimental results demonstrate that our approach exhibits significant performance improvements on multiple instances of vehicle routing problems, outperforming a wide range of baselines and getting results close to highly optimized and specialized algorithms. Additionally, our method possesses a degree of generalization capability, allowing it to adapt to vehicle routing problems of varying scales.","","979-8-3503-1801-2","10.1109/ICSMD60522.2023.10490595","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10490595","vehicle routing problem;imitation learning;deep reinforcement learning","Learning systems;Training;Data analysis;Heuristic algorithms;Vehicle routing;Neural networks;Deep reinforcement learning","","","","22","IEEE","15 Apr 2024","","","IEEE","IEEE Conferences"
"Generalized Adversarial and Hierarchical Co-occurrence Network based Synthetic Skeleton Generation and Human Identity Recognition","J. G. Zalameda; B. Kruse; A. M. Glandon; M. A. Witherow; S. Shetty; K. M. Iftekharuddin","Dept. of ECE, Old Dominion University, Norfolk, VA, USA; Dept. of Computer Science Mississippi State University, Starkville, MS, USA; Dept. of ECE, Old Dominion University, Norfolk, VA, USA; Dept. of ECE, Old Dominion University, Norfolk, VA, USA; Dept. of ECE, Old Dominion University, Norfolk, VA, USA; Dept. of ECE, Old Dominion University, Norfolk, VA, USA",2022 International Joint Conference on Neural Networks (IJCNN),"30 Sep 2022","2022","","","1","8","Human skeleton data provides a compact, low noise representation of relative joint locations that may be used in human identity and activity recognition. Hierarchical Co-occurrence Network (HCN) has been used for human activity recognition because of its ability to consider correlation between joints in convolutional operations in the network. HCN shows good identification accuracy but requires a large number of samples to train. Acquisition of this large-scale data can be time consuming and expensive, motivating synthetic skeleton data generation for data augmentation in HCN. We propose a novel method that integrates an Auxiliary Classifier Generative Adversarial Network (AC-GAN) and HCN hybrid framework for Assessment and Augmented Identity Recognition for Skeletons (AAIRS). The proposed AAIRS method performs generation and evaluation of synthetic 3-dimensional motion capture skeleton videos followed by human identity recognition. Synthetic skeleton data produced by the generator component of the AC-GAN is evaluated using an Inception Score-inspired realism metric computed from the HCN classifier outputs. We study the effect of increasing the percentage of synthetic samples in the training set on HCN performance. Before synthetic data augmentation, we achieve 74.49% HCN performance in 10-fold cross validation for 9-class human identification. With a synthetic-real mixture of 50%-50%, we achieve 78.22% mean accuracy, significantly $(\mathrm{p} < 0.05)$ outperforming the baseline HCN performance. The proposed framework demonstrates the feasibility of combining a synthetic data generation architecture with hierarchical co-occurrence feature learning for human identity recognition.","2161-4407","978-1-7281-8671-9","10.1109/IJCNN55064.2022.9892887","CERDEC(grant numbers:100659); National Science Foundation(grant numbers:1828593,1950704,1753793); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9892887","Security;Human Identity Recognition;Skeleton Data;Motion Capture Data;Generative Adversarial Network;Auxiliary Classification;Synthetic Skeleton Data;Cooccurrence","Training;Representation learning;Measurement;Neural networks;Generative adversarial networks;Skeleton;Motion capture","","1","","22","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Efficient Robust Principal Component Analysis via Block Krylov Iteration and CUR Decomposition","S. Fang; Z. Xu; S. Wu; S. Xie","School of Information Science and Engineering, Wuhan University of Science and Technology, China; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China; School of Information Science and Engineering, Wuhan University of Science and Technology, China; Signal Processing, RF & Optical Dept., Institute for Infocomm Research A*STAR, Singapore",2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"22 Aug 2023","2023","","","1348","1357","Robust principal component analysis (RPCA) is widely studied in computer vision. Recently an adaptive rank estimate based RPCA has achieved top performance in low-level vision tasks without the prior rank, but both the rank estimate and RPCA optimization algorithm involve singular value decomposition, which requires extremely huge computational resource for large-scale matrices. To address these issues, an efficient RPCA (eRPCA) algorithm is proposed based on block Krylov iteration and CUR decomposition in this paper. Specifically, the Krylov iteration method is employed to approximate the eigenvalue decomposition in the rank estimation, which requires $O(ndrq+n(rq)^{2})$ for an $(n\times d)$ input matrix, in which $q$ is a parameter with a small value, $r$ is the target rank. Based on the estimated rank, CUR decomposition is adopted to replace SVD in updating low-rank matrix component, whose complexity reduces from $O(rnd)$ to $O(r^{2}n)$ per iteration. Experimental results verify the efficiency and effectiveness of the proposed eRPCA over the state-of-the-art methods in various low-level vision applications.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203200","Optimization methods (other than deep learning)","Computer vision;Estimation;Approximation algorithms;Robustness;Pattern recognition;Matrix decomposition;Sparse matrices","","2","","40","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"UAV Trajectory Optimization for Large-Scale and Low-Power Data Collection: An Attention-Reinforced Learning Scheme","Y. Zhu; B. Yang; M. Liu; Z. Li","Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; College of Information Engineering, Northwest A&F University, Yangling, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China",IEEE Transactions on Wireless Communications,"10 Apr 2024","2024","23","4","3009","3024","Unmanned Aerial Vehicles (UAVs) exhibit great advantages in data collection from ground sensors in vast tracts of fields. Due to their limited power supply, most works assume that the UAV simply traverses each sensor’s fixed transmission range to collect data, thereby shortening the flight path. However, they neglect the quality of collected data, which may deteriorate dramatically as the transmission distance increases. In this paper, by leveraging the physical-layer protocol - LoRa, we propose a Packet Reception Ratio (PRR)-based probabilistic coverage model to evaluate the quality of data transmission, which directly determines the data acquisition efficiency. On this basis, to minimize the energy consumption of UAV and sensors while ensuring high-quality data acquisition, we formulate the UAV trajectory planning as a joint Energy Consumption and data Acquisition Efficiency (ECAE) optimization problem. To tackle the ECAE problem, we propose a Deep Reinforcement Learning (DRL)-based two-stage scheme. First, an attention-based encoder-decoder model is trained to generate an initial trajectory. Then an intuitive optimization algorithm is devised to further explore the optimal trajectory. Evaluation results show that our scheme can reduce the total energy cost of UAV and sensors by 27.1% as compared to the best baseline’s policy while maintaining a promising PRR.","1558-2248","","10.1109/TWC.2023.3304900","National Natural Science Foundation of China(grant numbers:62072436,61732017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10224843","Unmanned aerial vehicle;trajectory optimization;data collection;attention model;deep reinforcement learning;LoRa","Autonomous aerial vehicles;Sensors;Data collection;Energy consumption;Wireless communication;Optimization;Data models","","2","","49","IEEE","18 Aug 2023","","","IEEE","IEEE Journals"
"SKFAC: Training Neural Networks with Faster Kronecker-Factored Approximate Curvature","Z. Tang; F. Jiang; M. Gong; H. Li; Y. Wu; F. Yu; Z. Wang; M. Wang","School of Electronic and Engineering, Xidian University; School of Electronic and Engineering, Xidian University; School of Electronic and Engineering, Xidian University; School of Electronic and Engineering, Xidian University; School of Computer Science and Technology, Xidian University; Central Software Institute, 2012 Lab, Huawei Technologies Co. Ltd; Central Software Institute, 2012 Lab, Huawei Technologies Co. Ltd; Central Software Institute, 2012 Lab, Huawei Technologies Co. Ltd",2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"2 Nov 2021","2021","","","13474","13482","The bottleneck of computation burden limits the widespread use of the 2nd order optimization algorithms for training deep neural networks. In this paper, we present a computationally efficient approximation for natural gradient descent, named Swift Kronecker-Factored Approximate Curvature (SKFAC), which combines Kronecker factorization and a fast low-rank matrix inversion technique. Our research aims at both fully connected and convolutional layers. For the fully connected layers, by utilizing the low-rank property of Kronecker factors of Fisher information matrix, our method only requires inverting a small matrix to approximate the curvature with desirable accuracy. For convolutional layers, we propose a way with two strategies to save computational efforts without affecting the empirical performance by reducing across the spatial dimension or receptive fields of feature maps. Specifically, we propose two effective dimension reduction methods for this purpose: Spatial Subsampling and Reduce Sum. Experimental results of training several deep neural networks on Cifar-10 and ImageNet-1k datasets demonstrate that SKFAC can capture the main curvature and yield comparative performance to K-FAC. The proposed method bridges the wall-clock time gap between the 1st and 2nd order algorithms.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.01327","National Natural Science Foundation of China; Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578481","","Training;Deep learning;Dimensionality reduction;Neural networks;Text categorization;Approximation algorithms;Robustness","","7","","26","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Research on air Pollution Gases Recognition Method Based on LSTM Recurrent Neural Network and Gas Sensors Array","Q. Wang; T. Xie; S. Wang","College of Electronic Science and Engineer, Jilin university, Changehun, China; College of Electronic Science and Engineering, Jilin University, Changehun, China; College of Electronic Science and Engineering, Jilin University, Changehun, China",2018 Chinese Automation Congress (CAC),"24 Jan 2019","2018","","","3486","3491","The integration of sensors array and pattern recognition to replace large scale analytical instruments is an important and feasible method for accurate and rapid measurement of atmospheric pollution gases. This paper proposed a quantitative detection method of mixed gases based on long short-term memory (LSTM) recurrent neural network, including data pre-processing, network structure design, model training and prediction process. This method can extract the deep characteristics of sensors array's responses automatically and match the complex nonlinear characteristics more accurately. It is proved that the proposed LSTM prediction model has a strong applicability and higher accuracy in the concentration identification of gas mixtures by comparing with the experiments of existing models.","","978-1-7281-1312-8","10.1109/CAC.2018.8623060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8623060","gas sensors array;pattern recognition;air pollution gases;deep learning;recurrent neural networks","Sensor arrays;Gases;Gas detectors;Recurrent neural networks;Atmospheric measurements;Air pollution","","7","","15","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"Bearing Fault Diagnosis Using One-Dimensional Convolutional Neural Network","Z. Gao; Z. Wei; Y. Chen; T. Ying; H. Gao","College of IOT Engineering, Hohai University, Nanjing, China; WeiMing Intelligent Technology Co.,Ltd., Wuxi, China; College of Engineering, South China Agricultural University, Guangzhou, China; College of IOT Engineering, Hohai University, Nanjing, China; College of IOT Engineering, Hohai University, Nanjing, China","2022 22nd International Conference on Control, Automation and Systems (ICCAS)","9 Jan 2023","2022","","","158","162","In this paper, a fault diagnosis strategy using one-dimensional convolutional neural network (CNN) is developed for rolling bearing. Firstly, each basic unit in the CNN model to be proposed is introduced in detail, and the optimization algorithm required for the CNN is described to show the working principle, which provides a theoretical basis for the one-dimensional CNN model. Next, a series of preprocessing such as overlap sampling and unique thermal coding are performed on the rolling bearing dataset from Case Western Reserve University, and a batch normalization algorithm is proposed to improve the training efficiency and performance of the CNN model. Finally, the designed one-dimensional CNN model is trained, the adaptive ability of the model with variable load is tested, and good results are obtained.","2642-3901","978-89-93215-24-3","10.23919/ICCAS55662.2022.10003748","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10003748","big data;deep learning;convolutional neural network;rolling bearing fault diagnosis","Fault diagnosis;Training;Heating systems;Adaptation models;Neural networks;Rolling bearings;Encoding","","","","12","","9 Jan 2023","","","IEEE","IEEE Conferences"
"Blockchain Assisted Data Edge Verification With Consensus Algorithm for Machine Learning Assisted IoT","T. Vaiyapuri; K. Shankar; S. Rajendran; S. Kumar; S. Acharya; H. Kim","College of Computer Engineering and Sciences, Prince Sattam bin Abdulaziz University, Al-Kharj, Saudi Arabia; Department of Computer Science and Engineering, Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Chennai, India; Department of Computer Science and Engineering, Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Chennai, India; Big Data and Machine Learning Laboratory, South Ural State University, Chelyabinsk, Russia; Department of Convergence Science, Kongju National University, Gongju-si 32588, South Korea; Department of Convergence Science, Kongju National University, Gongju-si 32588, South Korea",IEEE Access,"9 Jun 2023","2023","11","","55370","55379","Internet of Things (IoT) devices are becoming increasingly ubiquitous in daily life. They are utilized in various sectors like healthcare, manufacturing, and transportation. The main challenges related to IoT devices are the potential for faults to occur and their reliability. In classical IoT fault detection, the client device must upload raw information to the central server for the training model, which can reveal sensitive business information. Blockchain (BC) technology and a fault detection algorithm are applied to overcome these challenges. Generally, the fusion of BC technology and fault detection algorithms can give a secure and more reliable IoT ecosystem. Therefore, this study develops a new Blockchain Assisted Data Edge Verification with Consensus Algorithm for Machine Learning (BDEV-CAML) technique for IoT Fault Detection purposes. The presented BDEV-CAML technique integrates the benefits of blockchain, IoT, and ML models to enhance the IoT network’s trustworthiness, efficacy, and security. In BC technology, IoT devices that possess a significant level of decentralized decision-making capability can attain a consensus on the efficiency of intrablock transactions. For fault detection in the IoT network, the deep directional gated recurrent unit (DBiGRU) model is used. Finally, the African vulture optimization algorithm (AVOA) technique is utilized for the optimal hyperparameter tuning of the DBiGRU model, which helps in improving the fault detection rate. A detailed set of experiments were carried out to highlight the enhanced performance of the BDEV-CAML algorithm. The comprehensive experimental results stated the improved performance of the BDEV-CAML technique over other existing models with maximum accuracy of 99.6%.","2169-3536","","10.1109/ACCESS.2023.3280798","Institute of Information and Communications Technology Planning and Evaluation (IITP) Grant funded by the Korea Government (MSIT) (Development of User Identity Certification and Management Technology for Self-Sovereign Identity Applications and Robust AI and Distributed Attack Detection for Edge AI Security)(grant numbers:2021-0-00565,2021-0-00511); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10138178","Blockchain;Internet of Things;consensus algorithm;fault detection;deep learning;hyperparameter tuning","Internet of Things;Fault detection;Logic gates;Blockchains;Security;Tuning;Consensus algorithm","","9","","26","CCBYNCND","29 May 2023","","","IEEE","IEEE Journals"
"Solving Seismic Wave Equations on Variable Velocity Models With Fourier Neural Operator","B. Li; H. Wang; S. Feng; X. Yang; Y. Lin","Department of Industrial and Systems Engineering, Lehigh University, Bethlehem, PA, USA; Theoretical Division, Los Alamos National Laboratory, Los Alamos, NM, USA; Theoretical Division, Los Alamos National Laboratory, Los Alamos, NM, USA; Department of Industrial and Systems Engineering, Lehigh University, Bethlehem, PA, USA; School of Data Science and Society, The University of North Carolina at Chapel Hill, Chapel Hill, NC, USA",IEEE Transactions on Geoscience and Remote Sensing,"27 Nov 2023","2023","61","","1","18","In the study of subsurface seismic imaging, solving the acoustic wave equation is a pivotal component in existing models. The advancement of deep learning (DL) enables solving partial differential equations (PDEs), including wave equations, by applying neural networks to identify the mapping between the inputs and the solution. This approach can be faster than traditional numerical methods when numerous instances are to be solved. Previous works that concentrate on solving the wave equation by neural networks consider either a single velocity model or multiple simple velocity models, which is restricted in practice. Instead, inspired by the idea of operator learning, this work leverages the Fourier neural operator (FNO) to effectively learn the frequency domain seismic wavefields under the context of variable velocity models. We also propose a new framework paralleled FNO (PFNO) for efficiently training the FNO-based solver given multiple source locations and frequencies. Numerical experiments demonstrate the high accuracy of both FNO and PFNO with complicated velocity models in the OpenFWI datasets. Furthermore, the cross-dataset generalization test verifies that PFNO adapts to out-of-distribution velocity models. Finally, PFNO admits higher computational efficiency on large-scale testing datasets than the traditional finite-difference method. The aforementioned advantages endow the FNO-based solver with the potential to build powerful models for research on seismic waves.","1558-0644","","10.1109/TGRS.2023.3333663","Los Alamos National Laboratory (LANL)—Laboratory Directed Research and Development Program(grant numbers:20210542MFR); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10319762","Fourier neural operator (FNO);Helmholtz equation;operator learning;seismic wave","Mathematical models;Propagation;Numerical models;Computational modeling;Training;Frequency-domain analysis;Data models","","3","","46","IEEE","16 Nov 2023","","","IEEE","IEEE Journals"
"Learning to Explore Distillability and Sparsability: A Joint Framework for Model Compression","Y. Liu; J. Cao; B. Li; W. Hu; S. Maybank","National Laboratory of Pattern Recognition, Chinese Academy of Sciences, Institution of Automation, Beijing, China; Ant Group, Beijing, China; National Laboratory of Pattern Recognition, Chinese Academy of Sciences, Institution of Automation, Beijing, China; National Laboratory of Pattern Recognition, Chinese Academy of Sciences, Institution of Automation, Beijing, China; Department of Computer Science and Information Systems, Birkbeck College, University of London, London, U.K.",IEEE Transactions on Pattern Analysis and Machine Intelligence,"3 Feb 2023","2023","45","3","3378","3395","Deep learning shows excellent performance usually at the expense of heavy computation. Recently, model compression has become a popular way of reducing the computation. Compression can be achieved using knowledge distillation or filter pruning. Knowledge distillation improves the accuracy of a lightweight network, while filter pruning removes redundant architecture in a cumbersome network. They are two different ways of achieving model compression, but few methods simultaneously consider both of them. In this paper, we revisit model compression and define two attributes of a model: distillability and sparsability, which reflect how much useful knowledge can be distilled and how many pruned ratios can be obtained, respectively. Guided by our observations and considering both accuracy and model size, a dynamically distillability-and-sparsability learning framework (DDSL) is introduced for model compression. DDSL consists of teacher, student and dean. Knowledge is distilled from the teacher to guide the student. The dean controls the training process by dynamically adjusting the distillation supervision and the sparsity supervision in a meta-learning framework. An alternating direction method of multiplier (ADMM)-based knowledge distillation-with-pruning (KDP) joint optimization algorithm is proposed to train the model. Extensive experimental results show that DDSL outperforms 24 state-of-the-art methods, including both knowledge distillation and filter pruning methods.","1939-3539","","10.1109/TPAMI.2022.3185317","National Key Research and Development Program of China(grant numbers:2020AAA0106800); Natural Science Foundation of China(grant numbers:61902401,62192785,61972071,U1936204,62122086,62036011,62192782,61721004); Beijing Natural Science Foundation(grant numbers:M22005); CAS Key Research Program of Frontier Sciences(grant numbers:QYZDJ-SSW-JSC040); Youth Innovation Promotion Association, CAS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9804342","Knowledge distillation;filter pruning;structured sparsity pruning;deep learning","Training;Optimization;Knowledge engineering;Computer architecture;Computational modeling;Analytical models;Heuristic algorithms","","7","","70","IEEE","22 Jun 2022","","","IEEE","IEEE Journals"
"CSR-Net: A Novel Complex-Valued Network for Fast and Precise 3-D Microwave Sparse Reconstruction","M. Wang; S. Wei; J. Shi; Y. Wu; Q. Qu; Y. Zhou; X. Zeng; B. Tian","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China",IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"24 Aug 2020","2020","13","","4476","4492","Since the compressed sensing (CS) theory broke through the limitation of the traditional Nyquist sampling theory, it has attracted extensive attention in the field of microwave imaging. However, in 3-D microwave sparse reconstruction application, conventional CS-based algorithms always suffer from huge computational cost. In this article, a novel 3-D microwave sparse reconstruction method based on a complex-valued sparse reconstruction network (CSR-Net) is proposed, which converts complex number operations into matrix operations for real and imaginary parts. Using the unfolding + network approximate scheme, each iteration process of CS-based iterative threshold optimization is designed as a block of CSR-Net, and a modified shrinkage term is introduced to improve the convergence performance of the approach. In addition, CSR-Net adopts a convolutional neural network module to replace a nonlinear sparse representation process, which dramatically reduces computational complexity and improves reconstruction performance over conventional CS-based iterative threshold optimization algorithms. Then, we divide the 3-D scene into a series of 2-D slices, and a phase correction scheme is adopted to ensure that the whole 3-D scene can be reconstructed with measurement matrix of a slice. Moreover, an efficient position-amplitude-random training method without additional real-measured data is employed for the proposed network, which effectively train the CSR-Net without enough real-measured data. Extensive experiment results demonstrate that CSR-Net outperforms both conventional iterative threshold optimization methods and deep network-based ISTA-NET-plus large margins. Its speed and reconstruction accuracy in 3-D imaging can achieve a state-of-the-art level.","2151-1535","","10.1109/JSTARS.2020.3014696","National Key R&D Program of China(grant numbers:2017-YFB0502700); National Natural Science Foundation of China(grant numbers:61671113,61501098); High-Resolution Earth Observation Youth Foundation(grant numbers:GFZX04061502); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9161274","3-D microwave imaging;complex-valued network;compressed sensing (CS);convolutional neural network (CNN);deep learning (DL);iterative threshold optimization","Image reconstruction;Microwave imaging;Microwave theory and techniques;Iterative algorithms;Training","","33","","47","CCBY","6 Aug 2020","","","IEEE","IEEE Journals"
"D2D Cooperative Communication Network Resource Allocation Algorithm Based on Improved Monte Carlo Tree Search","X. Li; G. Chen","School of Electronic and Information Engineering, Changchun University of Science and Technology, Changchun, China; School of Electronic and Information Engineering, Changchun University of Science and Technology, Changchun, China",IEEE Access,"21 Jul 2023","2023","11","","72689","72703","In recent years, with the rapid development of mobile communication, D2D (Device-to-Device, D2D) cooperative communication network has become the main component of future communication network, which greatly improves the spectrum efficiency of the network and the quality of user communication. However, the existing D2D network resource allocation schemes have some problems, such as weak dynamic resource allocation capability and low user communication quality. In view of this challenge, this paper proposes a resource allocation algorithm for D2D cooperative communication networks based on improved Monte Carlo tree search. First, a double-chain deep deciduous Monte Carlo tree search (Dcdd-MCTS) resource allocation model is established, Then, the loss function composed of deciduous MCTS and parallel convolution network is used to update the parameters of the deep neural network model of Dcdd-MCTS. Then, the theory of optimal classification is used to solve the user’s transmit power. Finally, the optimal scheme of dynamic output resource allocation is output. The simulation results show that Dcdd-MCTS has good convergence. In the research on the distance between devices, compared with single-chain deep MCTS and joint optimization algorithm, the proposed algorithm in this paper increases the system throughput by 5%, 2%, respectively, and reduces the outage probability by 33%,18%.","2169-3536","","10.1109/ACCESS.2023.3280604","Special Project on Industrial Technology Research and Development of Jilin Province(grant numbers:2022C047-8); “Thirteenth Five-Year Plan” Science and Technology Research Project of Jilin Provincial Department of Education, Research on Large-Scale Device-to-Device Access and Traffic Balancing Technology for Heterogeneous Wireless Networks(grant numbers:JJKH20181130KJ); National Natural Science Foundation of China(grant numbers:61540022); Key Research and Development Projects of Changchun Science and Technology Bureau(grant numbers:21ZGM43); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10138162","D2D cooperative communication network;Monte Carlo tree search;double chain parallel neural network;resource allocation;interference management","Device-to-device communication;Resource management;Relays;Interference;Communication networks;Optimization;Cooperative systems;Monte Carlo methods","","3","","54","CCBYNCND","29 May 2023","","","IEEE","IEEE Journals"
"Robust Fine-Grained Visual Recognition With Neighbor-Attention Label Correction","S. Mao; S. Zhang","National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University, Beijing, China; National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University, Beijing, China",IEEE Transactions on Image Processing,"3 Apr 2024","2024","33","","2614","2626","Existing deep learning methods for fine-grained visual recognition often rely on large-scale, well-annotated training data. Obtaining fine-grained annotations in the wild typically requires concentration and expertise, such as fine category annotation for species recognition, instance annotation for person re-identification (re-id) and dense annotation for segmentation, which inevitably leads to label noise. This paper aims to tackle label noise in deep model training for fine-grained visual recognition. We propose a Neighbor-Attention Label Correction (NALC) model to correct labels during the training stage. NALC samples a training batch and a validation batch from the training set. It hence leverages a meta-learning framework to correct labels in the training batch based on the validation batch. To enhance the optimization efficiency, we introduce a novel nested optimization algorithm for the meta-learning framework. The proposed training procedure consistently improves label accuracy in the training batch, consequently enhancing the learned image representation. Experimental results demonstrate that our method significantly increases label accuracy from 70% to over 98% and outperforms recent approaches by up to 13.4% in mean Average Precision (mAP) on various fine-grained image retrieval (FGIR) tasks, including instance retrieval on CUB200 and person re-id on Market1501. We also demonstrate the efficacy of NALC on noisy semantic segmentation datasets generated from Cityscapes, where it achieves a significant 7.8% improvement in mIOU score. NALC also exhibits robustness to different types of noise, including simulated noise such as Asymmetric, Pair-Flip, and Pattern noise, as well as practical noisy labels generated by tracklets and clustering.","1941-0042","","10.1109/TIP.2024.3378461","Natural Science Foundation of China(grant numbers:U20B2052,61936011); Okawa Foundation Research Award; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10485225","Noisy label;meta learning;image retrieval;person re-id;semantic segmentation","Training;Noise measurement;Visualization;Optimization;Task analysis;Feature extraction;Annotations","","","","68","IEEE","28 Mar 2024","","","IEEE","IEEE Journals"
